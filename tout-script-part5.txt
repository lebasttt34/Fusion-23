            return await gemini_client.generate_content(
                prompt=prompt,
                chat_history=chat_history or [],
                image_data=image_data,
                tools=tools
            )
            
        except Exception as e:
            raise Exception(f"Erreur Gemini: {e}")

class DeepSeekBrain(AutonomousBrain):
    """Cerveau autonome bas√© sur DeepSeek."""
    
    def __init__(self, telegram_client=None):
        super().__init__("DEEPSEEK", "DEEPSEEK", telegram_client)
    
    async def _generate_response(self, prompt: str, key_info: Dict,
                               chat_history: List[Dict] = None,
                               image_data: str = None,
                               tools: List[Dict] = None) -> Dict[str, Any]:
        """G√©n√®re une r√©ponse via l'API DeepSeek."""
        try:
            from app_clients_instances import deepseek_client
            
            messages = []
            if chat_history:
                for msg in chat_history:
                    content = " ".join([part.get("text", "") for part in msg.get("parts", [])])
                    if content.strip():
                        role = "assistant" if msg["role"] == "model" else msg["role"]
                        messages.append({"role": role, "content": content})
            
            messages.append({"role": "user", "content": prompt})
            
            result = await deepseek_client.chat_completion(messages=messages)
            
            # Conversion au format Gemini-like
            if isinstance(result, dict) and "choices" in result and result["choices"]:
                content = result["choices"][0]["message"]["content"]
                return {
                    "candidates": [{
                        "content": {"parts": [{"text": content}]}
                    }]
                }
            return {"candidates": [{"content": {"parts": [{"text": str(result)}]}}]}
                
        except Exception as e:
            raise Exception(f"Erreur DeepSeek: {e}")

class HuggingFaceBrain(AutonomousBrain):
    """Cerveau autonome bas√© sur HuggingFace."""
    
    def __init__(self, telegram_client=None):
        super().__init__("HUGGINGFACE", "HUGGINGFACE", telegram_client)
    
    async def _generate_response(self, prompt: str, key_info: Dict,
                               chat_history: List[Dict] = None,
                               image_data: str = None,
                               tools: List[Dict] = None) -> Dict[str, Any]:
        """G√©n√®re une r√©ponse via l'API HuggingFace."""
        try:
            from app_clients_instances import huggingface_client
            
            # Utilise un mod√®le de g√©n√©ration de texte
            model_name = "microsoft/DialoGPT-large"
            result = await huggingface_client.inference(model_name=model_name, input_text=prompt)
            
            # Conversion au format Gemini-like
            if isinstance(result, list) and result:
                generated_text = result[0].get("generated_text", prompt)
                # Extrait seulement la nouvelle partie g√©n√©r√©e
                new_text = generated_text[len(prompt):].strip()
                if not new_text:
                    new_text = "R√©ponse g√©n√©r√©e par HuggingFace"
                
                return {
                    "candidates": [{
                        "content": {"parts": [{"text": new_text}]}
                    }]
                }
            return {"candidates": [{"content": {"parts": [{"text": str(result)}]}}]}
                
        except Exception as e:
            raise Exception(f"Erreur HuggingFace: {e}")

class TavilyBrain(AutonomousBrain):
    """Cerveau autonome bas√© sur Tavily."""
    
    def __init__(self, telegram_client=None):
        super().__init__("TAVILY", "TAVILY", telegram_client)
    
    async def _generate_response(self, prompt: str, key_info: Dict,
                               chat_history: List[Dict] = None,
                               image_data: str = None,
                               tools: List[Dict] = None) -> Dict[str, Any]:
        """G√©n√®re une r√©ponse via l'API Tavily."""
        try:
            from app_clients_instances import tavily_client
            
            result = await tavily_client.search(query=prompt, max_results=5)
            
            # Traitement des r√©sultats Tavily
            answer = result.get("answer", "")
            results = result.get("results", [])
            
            # Synth√®se de la r√©ponse
            synthesis = f"R√©ponse Tavily: {answer}\n\n"
            if results:
                synthesis += "Sources:\n"
                for i, res in enumerate(results[:3], 1):
                    title = res.get("title", "Sans titre")
                    content = res.get("content", "")[:200]
                    synthesis += f"{i}. {title}: {content}...\n"
            
            return {
                "candidates": [{
                    "content": {"parts": [{"text": neutralize_urls(synthesis)}]}
                }]
            }
                
        except Exception as e:
            raise Exception(f"Erreur Tavily: {e}")

class SerperBrain(AutonomousBrain):
    """Cerveau autonome bas√© sur Serper."""
    
    def __init__(self, telegram_client=None):
        super().__init__("SERPER", "SERPER", telegram_client)
    
    async def _generate_response(self, prompt: str, key_info: Dict,
                               chat_history: List[Dict] = None,
                               image_data: str = None,
                               tools: List[Dict] = None) -> Dict[str, Any]:
        """G√©n√®re une r√©ponse via l'API Serper."""
        try:
            from app_clients_instances import serper_client
            
            result = await serper_client.search(query=prompt)
            
            # Traitement des r√©sultats Serper
            organic = result.get("organic", [])
            answer_box = result.get("answerBox", {})
            
            synthesis = ""
            if answer_box:
                synthesis += f"R√©ponse directe: {answer_box.get('answer', '')}\n\n"
            
            if organic:
                synthesis += "R√©sultats de recherche:\n"
                for i, res in enumerate(organic[:3], 1):
                    title = res.get("title", "Sans titre")
                    snippet = res.get("snippet", "")
                    synthesis += f"{i}. {title}: {snippet}\n"
            
            if not synthesis:
                synthesis = "Aucun r√©sultat trouv√© pour cette recherche."
            
            return {
                "candidates": [{
                    "content": {"parts": [{"text": neutralize_urls(synthesis)}]}
                }]
            }
                
        except Exception as e:
            raise Exception(f"Erreur Serper: {e}")

class GoogleBrain(AutonomousBrain):
    """Cerveau autonome bas√© sur Google Custom Search."""
    
    def __init__(self, telegram_client=None):
        super().__init__("GOOGLE", "GOOGLE_CUSTOM_SEARCH", telegram_client)
    
    async def _generate_response(self, prompt: str, key_info: Dict,
                               chat_history: List[Dict] = None,
                               image_data: str = None,
                               tools: List[Dict] = None) -> Dict[str, Any]:
        """G√©n√®re une r√©ponse via l'API Google Custom Search."""
        try:
            from app_clients_instances import google_custom_search_client
            
            result = await google_custom_search_client.search(query=prompt)
            
            # Traitement des r√©sultats Google
            items = result.get("items", [])
            
            synthesis = f"R√©sultats Google pour: {prompt}\n\n"
            if items:
                for i, item in enumerate(items[:3], 1):
                    title = item.get("title", "Sans titre")
                    snippet = item.get("snippet", "")
                    synthesis += f"{i}. {title}: {snippet}\n"
            else:
                synthesis += "Aucun r√©sultat trouv√©."
            
            return {
                "candidates": [{
                    "content": {"parts": [{"text": neutralize_urls(synthesis)}]}
                }]
            }
                
        except Exception as e:
            raise Exception(f"Erreur Google: {e}")

class WolframBrain(AutonomousBrain):
    """Cerveau autonome bas√© sur Wolfram Alpha."""
    
    def __init__(self, telegram_client=None):
        super().__init__("WOLFRAM", "WOLFRAMALPHA", telegram_client)
    
    async def _generate_response(self, prompt: str, key_info: Dict,
                               chat_history: List[Dict] = None,
                               image_data: str = None,
                               tools: List[Dict] = None) -> Dict[str, Any]:
        """G√©n√®re une r√©ponse via l'API Wolfram Alpha."""
        try:
            from app_clients_instances import wolfram_alpha_client
            
            result = await wolfram_alpha_client.query(input_text=prompt)
            
            # Traitement des r√©sultats Wolfram
            query_result = result.get("queryresult", {})
            pods = query_result.get("pods", [])
            
            synthesis = f"R√©sultat Wolfram Alpha pour: {prompt}\n\n"
            if pods:
                for pod in pods[:3]:
                    title = pod.get("title", "")
                    subpods = pod.get("subpods", [])
                    if subpods:
                        text = subpods[0].get("plaintext", "")
                        if text:
                            synthesis += f"{title}: {text}\n"
            else:
                synthesis += "Aucun r√©sultat calculable trouv√©."
            
            return {
                "candidates": [{
                    "content": {"parts": [{"text": synthesis}]}
                }]
            }
                
        except Exception as e:
            raise Exception(f"Erreur Wolfram: {e}")

# Fonction factory pour cr√©er les cerveaux
def create_brain(brain_type: str, telegram_client=None) -> AutonomousBrain:
    """Factory pour cr√©er un cerveau du type demand√©."""
    brain_classes = {
        "GEMINI": GeminiBrain,
        "DEEPSEEK": DeepSeekBrain,
        "HUGGINGFACE": HuggingFaceBrain,
        "TAVILY": TavilyBrain,
        "SERPER": SerperBrain,
        "GOOGLE_CUSTOM_SEARCH": GoogleBrain,
        "WOLFRAMALPHA": WolframBrain
    }
    
    if brain_type not in brain_classes:
        raise ValueError(f"Type de cerveau non support√©: {brain_type}")
    
    return brain_classes[brain_type](telegram_client)
    
    import asyncio
import json
import random
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Union, Tuple
import logging
from pathlib import Path

from config import config
from utils import log_message, load_json, save_json, get_current_time

class APIKeyLibrary:
    """
    Biblioth√®que centralis√©e pour la gestion des cl√©s API et endpoints.
    Chaque cerveau peut utiliser tous les endpoints de son service.
    """
    def __init__(self):
        self.api_keys = self._initialize_api_keys()
        self.endpoint_rotation = {}
        self.last_rotation_time = {}
        self.failed_endpoints = {}
        
    def _initialize_api_keys(self) -> Dict[str, List[Dict]]:
        """Initialise toutes les cl√©s API avec leurs endpoints."""
        return {
            "GEMINI": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "generate_content",
                            "url": "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent",
                            "method": "POST"
                        },
                        {
                            "name": "list_models",
                            "url": "https://generativelanguage.googleapis.com/v1beta/models",
                            "method": "GET"
                        },
                        {
                            "name": "embed_content",
                            "url": "https://generativelanguage.googleapis.com/v1beta/models/embedding-001:embedContent",
                            "method": "POST"
                        }
                    ]
                } for key in config.GEMINI_API_KEYS
            ],
            "DEEPSEEK": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "chat_completions",
                            "url": "https://api.deepseek.com/chat/completions",
                            "method": "POST"
                        },
                        {
                            "name": "list_models",
                            "url": "https://api.deepseek.com/models",
                            "method": "GET"
                        }
                    ]
                } for key in config.DEEPSEEK_KEYS
            ],
            "HUGGINGFACE": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "inference",
                            "url": "https://api-inference.huggingface.co/models/",
                            "method": "POST"
                        },
                        {
                            "name": "list_models",
                            "url": "https://huggingface.co/api/models",
                            "method": "GET"
                        }
                    ]
                } for key in config.HUGGINGFACE_KEYS
            ],
            "TAVILY": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "search",
                            "url": "https://api.tavily.com/search",
                            "method": "POST"
                        },
                        {
                            "name": "extract",
                            "url": "https://api.tavily.com/extract",
                            "method": "POST"
                        }
                    ]
                } for key in config.TAVILY_KEYS
            ],
            "SERPER": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "search",
                            "url": "https://google.serper.dev/search",
                            "method": "POST"
                        },
                        {
                            "name": "images",
                            "url": "https://google.serper.dev/images",
                            "method": "POST"
                        },
                        {
                            "name": "news",
                            "url": "https://google.serper.dev/news",
                            "method": "POST"
                        }
                    ]
                } for key in config.SERPER_KEYS
            ],
            "GOOGLE_CUSTOM_SEARCH": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "custom_search",
                            "url": "https://www.googleapis.com/customsearch/v1",
                            "method": "GET",
                            "cx": cx
                        } for cx in config.GOOGLE_CX_LIST
                    ]
                } for key in config.GOOGLE_API_KEYS
            ],
            "WOLFRAMALPHA": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "query",
                            "url": "https://api.wolframalpha.com/v2/query",
                            "method": "GET"
                        },
                        {
                            "name": "simple",
                            "url": "https://api.wolframalpha.com/v1/simple",
                            "method": "GET"
                        }
                    ]
                } for key in config.WOLFRAM_APP_IDS
            ]
        }
    
    def get_available_key(self, service: str) -> Optional[Dict]:
        """R√©cup√®re une cl√© API disponible pour un service donn√©."""
        if service not in self.api_keys:
            return None
            
        available_keys = []
        for key_info in self.api_keys[service]:
            key_id = f"{service}_{key_info['key'][:8]}"
            if key_id not in self.failed_endpoints or self.failed_endpoints[key_id] < time.time() - 300:
                available_keys.append(key_info)
        
        if not available_keys:
            # Reset failed endpoints if all are failed
            for key_info in self.api_keys[service]:
                key_id = f"{service}_{key_info['key'][:8]}"
                if key_id in self.failed_endpoints:
                    del self.failed_endpoints[key_id]
            available_keys = self.api_keys[service]
        
        return random.choice(available_keys) if available_keys else None
    
    def mark_key_failed(self, service: str, key: str):
        """Marque une cl√© comme d√©faillante temporairement."""
        key_id = f"{service}_{key[:8]}"
        self.failed_endpoints[key_id] = time.time()
        log_message(f"Cl√© {key_id} marqu√©e comme d√©faillante pour 5 minutes", level="warning")
    
    def get_endpoint(self, service: str, endpoint_name: str) -> Optional[Dict]:
        """R√©cup√®re un endpoint sp√©cifique pour un service."""
        key_info = self.get_available_key(service)
        if not key_info:
            return None
            
        for endpoint in key_info["endpoints"]:
            if endpoint["name"] == endpoint_name:
                return {
                    "key": key_info["key"],
                    "endpoint": endpoint
                }
        return None
    
    def rotate_key(self, service: str) -> Optional[Dict]:
        """Force la rotation vers une nouvelle cl√© pour un service."""
        if service not in self.endpoint_rotation:
            self.endpoint_rotation[service] = 0
        
        if service in self.api_keys and self.api_keys[service]:
            self.endpoint_rotation[service] = (self.endpoint_rotation[service] + 1) % len(self.api_keys[service])
            self.last_rotation_time[service] = time.time()
            return self.api_keys[service][self.endpoint_rotation[service]]
        return None

class BrainMemoryManager:
    """
    Gestionnaire de m√©moire pour les 7 cerveaux autonomes.
    Chaque cerveau maintient sa propre m√©moire locale et acc√®de √† la m√©moire partag√©e.
    """
    def __init__(self, brain_id: str):
        self.brain_id = brain_id
        self.local_memory = {}
        self.shared_memory_file = config.BRAIN_MEMORY_FILE
        self.last_memory_update = time.time()
        
    async def load_memory(self) -> Dict[str, Any]:
        """Charge la m√©moire locale et partag√©e."""
        try:
            shared_memory = await load_json(self.shared_memory_file, {})
            if self.brain_id not in shared_memory:
                shared_memory[self.brain_id] = {
                    "interactions": [],
                    "learned_patterns": {},
                    "success_rate": 1.0,
                    "last_active": datetime.now(timezone.utc).isoformat()
                }
                await save_json(self.shared_memory_file, shared_memory)
            
            self.local_memory = shared_memory[self.brain_id]
            return self.local_memory
        except Exception as e:
            log_message(f"Erreur chargement m√©moire pour cerveau {self.brain_id}: {e}", level="error")
            return {}
    
    async def save_memory(self):
        """Sauvegarde la m√©moire locale dans le fichier partag√©."""
        try:
            shared_memory = await load_json(self.shared_memory_file, {})
            shared_memory[self.brain_id] = self.local_memory
            shared_memory[self.brain_id]["last_active"] = datetime.now(timezone.utc).isoformat()
            await save_json(self.shared_memory_file, shared_memory)
            self.last_memory_update = time.time()
        except Exception as e:
            log_message(f"Erreur sauvegarde m√©moire pour cerveau {self.brain_id}: {e}", level="error")
    
    async def add_interaction(self, user_query: str, response: str, tools_used: List[str] = None):
        """Ajoute une interaction √† la m√©moire."""
        interaction = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "user_query": user_query[:200],  # Limite pour √©viter la surcharge m√©moire
            "response": response[:500],
            "tools_used": tools_used or [],
            "brain_id": self.brain_id
        }
        
        if "interactions" not in self.local_memory:
            self.local_memory["interactions"] = []
            
        self.local_memory["interactions"].append(interaction)
        
        # Limite le nombre d'interactions en m√©moire
        if len(self.local_memory["interactions"]) > 100:
            self.local_memory["interactions"] = self.local_memory["interactions"][-100:]
        
        await self.save_memory()
    
    async def get_relevant_context(self, query: str, limit: int = 5) -> List[Dict]:
        """R√©cup√®re le contexte pertinent bas√© sur la requ√™te."""
        await self.load_memory()
        
        if "interactions" not in self.local_memory:
            return []
        
        # Simple matching bas√© sur les mots-cl√©s
        query_words = set(query.lower().split())
        relevant_interactions = []
        
        for interaction in self.local_memory["interactions"]:
            interaction_words = set(interaction["user_query"].lower().split())
            if query_words.intersection(interaction_words):
                relevance_score = len(query_words.intersection(interaction_words)) / len(query_words.union(interaction_words))
                relevant_interactions.append((relevance_score, interaction))
        
        # Trie par pertinence et retourne les plus pertinents
        relevant_interactions.sort(key=lambda x: x[0], reverse=True)
        return [interaction for _, interaction in relevant_interactions[:limit]]
    
    async def update_success_rate(self, success: bool):
        """Met √† jour le taux de succ√®s du cerveau."""
        if "success_rate" not in self.local_memory:
            self.local_memory["success_rate"] = 1.0
        if "total_attempts" not in self.local_memory:
            self.local_memory["total_attempts"] = 0
        if "successful_attempts" not in self.local_memory:
            self.local_memory["successful_attempts"] = 0
            
        self.local_memory["total_attempts"] += 1
        if success:
            self.local_memory["successful_attempts"] += 1
            
        self.local_memory["success_rate"] = self.local_memory["successful_attempts"] / self.local_memory["total_attempts"]
        await self.save_memory()

class TelegramMemoryIntegration:
    """
    Int√©gration avec la m√©moire du groupe priv√© Telegram.
    Toutes les interactions sont stock√©es dans le groupe priv√©.
    """
    def __init__(self, bot_client):
        self.bot_client = bot_client
        self.group_id = config.PRIVATE_GROUP_ID
        self.memory_cache = []
        self.last_cache_update = 0
        
    async def read_group_memory(self, limit: int = 50) -> str:
        """Lit la m√©moire compl√®te du groupe priv√© Telegram."""
        try:
            if not self.group_id or not self.bot_client:
                return "M√©moire du groupe priv√© non configur√©e ou client bot non disponible."
            
            # En production, ici on lirait les messages r√©cents du groupe
            # Pour cette impl√©mentation, on retourne une m√©moire simul√©e
            current_time = datetime.now().isoformat()
            memory_content = f"""
=== M√âMOIRE GROUPE PRIV√â TELEGRAM ===
Derni√®re mise √† jour: {current_time}
Groupe ID: {self.group_id}

Interactions r√©centes:
- Traitement de requ√™tes utilisateur
- Ex√©cution d'outils et analyses
- G√©n√©ration de d√©fis de codage
- Monitoring de la sant√© des APIs
- Rotation automatique des cerveaux

Statut syst√®me: Op√©rationnel
Cerveaux actifs: 7 (GEMINI, DEEPSEEK, HUGGINGFACE, TAVILY, SERPER, GOOGLE_CUSTOM_SEARCH, WOLFRAMALPHA)
"""
            return memory_content
        except Exception as e:
            log_message(f"Erreur lecture m√©moire groupe: {e}", level="error")
            return "Erreur d'acc√®s √† la m√©moire du groupe."
    
    async def write_to_group(self, content: str, content_type: str = "info"):
        """√âcrit du contenu dans le groupe priv√© Telegram."""
        try:
            if not self.group_id or not self.bot_client:
                log_message(f"[{content_type.upper()}] {content}")
                return
            
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            formatted_message = f"üß† [{timestamp}] [{content_type.upper()}]\n{content}"
            
            await self.bot_client.send_message(self.group_id, formatted_message)
            log_message(f"Message √©crit dans le groupe priv√©: {formatted_message[:100]}...")
            
        except Exception as e:
            log_message(f"Erreur √©criture groupe: {e}", level="error")
    
    async def log_brain_activity(self, brain_id: str, activity: str, details: Dict = None):
        """Log l'activit√© d'un cerveau dans le groupe."""
        activity_log = f"Cerveau {brain_id}: {activity}"
        if details:
            try:
                details_str = json.dumps(details, indent=2, ensure_ascii=False)
                activity_log += f"\nD√©tails: {details_str}"
            except TypeError:
                activity_log += f"\nD√©tails (non s√©rialisables): {str(details)}"
        
        await self.write_to_group(activity_log, "BRAIN_ACTIVITY")
    
    async def log_error(self, brain_id: str, error: str):
        """Log une erreur dans le groupe."""
        error_log = f"‚ùå ERREUR - Cerveau {brain_id}: {error}"
        await self.write_to_group(error_log, "ERROR")
    
    async def log_success(self, brain_id: str, task: str, result: str):
        """Log un succ√®s dans le groupe."""
        success_log = f"‚úÖ SUCC√àS - Cerveau {brain_id}: {task}\nR√©sultat: {result[:200]}..."
        await self.write_to_group(success_log, "SUCCESS")

class BrainCoordinator:
    """
    Coordinateur pour les 7 cerveaux autonomes.
    G√®re la rotation, le basculement automatique et la coordination.
    """
    def __init__(self):
        self.active_brain_index = 0
        self.brain_names = ["GEMINI", "DEEPSEEK", "HUGGINGFACE", "TAVILY", "SERPER", "GOOGLE_CUSTOM_SEARCH", "WOLFRAMALPHA"]
        self.last_rotation = time.time()
        self.brain_health = {brain: True for brain in self.brain_names}
        self.brain_load = {brain: 0 for brain in self.brain_names}
        
    def get_next_brain(self) -> str:
        """S√©lectionne le prochain cerveau selon la rotation et la sant√©."""
        current_time = time.time()
        
        # Rotation automatique toutes les 45 minutes
        if current_time - self.last_rotation >= config.BRAIN_ROTATION_INTERVAL_SECONDS:
            self.active_brain_index = (self.active_brain_index + 1) % len(self.brain_names)
            self.last_rotation = current_time
            log_message(f"Rotation automatique vers le cerveau: {self.brain_names[self.active_brain_index]}")
        
        # Recherche d'un cerveau sain en partant du cerveau actuel
        attempts = 0
        start_index = self.active_brain_index
        while attempts < len(self.brain_names):
            current_brain_name = self.brain_names[self.active_brain_index]
            
            if self.brain_health.get(current_brain_name, True) and self.brain_load.get(current_brain_name, 0) < 5:
                # Avance l'index pour la prochaine requ√™te
                next_index = (self.active_brain_index + 1) % len(self.brain_names)
                self.active_brain_index = next_index
                return current_brain_name
            
            # Passe au cerveau suivant si l'actuel n'est pas disponible
            self.active_brain_index = (self.active_brain_index + 1) % len(self.brain_names)
            attempts += 1
        
        # Si aucun cerveau n'est disponible, utilise le premier par d√©faut
        log_message("Aucun cerveau optimal trouv√©, utilisation du premier disponible", level="warning")
        self.active_brain_index = (start_index + 1) % len(self.brain_names)
        return self.brain_names[start_index]
    
    def mark_brain_failed(self, brain_name: str):
        """Marque un cerveau comme d√©faillant."""
        self.brain_health[brain_name] = False
        log_message(f"Cerveau {brain_name} marqu√© comme d√©faillant", level="warning")
        
        # Auto-r√©cup√©ration apr√®s 10 minutes
        asyncio.create_task(self._auto_recover_brain(brain_name))
    
    async def _auto_recover_brain(self, brain_name: str):
        """R√©cup√©ration automatique d'un cerveau apr√®s un d√©lai."""
        await asyncio.sleep(600)  # 10 minutes
        self.brain_health[brain_name] = True
        log_message(f"Cerveau {brain_name} remis en service automatiquement")
    
    def update_brain_load(self, brain_name: str, load_change: int):
        """Met √† jour la charge d'un cerveau."""
        if brain_name in self.brain_load:
            self.brain_load[brain_name] = max(0, self.brain_load[brain_name] + load_change)
    
    def get_brain_status(self) -> Dict[str, Any]:
        """Retourne le statut de tous les cerveaux."""
        return {
            "active_brain_for_next_request": self.brain_names[self.active_brain_index],
            "brain_health": self.brain_health.copy(),
            "brain_load": self.brain_load.copy(),
            "last_rotation": datetime.fromtimestamp(self.last_rotation).isoformat(),
            "next_rotation_due": datetime.fromtimestamp(self.last_rotation + config.BRAIN_ROTATION_INTERVAL_SECONDS).isoformat()
        }

# Instances globales
api_key_library = APIKeyLibrary()
brain_coordinator = BrainCoordinator()

import asyncio
import json
import random
import time
import difflib
import hashlib
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, List, Optional
import concurrent.futures
from functools import lru_cache

from config import config
from brain_library import api_key_library, TelegramMemoryIntegration
from autonomous_brain import create_brain
from utils import log_message, save_json

# --- D√©but des Mocks pour la d√©monstration (√† retirer en production si vos modules existent) ---
# Ces mocks sont ici pour que le code puisse √™tre "ex√©cut√©" m√™me sans vos modules complets.
# En production, vous utiliserez les imports r√©els ci-dessus.

class MockConfig:
    CODING_CHALLENGE_INTERVAL_SECONDS = 900  # 15 minutes
    DAILY_CHALLENGE_PATH = Path("./daily_challenges") # Chemin pour sauvegarder les d√©fis
    def __init__(self):
        self.DAILY_CHALLENGE_PATH.mkdir(parents=True, exist_ok=True)
config = MockConfig()

class MockTelegramMemoryIntegration:
    def __init__(self, client=None):
        pass
    async def write_to_group(self, message, message_type):
        print(f"Telegram Group ({message_type}): {message}")
    async def log_error(self, brain_id, error_message):
        print(f"Telegram Error ({brain_id}): {error_message}")

class MockBrain:
    # Le MockBrain accepte maintenant des configs plus d√©taill√©es
    def __init__(self, brain_config: Dict[str, Any], telegram_client=None):
        self.brain_id = brain_config.get("id", "Inconnu") # Utilise un ID unique pour ce cerveau
        self.brain_type = brain_config.get("type", "UNKNOWN")
        self.api_key = brain_config.get("api_key", "NO_KEY")
        self.endpoints = brain_config.get("endpoints", [])
        print(f"MockBrain {self.brain_id} ({self.brain_type}) cr√©√© avec cl√©: {self.api_key[:5]}... et {len(self.endpoints)} endpoints.")

    async def initialize(self):
        print(f"Brain {self.brain_id} initialized.")
    
    async def participate_in_coding_challenge(self, prompt):
        # Simule une g√©n√©ration de code en utilisant les d√©tails du cerveau
        await asyncio.sleep(random.uniform(1, 5)) # Simule le temps de traitement
        if random.random() < 0.1: # 10% de chance d'erreur
            return {"error": "Simulated error", "brain_id": self.brain_id}
        
        code = f"""# Code g√©n√©r√© par {self.brain_id} ({self.brain_type})
# Cl√© utilis√©e: {self.api_key[:5]}...
# Endpoints: {', '.join(self.endpoints[:2])}... ({len(self.endpoints)} total)
# D√©fi: {prompt.splitlines()[0]}
def solve_challenge():
    print("D√©fi r√©solu par {self.brain_id}!")
    return "Solution simul√©e"
"""
        return {"code": code, "brain_id": self.brain_id}

# La fonction create_brain doit maintenant accepter la configuration compl√®te du cerveau
def create_brain(brain_config: Dict[str, Any], telegram_client=None):
    # En production, cette fonction dans autonomous_brain.py cr√©erait la vraie instance du cerveau
    # en fonction de brain_config["type"] et lui passerait les autres d√©tails.
    return MockBrain(brain_config, telegram_client)

def log_message(message, level="info"):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] [{level.upper()}] {message}")

async def save_json(file_path, data):
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)
    print(f"JSON saved to {file_path}")

# --- Fin des Mocks ---


class CodingChallengeSystem:
    """
    Syst√®me de d√©fis de codage automatis√© pour les 7 cerveaux autonomes.
    G√©n√®re des d√©fis toutes les 15 minutes et fait participer tous les cerveaux.
    """
    def __init__(self, telegram_client=None):
        self.telegram_client = telegram_client
        self.telegram_memory = TelegramMemoryIntegration(telegram_client)
        self.brains = {}
        self.challenge_history = []
        self.is_running = False
        
        # --- MODIFICATION MAJEURE ICI ---
        # Au lieu d'une liste de types, nous avons une liste de configurations de cerveaux.
        # Chaque configuration repr√©sente une instance de cerveau unique avec sa cl√© et ses endpoints.
        # C'EST ICI QUE VOUS METTREZ VOS 31 CL√âS ET 90 ENDPOINTS.
        # Par exemple, si vous avez 31 cl√©s, vous aurez 31 entr√©es dans cette liste.
        # Si un cerveau utilise plusieurs endpoints, ils seront list√©s dans 'endpoints'.
        self.brain_configs = self._generate_example_brain_configs() # Utilise un g√©n√©rateur d'exemples
        
    def _generate_example_brain_configs(self) -> List[Dict[str, Any]]:
        """
        G√©n√®re une liste d'exemples de configurations de cerveaux.
        EN PRODUCTION, CETTE LISTE SERAIT CHARG√âE DEPUIS config.py OU brain_library.py
        ET CONTENDRIAIT VOS VRAIES 31 CL√âS ET 90 ENDPOINTS.
        """
        example_configs = []
        brain_base_types = ["GEMINI", "DEEPSEEK", "HUGGINGFACE", "TAVILY", "SERPER", "GOOGLE_CUSTOM_SEARCH", "WOLFRAMALPHA"]
        
        # Pour simuler 31 cl√©s et 90 endpoints, nous allons cr√©er plus d'instances de cerveaux.
        # Chaque "cerveau" dans cette liste aura un ID unique et sa propre cl√©/endpoints.
        
        # Exemple: 5 instances GEMINI, chacune avec une cl√© et des endpoints diff√©rents
        for i in range(5):
            example_configs.append({
                "id": f"GEMINI_instance_{i+1}",
                "type": "GEMINI",
                "api_key": f"GEMINI_KEY_{i+1}_ABCDEFGH",
                "endpoints": [f"gemini_ep_{j+1}" for j in range(random.randint(2, 5))] # Ex: 2 √† 5 endpoints par instance
            })
        
        # Exemple: 4 instances DEEPSEEK
        for i in range(4):
            example_configs.append({
                "id": f"DEEPSEEK_instance_{i+1}",
                "type": "DEEPSEEK",
                "api_key": f"DEEPSEEK_KEY_{i+1}_IJKLMNOP",
                "endpoints": [f"deepseek_ep_{j+1}" for j in range(random.randint(1, 3))]
            })

        # Pour atteindre 31 cl√©s et 90 endpoints, vous devrez √©tendre cette logique.
        # Voici une approche g√©n√©rique pour simuler le reste:
        remaining_keys = 31 - (5 + 4) # 22 cl√©s restantes
        endpoint_counter = 0

        for i in range(remaining_keys):
            # Assigner al√©atoirement un type de cerveau de base
            brain_type = random.choice(brain_base_types)
            num_endpoints = random.randint(1, 4) # Chaque cl√© aura 1 √† 4 endpoints

            endpoints_for_this_brain = []
            for _ in range(num_endpoints):
                endpoints_for_this_brain.append(f"generic_ep_{endpoint_counter}")
                endpoint_counter += 1

            example_configs.append({
                "id": f"{brain_type}_instance_{i+10}", # Pour √©viter les conflits d'ID
                "type": brain_type,
                "api_key": f"GENERIC_KEY_{i+10}_QRSTUVWX",
                "endpoints": endpoints_for_this_brain
            })
        
        # V√©rifiez que le nombre total d'endpoints est proche de 90 (ceci est un exemple, le v√¥tre sera pr√©cis)
        total_endpoints_in_example = sum(len(c["endpoints"]) for c in example_configs)
        print(f"Exemple g√©n√©r√©: {len(example_configs)} cerveaux, environ {total_endpoints_in_example} endpoints.")

        return example_configs

    async def initialize(self):
        """Initialise tous les cerveaux pour les d√©fis."""
        try:
            # --- MODIFICATION ICI : It√©rer sur les configurations d√©taill√©es ---
            for brain_config in self.brain_configs:
                # create_brain doit maintenant accepter le dictionnaire de configuration complet
                self.brains[brain_config["id"]] = create_brain(brain_config, self.telegram_client)
                await self.brains[brain_config["id"]].initialize()
                
            await self.telegram_memory.write_to_group(
                f"üß† Syst√®me de d√©fis de codage initialis√© - {len(self.brains)} cerveaux pr√™ts",
                "SYSTEM_INIT"
            )
            log_message("Syst√®me de d√©fis de codage initialis√© avec succ√®s")
            return True
            
        except Exception as e:
            log_message(f"Erreur initialisation syst√®me d√©fis: {e}", level="error")
            return False
    
    async def start_periodic_challenges(self):
        """D√©marre les d√©fis p√©riodiques automatis√©s."""
        self.is_running = True
        await self.telegram_memory.write_to_group(
            "üöÄ D√©fis de codage automatis√©s d√©marr√©s - Intervalle: 15 minutes",
            "SYSTEM_START"
        )
        
        while self.is_running:
            try:
                await self.run_coding_challenge()
                await asyncio.sleep(config.CODING_CHALLENGE_INTERVAL_SECONDS)
            except Exception as e:
                log_message(f"Erreur dans la boucle de d√©fis: {e}", level="error")
                await asyncio.sleep(60)  # Attendre 1 minute en cas d'erreur
    
    def stop_challenges(self):
        """Arr√™te les d√©fis p√©riodiques."""
        self.is_running = False
        log_message("Syst√®me de d√©fis de codage arr√™t√©")
    
    def generate_challenge_prompt(self) -> str:
        """G√©n√®re un prompt de d√©fi de codage al√©atoire."""
        challenge_types = [
            "ALGORITHME",
            "OPTIMISATION", 
            "DEBUG",
            "IA_CREATIVE",
            "SCRIPT_UTILE",
            "STRUCTURE_DONNEES",
            "RESOLUTION_PROBLEME",
            "CODE_GOLF"
        ]
        
        algorithms = [
            "tri fusion", "recherche binaire", "parcours graphe", "programmation dynamique",
            "arbre binaire", "table de hachage", "pile et file", "r√©cursivit√©",
            "backtracking", "greedy algorithm", "dijkstra", "kruskal"
        ]
        
        domains = [
            "traitement de donn√©es", "analyse statistique", "manipulation de fichiers",
            "interfaces utilisateur", "API REST", "base de donn√©es", "machine learning",
            "traitement d'images", "traitement de texte", "cryptographie", "jeux",
            "automatisation", "web scraping", "calculs math√©matiques"
        ]
        
        challenge_type = random.choice(challenge_types)
        algorithm = random.choice(algorithms)
        domain = random.choice(domains)
        
        prompts = {
            "ALGORITHME": f"""
D√©fi Algorithme - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Impl√©mente un algorithme de {algorithm} optimis√© pour {domain}.

Exigences:
- Code Python clair et efficace
- Complexit√© temporelle O(n log n) maximum
- Gestion des cas limites
- Tests unitaires inclus
- Documentation compl√®te

Contraintes:
- Maximum 150 lignes de code
- Utilisation de structures de donn√©es appropri√©es
- Code pr√™t pour la production

G√©n√®re du code Python fonctionnel et optimis√©.
""",
            "OPTIMISATION": f"""
D√©fi Optimisation - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Optimise ce code pour {domain} en utilisant {algorithm}:

```python
def slow_function(data):
    result = []
    for i in range(len(data)):
        for j in range(len(data)):
            if data[i] > data[j]:
                result.append((i, j))
    return result


Objectifs:

R√©duire la complexit√© temporelle

Minimiser l'usage m√©moire

Am√©liorer la lisibilit√©

Maintenir la fonctionnalit√©

Fournis le code optimis√© avec explications des am√©liorations.
""",
"DEBUG": f"""
D√©fi Debug - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Trouve et corrige les bugs dans ce code pour {domain}:

def process_data(items):
    result = {{}}
    for i, item in enumerate(items):
        if item % 2 == 0:
            result[i] = item * 2
        else:
            result[i] = item / 2
    return result

data = [1, 2, 3, 4, 5]
print(process_data(data))

T√¢ches:

Identifier tous les bugs

Corriger le code

Ajouter la gestion d'erreurs

Am√©liorer la robustesse

Fournis le code corrig√© et fonctionnel.
""",
"IA_CREATIVE": f"""
D√©fi IA Cr√©ative - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Cr√©e un syst√®me intelligent pour {domain} utilisant {algorithm}.

Fonctionnalit√©s requises:

Apprentissage adaptatif

Pr√©dictions pr√©cises

Interface intuitive

Visualisation des r√©sultats

Sp√©cifications:

Code modulaire et extensible

Documentation technique

Exemples d'utilisation

Tests de validation

G√©n√®re un syst√®me IA complet et innovant.
""",
"SCRIPT_UTILE": f"""
D√©fi Script Utile - {datetime.now().strftime('%Y-%m-%d %H:%M')}

D√©veloppe un script pratique pour automatiser {domain}.

Caract√©ristiques:

Interface en ligne de commande

Configuration par fichier

Logging d√©taill√©

Gestion d'erreurs robuste

Fonctionnalit√©s:

Traitement par lots

Sauvegarde automatique

Rapports de progression

Mode debug

Cr√©e un outil pr√™t √† l'emploi et professionnel.
""",
"STRUCTURE_DONNEES": f"""
D√©fi Structure de Donn√©es - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Impl√©mente une structure de donn√©es avanc√©e pour {domain}.

Op√©rations requises:

Insertion O(log n)

Recherche O(log n)

Suppression O(log n)

Parcours efficace

Bonus:

S√©rialisation/d√©s√©rialisation

Op√©rations de masse

Thread-safety

Visualisation

Fournis une impl√©mentation compl√®te et test√©e.
""",
"RESOLUTION_PROBLEME": f"""
D√©fi R√©solution de Probl√®me - {datetime.now().strftime('%Y-%m-%d %H:%M')}

R√©sous ce probl√®me complexe pour {domain}:

Probl√®me: Tu as une liste de t√¢ches avec des d√©pendances. Chaque t√¢che a une dur√©e et des pr√©requis.
Trouve l'ordre d'ex√©cution optimal qui minimise le temps total tout en respectant les contraintes.

Contraintes:

Maximum 3 t√¢ches en parall√®le

Certaines t√¢ches sont critiques (priorit√© haute)

Gestion des conflits de ressources

Fournis l'algorithme de planification optimal.
""",
"CODE_GOLF": f"""
D√©fi Code Golf - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Impl√©mente {algorithm} en moins de 50 caract√®res Python.

R√®gles:

Fonctionnalit√© compl√®te pr√©serv√©e

Code lisible malgr√© la concision

Pas de caract√®res Unicode exotiques

Commentaire explicatif obligatoire

Objectif: Code le plus court possible tout en restant pythonique.
"""
}
        return prompts.get(challenge_type, prompts["ALGORITHME"])

    async def run_coding_challenge(self):
        """Ex√©cute un d√©fi de codage avec tous les cerveaux."""
        try:
            # G√©n√©ration du d√©fi
            challenge_prompt = self.generate_challenge_prompt()
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            await self.telegram_memory.write_to_group(
                f"üéØ NOUVEAU D√âFI DE CODAGE - {timestamp}\n\n{challenge_prompt}",
                "CHALLENGE_START"
            )
            
            log_message(f"Lancement du d√©fi de codage: {timestamp}")
            
            # Ex√©cution en parall√®le pour tous les cerveaux
            tasks = []
            # --- MODIFICATION ICI : It√©rer sur les cerveaux d√©j√† initialis√©s ---
            for brain_id, brain_instance in self.brains.items():
                task = asyncio.create_task(
                    self._brain_challenge_task(brain_instance, challenge_prompt, timestamp)
                )
                tasks.append((brain_id, task))
            
            # Attente des r√©sultats
            results = {}
            for brain_id, task in tasks:
                try:
                    result = await asyncio.wait_for(task, timeout=300)  # 5 minutes max
                    results[brain_id] = result
                except asyncio.TimeoutError:
                    results[brain_id] = {"error": "Timeout", "brain_id": brain_id}
                    await self.telegram_memory.log_error(brain_id, "Timeout d√©fi de codage")
                except Exception as e:
                    results[brain_id] = {"error": str(e), "brain_id": brain_id}
                    await self.telegram_memory.log_error(brain_id, f"Erreur d√©fi: {e}")
            
            # Sauvegarde et analyse des r√©sultats
            await self._save_challenge_results(challenge_prompt, results, timestamp)
            await self._analyze_and_report_results(results, timestamp)
            
            # Mise √† jour de l'historique
            self.challenge_history.append({
                "timestamp": timestamp,
                "challenge": challenge_prompt[:100] + "...",
                "participants": len(results),
                "successful": len([r for r in results.values() if "error" not in r])
            })
            
            # Limitation de l'historique
            if len(self.challenge_history) > 50:
                self.challenge_history = self.challenge_history[-50:]
            
        except Exception as e:
            log_message(f"Erreur d√©fi de codage: {e}", level="error")
            await self.telegram_memory.log_error("SYSTEM", f"Erreur d√©fi global: {e}")

    async def _brain_challenge_task(self, brain: Any, challenge_prompt: str, timestamp: str) -> Dict[str, Any]:
        """T√¢che de d√©fi pour un cerveau sp√©cifique."""
        try:
            # Le cerveau participe au d√©fi
            result = await brain.participate_in_coding_challenge(challenge_prompt)
            
            if "error" not in result and "code" in result:
                # Sauvegarde du code g√©n√©r√©
                code_filename = config.DAILY_CHALLENGE_PATH / f"challenge_{brain.brain_id}_{timestamp}.py"
                
                header = f"""# -*- coding: utf-8 -*-
# D√©fi de codage automatis√©
# Cerveau: {brain.brain_id}
# Timestamp: {timestamp}
# Challenge: {challenge_prompt[:100]}...

"""
                try:
                    with open(code_filename, "w", encoding="utf-8") as f:
                        f.write(header + result["code"])
                    
                    result["saved_file"] = str(code_filename)
                    log_message(f"Code sauvegard√©: {code_filename}")
                    
                except Exception as e:
                    log_message(f"Erreur sauvegarde {brain.brain_id}: {e}", level="error")
            
            return result
            
        except Exception as e:
            return {"error": str(e), "brain_id": brain.brain_id}

    async def _save_challenge_results(self, challenge_prompt: str, results: Dict, timestamp: str):
        """Sauvegarde tous les r√©sultats du d√©fi."""
        try:
            challenge_data = {
                "timestamp": timestamp,
                "challenge_prompt": challenge_prompt,
                "results": results,
                "summary": {
                    "total_participants": len(results),
                    "successful_responses": len([r for r in results.values() if "error" not in r]),
                    "failed_responses": len([r for r in results.values() if "error" in r]),
                    "generated_files": [r.get("saved_file") for r in results.values() if r.get("saved_file")]
                }
            }
            
            results_file = config.DAILY_CHALLENGE_PATH / f"challenge_results_{timestamp}.json"
            await save_json(results_file, challenge_data)
            
            log_message(f"R√©sultats du d√©fi sauvegard√©s: {results_file}")
            
        except Exception as e:
            log_message(f"Erreur sauvegarde r√©sultats: {e}", level="error")

    async def _analyze_and_report_results(self, results: Dict, timestamp: str):
        """Analyse et rapporte les r√©sultats dans le groupe Telegram."""
        try:
            successful = [r for r in results.values() if "error" not in r]
            failed = [r for r in results.values() if "error" in r]
            
            report = f"""
üìä RAPPORT D√âFI DE CODAGE - {timestamp}

‚úÖ Succ√®s: {len(successful)}/{len(results)} cerveaux
‚ùå √âchecs: {len(failed)}/{len(results)} cerveaux

=== D√âTAILS SUCC√àS ===
"""
            for result in successful:
                brain_id = result.get("brain_id", "Inconnu")
                code_length = len(result.get("code", ""))
                report += f"‚Ä¢ {brain_id}: {code_length} caract√®res g√©n√©r√©s\n"
            
            if failed:
                report += "\n=== D√âTAILS √âCHECS ===\n"
                for result in failed:
                    brain_id = result.get("brain_id", "Inconnu")
                    error = result.get("error", "Erreur inconnue")
                    report += f"‚Ä¢ {brain_id}: {error}\n"
            
            # S√©lection du meilleur code
            if successful:
                best_result = max(successful, key=lambda x: len(x.get("code", "")))
                best_brain = best_result.get("brain_id", "Inconnu")
                best_code = best_result.get("code", "")[:500]
                
                report += f"\nüèÜ MEILLEUR CODE - {best_brain}:\n```python\n{best_code}...\n```"
            
            await self.telegram_memory.write_to_group(report, "CHALLENGE_REPORT")
            
        except Exception as e:
            log_message(f"Erreur analyse r√©sultats: {e}", level="error")

    def get_challenge_statistics(self) -> Dict[str, Any]:
        """Retourne les statistiques des d√©fis."""
        if not self.challenge_history:
            return {"total_challenges": 0}
        
        total_challenges = len(self.challenge_history)
        total_participants = sum(c["participants"] for c in self.challenge_history)
        total_successful = sum(c["successful"] for c in self.challenge_history)
        
        avg_success_rate = (total_successful / total_participants * 100) if total_participants > 0 else 0
        
        return {
            "total_challenges": total_challenges,
            "total_participants": total_participants,
            "total_successful": total_successful,
            "average_success_rate": round(avg_success_rate, 2),
            "last_challenge": self.challenge_history[-1] if self.challenge_history else None,
            "is_running": self.is_running
        }

# Fonctions utilitaires pour le syst√®me de d√©fis

def diff_text(old_text: str, new_text: str) -> str:
    """G√©n√®re un diff unifi√© entre deux textes."""
    diff = difflib.unified_diff(
        old_text.splitlines(),
        new_text.splitlines(),
        lineterm=""
    )
    return "\n".join(diff)

def analyze_python_code(code: str) -> Dict[str, Any]:
    """Analyse un code Python et retourne des m√©triques."""
    try:
        lines = code.split('\n')
        non_empty_lines = [line for line in lines if line.strip()]

        # M√©triques basiques
        metrics = {
            "total_lines": len(lines),
            "code_lines": len(non_empty_lines),
            "comment_lines": len([line for line in lines if line.strip().startswith('#')]),
            "blank_lines": len(lines) - len(non_empty_lines),
            "functions": len([line for line in lines if line.strip().startswith('def ')]),
            "classes": len([line for line in lines if line.strip().startswith('class ')]),
            "imports": len([line for line in lines if line.strip().startswith(('import ', 'from '))]),
            "complexity_score": calculate_complexity(code)
        }
        
        # V√©rification syntaxique
        try:
            compile(code, '<string>', 'exec')
            metrics["syntax_valid"] = True
            metrics["syntax_error"] = None
        except SyntaxError as e:
            metrics["syntax_valid"] = False
            metrics["syntax_error"] = str(e)
        
        return metrics
        
    except Exception as e:
        return {"error": f"Erreur analyse: {e}"}

def calculate_complexity(code: str) -> int:
    """Calcule un score de complexit√© approximatif."""
    complexity_keywords = [
        'if', 'elif', 'else', 'for', 'while', 'try', 'except',
        'with', 'def', 'class', 'lambda', 'and', 'or'
    ]
    complexity = 1  # Base complexity
    for line in code.split('\n'):
        line = line.strip().lower()
        for keyword in complexity_keywords:
            if keyword in line:
                complexity += 1
    return complexity

def format_error(error: Exception) -> str:
    """Formate une erreur de mani√®re visuelle."""
    return f"""
‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è ERREUR ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
Type: {type(error).__name__}
Message: {str(error)}
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
"""

@lru_cache(maxsize=100)
def generate_code_cached(prompt: str, temperature: float = 0.7) -> str:
    """G√©n√®re du code avec cache pour les prompts r√©p√©t√©s."""
    # Cette fonction serait connect√©e √† un mod√®le IA en production
    return f"# Code g√©n√©r√© pour: {prompt[:50]}...\nprint('Code g√©n√©r√© avec cache')"

def batch_generate(prompts: List[str], max_workers: int = 4) -> List[str]:
    """G√©n√®re du code pour plusieurs prompts en parall√®le."""
    def _generate_single(prompt):
        return f"# Code g√©n√©r√© pour: {prompt[:50]}...\nprint('Code g√©n√©r√© en lot')"

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        return list(executor.map(_generate_single, prompts))

def adaptive_temp(prompt: str) -> float:
    """Adapte la temp√©rature selon le type de prompt."""
    technical_keywords = ["optimiser", "algorithme", "complexit√©", "performance", "debug"]
    return 0.3 if any(kw in prompt.lower() for kw in technical_keywords) else 0.7

def fix_common_errors(code: str) -> str:
    """Applique des corrections automatiques communes."""
    fixes = {
        "print(": "print(",
        "def ": "def ",
        "= =": "==",
        "elif ": "elif ",
        "esle:": "else:",
        "ture": "True",
        "flase": "False"
    }
    for error, fix in fixes.items():
        code = code.replace(error, fix)
    return code

def warmup_ai(model, iterations: int = 3):
    """Pr√©chauffe un mod√®le IA avec des requ√™tes factices."""
    dummy_prompts = ["print('hello')", "def test(): pass", "1+1"]
    for _ in range(iterations):
        for prompt in dummy_prompts:
            if hasattr(model, 'generate'):
                try:
                    model.generate(prompt)
                except:
                    pass # Ignore les erreurs de pr√©chauffage

# Instance globale du syst√®me de d√©fis

coding_challenge_system = None

def get_coding_challenge_system(telegram_client=None) -> CodingChallengeSystem:
    """Retourne l'instance globale du syst√®me de d√©fis."""
    global coding_challenge_system
    if coding_challenge_system is None:
        coding_challenge_system = CodingChallengeSystem(telegram_client)
    return coding_challenge_system

import asyncio
import hashlib
import io
import re
import time
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any
from urllib.parse import urlparse
import httpx

from config import config
from brain_library import TelegramMemoryIntegration
from utils import log_message

class URLDefanger:
    """
    Neutralise les URLs pour emp√™cher les clics accidentels
    et bloque les trackers connus.
    """
    def __init__(self, mode: str = "secure"):
        self.mode = mode
        self.url_pattern = re.compile(r'https?://[^\s\]]+')
        self.tracker_domains = [
            "doubleclick.net", "googleadservices.com", "googlesyndication.com",
            "facebook.com/tr", "analytics.google.com", "hotjar.com",
            "mouseflow.com", "crazyegg.com", "fullstory.com"
        ]
    
    def _generate_hash(self, url: str) -> str:
        """G√©n√®re un identifiant unique pour l'URL."""
        return hashlib.sha256(url.encode()).hexdigest()[:8]
    
    def defang_url(self, url: str) -> str:
        """Transforme une URL en version s√©curis√©e."""
        # Bloque les trackers connus
        for tracker in self.tracker_domains:
            if tracker in url:
                return "[TRACKER_BLOQU√â]"
        
        if self.mode == "secure":
            return f"[URL_BLOQU√âE:#{self._generate_hash(url)}]"
        else:
            parsed = urlparse(url)
            return f"[URL:{parsed.netloc}/...#{self._generate_hash(url)}]"
    
    def defang_text(self, text: str) -> str:
        """Nettoie tout le contenu texte."""
        return self.url_pattern.sub(
            lambda m: self.defang_url(m.group(0)), 
            text
        )

class SecurePageArchiver:
    """
    T√©l√©charge, s√©curise et archive des pages web
    avec gestion des gros fichiers et protection anti-tracking.
    """
    def __init__(self, telegram_client=None):
        self.telegram_client = telegram_client
        self.telegram_memory = TelegramMemoryIntegration(telegram_client)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept-Language": "fr-FR,fr;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
    async def fetch_page(self, url: str) -> Optional[httpx.Response]:
        """T√©l√©charge une page avec gestion robuste des erreurs."""
        try:
            async with httpx.AsyncClient(
                timeout=30.0,
                headers=self.headers,
                follow_redirects=True,
                http2=True,
                limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
            ) as client:
                response = await client.get(url)
                
                # V√©rification de la taille du contenu
                content_length = response.headers.get('content-length')
                if content_length and int(content_length) > config.MAX_CHUNK_SIZE:
                    log_message(f"Contenu trop volumineux pour {url}: {content_length} bytes", level="warning")
                    return None
                
                response.raise_for_status()
                return response
                
        except httpx.HTTPStatusError as e:
            log_message(f"Erreur HTTP pour {url}: {e.response.status_code}", level="warning")
            return None
        except httpx.RequestError as e:
            log_message(f"Erreur r√©seau pour {url}: {e}", level="warning")
            return None
        except Exception as e:
            log_message(f"Erreur inattendue pour {url}: {e}", level="error")
            return None

    async def secure_content(self, url: str, content: str) -> str:
        """Applique les protections de s√©curit√© au contenu."""
        defanger = URLDefanger(mode="secure")
        
        header = f"""
‚ö†Ô∏è CONTENU ARCHIV√â - LIENS NEUTRALIS√âS ‚ö†Ô∏è
URL originale: {url}
Horodatage: {datetime.utcnow().isoformat()}Z
Taille originale: {len(content)} caract√®res
S√©curis√© par: SecurePageArchiver v2.0
=====================================

"""
        
        # Neutralisation des URLs
        secured_content = defanger.defang_text(content)
        
        # Suppression des scripts malveillants
        secured_content = re.sub(r'<script[^>]*>.*?</script>', '[SCRIPT_SUPPRIM√â]', secured_content, flags=re.DOTALL | re.IGNORECASE)
        secured_content = re.sub(r'javascript:[^"\']*', '[JAVASCRIPT_BLOQU√â]', secured_content, flags=re.IGNORECASE)
        secured_content = re.sub(r'on\w+\s*=\s*["\'][^"\']*["\']', '[EVENT_HANDLER_BLOQU√â]', secured_content, flags=re.IGNORECASE)
        
        return header + secured_content

    async def send_chunk(self, chunk: str, url: str, user_id: str, chunk_index: int) -> bool:
        """Envoie un fragment de contenu s√©curis√© via Telegram."""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"SAFE_{user_id}_{timestamp}_p{chunk_index:03d}.txt"
            
            # Limitation de la taille du chunk pour Telegram
            if len(chunk) > 4000:  # Limite Telegram pour les messages
                chunk = chunk[:4000] + "\n\n[CONTENU_TRONQU√â]"
            
            # Envoi dans le groupe priv√©
            await self.telegram_memory.write_to_group(
                f"üìÑ Fragment {chunk_index+1} | {url[:50]}...\n\n{chunk}",
                "ARCHIVE_CHUNK"
            )
            
            log_message(f"Fragment {chunk_index} envoy√© pour {url}")
            return True
            
        except Exception as e:
            log_message(f"Erreur envoi fragment {chunk_index}: {e}", level="error")
            return False

    async def archive_single_page(self, url: str, user_id: str) -> Dict[str, Any]:
        """Archive une seule page web."""
        start_time = time.time()
        result = {
            "url": url,
            "user_id": user_id,
            "timestamp": datetime.now().isoformat(),
            "success": False,
            "chunks_sent": 0,
            "total_size": 0,
            "processing_time": 0,
            "error": None
        }
        
        try:
            # Log du d√©but d'archivage
            await self.telegram_memory.log_brain_activity(
                "ARCHIVER",
                f"D√©but archivage: {url}",
                {"user_id": user_id}
            )
            
            # T√©l√©chargement de la page
            response = await self.fetch_page(url)
            if not response:
                result["error"] = "√âchec du t√©l√©chargement"
                return result
            
            # V√©rification du type de contenu
            content_type = response.headers.get('content-type', '').lower()
            if not any(ct in content_type for ct in ['text/html', 'text/plain', 'application/json']):
                result["error"] = f"Type de contenu non support√©: {content_type}"
                return result
            
            # S√©curisation du contenu
            raw_content = response.text
            result["total_size"] = len(raw_content)
            
            secured_content = await self.secure_content(url, raw_content)
            
            # D√©coupage en chunks
            chunks = []
            chunk_size = config.MAX_CHUNK_SIZE // 2  # Plus petit pour la s√©curit√© Telegram
            
            for i in range(0, len(secured_content), chunk_size):
                chunk = secured_content[i:i + chunk_size]
                chunks.append(chunk)
            
            # Envoi des chunks
            successful_chunks = 0
            for i, chunk in enumerate(chunks):
                if await self.send_chunk(chunk, url, user_id, i):
                    successful_chunks += 1
                    await asyncio.sleep(0.5)  # √âviter le rate limiting Telegram
                else:
                    break
            
            result["chunks_sent"] = successful_chunks
            result["success"] = successful_chunks > 0
            
            # Log de fin
            processing_time = time.time() - start_time
            result["processing_time"] = processing_time
            
            await self.telegram_memory.log_success(
                "ARCHIVER",
                f"Page archiv√©e: {url}",
                f"{successful_chunks} fragments envoy√©s"
            )
            
        except Exception as e:
            result["error"] = str(e)
            await self.telegram_memory.log_error("ARCHIVER", f"Erreur archivage {url}: {e}")
        
        return result

class ArchiveCoordinator:
    """
    Coordinateur pour l'archivage de multiples pages en parall√®le.
    """
    def __init__(self, telegram_client=None, max_concurrent: int = 3):
        self.archiver = SecurePageArchiver(telegram_client)
        self.max_concurrent = max_concurrent
        self.telegram_memory = TelegramMemoryIntegration(telegram_client)
        
    async def archive_multiple_pages(self, links: List[str], user_id: str) -> Dict[str, Any]:
        """Archive plusieurs pages en parall√®le avec limitation de concurrence."""
        start_time = time.time()
        
        await self.telegram_memory.write_to_group(
            f"üöÄ D√©but archivage de {len(links)} pages pour l'utilisateur {user_id}",
            "ARCHIVE_START"
        )
        
        # Limitation du nombre de liens
        if len(links) > 10:
            links = links[:10]
            await self.telegram_memory.write_to_group(
                "‚ö†Ô∏è Limitation appliqu√©e: maximum 10 liens par session",
                "ARCHIVE_LIMIT"
            )
        
        # Semaphore pour limiter la concurrence
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def archive_with_semaphore(url: str) -> Dict[str, Any]:
            async with semaphore:
                return await self.archiver.archive_single_page(url, user_id)
        
        # Ex√©cution en parall√®le
        tasks = [archive_with_semaphore(url) for url in links]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Compilation des r√©sultats
        summary = {
            "total_links": len(links),
            "successful": 0,
            "failed": 0,
            "total_chunks": 0,
            "total_size": 0,
            "processing_time": time.time() - start_time,
            "results": []
        }
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                error_result = {
                    "url": links[i] if i < len(links) else "Unknown",
                    "success": False,
                    "error": str(result)
                }
                summary["results"].append(error_result)
                summary["failed"] += 1
            else:
                summary["results"].append(result)
                if result["success"]:
                    summary["successful"] += 1
                    summary["total_chunks"] += result["chunks_sent"]
                    summary["total_size"] += result["total_size"]
                else:
                    summary["failed"] += 1
        
        # Rapport final
        await self.telegram_memory.write_to_group(
            f"""
üìä RAPPORT D'ARCHIVAGE TERMIN√â

üë§ Utilisateur: {user_id}
üìä Statistiques:
  ‚Ä¢ Total: {summary['total_links']} liens
  ‚Ä¢ Succ√®s: {summary['successful']} pages
  ‚Ä¢ √âchecs: {summary['failed']} pages
  ‚Ä¢ Fragments: {summary['total_chunks']} envoy√©s
  ‚Ä¢ Taille: {summary['total_size']:,} caract√®res
  ‚Ä¢ Dur√©e: {summary['processing_time']:.2f}s

‚úÖ Toutes les pages sont s√©curis√©es et archiv√©es dans ce groupe.
""",
            "ARCHIVE_COMPLETE"
        )
        
        return summary

# Fonction principale pour l'archivage (utilis√©e par les outils)
async def fetch_and_archive_pages(links: List[str], user_id: str, context=None) -> Dict[str, Any]:
    """
    Fonction principale pour t√©l√©charger, s√©curiser et archiver des pages web.
    
    Args:
        links: Liste des URLs √† archiver
        user_id: Identifiant de l'utilisateur
        context: Contexte (non utilis√©, pour compatibilit√©)
    
    Returns:
        Dict contenant le r√©sum√© de l'archivage
    """
    try:
        # Import du client Telegram depuis les instances
        from app_clients_instances import telegram_bot_client
        
        coordinator = ArchiveCoordinator(telegram_bot_client, max_concurrent=3)
        summary = await coordinator.archive_multiple_pages(links, user_id)
        
        return {
            "tool_output": f"‚úÖ Archivage termin√©: {summary['successful']}/{summary['total_links']} pages archiv√©es avec succ√®s. {summary['total_chunks']} fragments envoy√©s dans le groupe priv√©.",
            "summary": summary
        }
        
    except Exception as e:
        error_msg = f"‚ùå Erreur syst√®me d'archivage: {e}"
        log_message(f"Erreur fetch_and_archive_pages: {e}", level="error")
        return {
            "tool_output": error_msg,
            "error": str(e)
        }

# Classes utilitaires suppl√©mentaires

class ContentAnalyzer:
    """Analyse le contenu des pages archiv√©es."""
    
    @staticmethod
    def extract_metadata(content: str, url: str) -> Dict[str, Any]:
        """Extrait les m√©tadonn√©es d'une page."""
        metadata = {
            "url": url,
            "timestamp": datetime.now().isoformat(),
            "size": len(content),
            "title": "",
            "description": "",
            "language": "unknown",
            "charset": "unknown"
        }
        
        # Extraction du titre
        title_match = re.search(r'<title[^>]*>(.*?)</title>', content, re.IGNORECASE | re.DOTALL)
        if title_match:
            metadata["title"] = title_match.group(1).strip()[:200]
        
        # Extraction de la description
        desc_match = re.search(r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']', content, re.IGNORECASE)
        if desc_match:
            metadata["description"] = desc_match.group(1).strip()[:500]
        
        # Extraction de la langue
        lang_match = re.search(r'<html[^>]*lang=["\']([^"\']*)["\']', content, re.IGNORECASE)
        if lang_match:
            metadata["language"] = lang_match.group(1).strip()
        
        return metadata
    
    @staticmethod
    def count_elements(content: str) -> Dict[str, int]:
        """Compte les √©l√©ments HTML dans le contenu."""
        elements = {
            "links": len(re.findall(r'<a[^>]*href=', content, re.IGNORECASE)),
            "images": len(re.findall(r'<img[^>]*src=', content, re.IGNORECASE)),
            "scripts": len(re.findall(r'<script[^>]*>', content, re.IGNORECASE)),
            "forms": len(re.findall(r'<form[^>]*>', content, re.IGNORECASE)),
            "paragraphs": len(re.findall(r'<p[^>]*>', content, re.IGNORECASE)),
            "headings": len(re.findall(r'<h[1-6][^>]*>', content, re.IGNORECASE))
        }
        
        return elements

class ArchiveStorage:
    """Gestionnaire de stockage pour les archives."""
    
    def __init__(self):
        self.archive_dir = config.BASE_DIR / "archives"
        self.archive_dir.mkdir(exist_ok=True)
        
    async def save_archive_metadata(self, user_id: str, summary: Dict[str, Any]):
        """Sauvegarde les m√©tadonn√©es d'archivage."""
        try:
            metadata_file = self.archive_dir / f"archive_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            # Import de save_json depuis utils
            from utils import save_json
            await save_json(metadata_file, summary)
            
            log_message(f"M√©tadonn√©es d'archivage sauvegard√©es: {metadata_file}")
            
        except Exception as e:
            log_message(f"Erreur sauvegarde m√©tadonn√©es: {e}", level="error")
    
    def get_user_archives(self, user_id: str) -> List[Path]:
        """R√©cup√®re la liste des archives d'un utilisateur."""
        pattern = f"archive_{user_id}_*.json"
        return list(self.archive_dir.glob(pattern))
    
    def cleanup_old_archives(self, days: int = 30):
        """Nettoie les anciennes archives."""
        cutoff_time = time.time() - (days * 24 * 60 * 60)
        
        for archive_file in self.archive_dir.glob("archive_*.json"):
            if archive_file.stat().st_mtime < cutoff_time:
                try:
                    archive_file.unlink()
                    log_message(f"Archive supprim√©e: {archive_file}")
                except Exception as e:
                    log_message(f"Erreur suppression archive {archive_file}: {e}", level="error")

# Instance globale du stockage
archive_storage = ArchiveStorage()

import asyncio
import json
import re
import base64
import traceback
from typing import Dict, Any, List, Optional, Union, Tuple

from app_singletons import endpoint_health_manager, quota_manager
from config import config
from utils import log_message, neutralize_urls, find_tool_by_name

# Import des clients API optimis√© pour √©viter les d√©pendances circulaires
def get_api_clients():
    """Import dynamique des clients API."""
    try:
        from app_clients_instances import (
            webcontainer_client, ocr_client, deepseek_client, serper_client,
            wolfram_alpha_client, tavily_client, apiflash_client, crawlbase_client,
            detect_language_client, guardian_client, ip2location_client, shodan_client,
            weather_api_client, cloudmersive_client, greynoise_client, pulsedive_client,
            stormglass_client, loginradius_client, jsonbin_client, huggingface_client,
            twilio_client, abstractapi_client, google_custom_search_client,
            randommer_client, tomorrow_io_client, openweathermap_client, mockaroo_client,
            openpagerank_client, rapidapi_client, telegram_bot_client
        )
        return {
            'webcontainer': webcontainer_client,
            'ocr': ocr_client,
            'deepseek': deepseek_client,
            'serper': serper_client,
            'wolfram': wolfram_alpha_client,
            'tavily': tavily_client,
            'apiflash': apiflash_client,
            'crawlbase': crawlbase_client,
            'detect_language': detect_language_client,
            'guardian': guardian_client,
            'ip2location': ip2location_client,
            'shodan': shodan_client,
            'weather_api': weather_api_client,
            'cloudmersive': cloudmersive_client,
            'greynoise': greynoise_client,
            'pulsedive': pulsedive_client,
            'stormglass': stormglass_client,
            'loginradius': loginradius_client,
            'jsonbin': jsonbin_client,
            'huggingface': huggingface_client,
            'twilio': twilio_client,
            'abstractapi': abstractapi_client,
            'google_search': google_custom_search_client,
            'randommer': randommer_client,
            'tomorrow_io': tomorrow_io_client,
            'openweathermap': openweathermap_client,
            'mockaroo': mockaroo_client,
            'openpagerank': openpagerank_client,
            'rapidapi': rapidapi_client,
            'telegram': telegram_bot_client
        }
    except ImportError as e:
        log_message(f"Erreur import clients API: {e}", level="warning")
        return {}

async def execute_tool(tool_name: str, context: Any = None, **kwargs) -> Dict[str, Any]:
    """
    Ex√©cute un outil sp√©cifique en fonction de son nom et des arguments fournis.
    Compatible avec les 7 cerveaux autonomes.
    """
    log_message(f"Ex√©cution de l'outil: {tool_name} avec kwargs: {kwargs}")
    tool_config_info = find_tool_by_name(tool_name)

    if not tool_config_info:
        log_message(f"Outil non trouv√©: {tool_name}", level="error")
        return {"tool_name": tool_name, "tool_args": kwargs, "tool_output": f"Erreur: Outil '{tool_name}' non trouv√© ou non configur√©."}

    result = ""
    try:
        # Routage vers les fonctions d'outils sp√©cifiques
        if tool_name == "google_search":
            result = await google_search_tool(kwargs.get("queries"))
        elif tool_name == "media_control":
            result = await media_control_tool(
                action=kwargs.get("action"),
                position=kwargs.get("position"),
                offset=kwargs.get("offset")
            )
        elif tool_name == "clock":
            result = await clock_tool(**kwargs)
        elif tool_name == "ocr_space":
            result = await ocr_space_tool(kwargs.get("image_base64"))
        elif tool_name == "deepseek_chat":
            result = await deepseek_chat_tool(kwargs.get("prompt"), kwargs.get("model"))
        elif tool_name == "serper_dev":
            result = await serper_dev_tool(kwargs.get("query_text"))
        elif tool_name == "wolfram_alpha":
            result = await wolfram_alpha_tool(kwargs.get("input_text"))
        elif tool_name == "tavily_search":
            result = await tavily_search_tool(kwargs.get("query_text"), kwargs.get("max_results"))
        elif tool_name == "apiflash_screenshot":
            result = await apiflash_screenshot_tool(kwargs.get("url"))
        elif tool_name == "crawlbase_scraper":
            result = await crawlbase_scraper_tool(kwargs.get("url"), kwargs.get("use_js"))
        elif tool_name == "detect_language":
            result = await detect_language_tool(kwargs.get("text"))
        elif tool_name == "guardian_news":
            result = await guardian_news_tool(kwargs.get("query_text"))
        elif tool_name == "ip2location":
            result = await ip2location_tool(kwargs.get("ip_address"))
        elif tool_name == "shodan":
            result = await shodan_tool(kwargs.get("query_text"))
        elif tool_name == "weather_api":
            result = await weather_api_tool(kwargs.get("location"))
        elif tool_name == "cloudmersive_domain":
            result = await cloudmersive_domain_tool(kwargs.get("domain"))
        elif tool_name == "greynoise":
            result = await greynoise_tool(kwargs.get("ip_address"))
        elif tool_name == "pulsedive":
            result = await pulsedive_tool(kwargs.get("indicator"), kwargs.get("type"))
        elif tool_name == "stormglass":
            result = await stormglass_tool(kwargs.get("lat"), kwargs.get("lng"), kwargs.get("params"))
        elif tool_name == "loginradius_ping":
            result = await loginradius_ping_tool()
        elif tool_name == "jsonbin_io":
            result = await jsonbin_io_tool(kwargs.get("data"), kwargs.get("private"), kwargs.get("bin_id"))
        elif tool_name == "huggingface_inference":
            result = await huggingface_inference_tool(kwargs.get("model_name"), kwargs.get("input_text"))
        elif tool_name == "twilio_balance":
            result = await twilio_balance_tool()
        elif tool_name == "abstractapi":
            result = await abstractapi_tool(kwargs.get("input_value"), kwargs.get("api_type"))
        elif tool_name == "google_custom_search":
            result = await google_custom_search_tool(kwargs.get("query_text"))
        elif tool_name == "randommer_phone":
            result = await randommer_phone_tool(kwargs.get("country_code"), kwargs.get("quantity"))
        elif tool_name == "tomorrow_io_weather":
            result = await tomorrow_io_weather_tool(kwargs.get("location"), kwargs.get("fields"))
        elif tool_name == "openweathermap_weather":
            result = await openweathermap_weather_tool(kwargs.get("location"))
        elif tool_name == "mockaroo_data":
            result = await mockaroo_data_tool(kwargs.get("count"), kwargs.get("fields_json"))
        elif tool_name == "openpagerank":
            result = await openpagerank_tool(kwargs.get("domains"))
        elif tool_name == "rapidapi":
            result = await rapidapi_tool(kwargs.get("api_name"), **kwargs.get("api_kwargs", {}))
        elif tool_name == "run_in_sandbox":
            result = await run_in_sandbox_tool(kwargs.get("code"), kwargs.get("language"))
        elif tool_name == "webcontainer_sandbox":
            result = await webcontainer_sandbox_tool(kwargs.get("code"), kwargs.get("language"))
        elif tool_name == "fetch_and_archive_pages":
            # Import dynamique pour √©viter les d√©pendances circulaires
            from security_archiver import fetch_and_archive_pages
            result = await fetch_and_archive_pages(kwargs.get("links"), kwargs.get("user_id"), context=context)
        else:
            log_message(f"Aucun gestionnaire d'outil d√©fini pour: {tool_name}", level="error")
            result = f"Erreur: Aucun gestionnaire d'outil d√©fini pour '{tool_name}'."

    except Exception as e:
        log_message(f"Erreur lors de l'ex√©cution de l'outil {tool_name}: {e}", level="error")
        log_message(f"Traceback: {traceback.format_exc()}", level="error")
        result = f"Erreur lors de l'ex√©cution de l'outil {tool_name}: {e}"

    return {"tool_name": tool_name, "tool_args": kwargs, "tool_output": result}

# --- Fonctions d'outils sp√©cifiques ---

async def google_search_tool(queries: List[str]) -> str:
    """Effectue une recherche Google en utilisant Serper ou Google Custom Search."""
    if not queries:
        return "Erreur: Aucune requ√™te fournie"
    
    api_clients = get_api_clients()
    results = []
    
    for query in queries:
        log_message(f"Recherche Google pour: {query}")
        
        # Prioriser Google Custom Search si disponible
        try:
            if 'google_search' in api_clients and api_clients['google_search']:
                response = await api_clients['google_search'].search(query)
            else:
                response = await api_clients['serper'].search(query)
            
            if isinstance(response, str):
                response = neutralize_urls(response)
            elif isinstance(response, dict) and response.get("error"):
                pass
            
            results.append(f"R√©sultat pour '{query}': {response}")
            
        except Exception as e:
            results.append(f"Erreur pour '{query}': {e}")
    
    return "\n".join(results)

async def media_control_tool(action: str, position: Optional[int] = None, offset: Optional[int] = None) -> str:
    """Contr√¥le la lecture multim√©dia (simulation)."""
    actions_map = {
        "like": "M√©dia actuel aim√©.",
        "dislike": "M√©dia actuel non aim√©.",
        "next": "Passage au m√©dia suivant.",
        "previous": "Passage au m√©dia pr√©c√©dent.",
        "pause": "M√©dia actuel mis en pause.",
        "resume": "Lecture du m√©dia reprise.",
        "stop": "M√©dia actuel arr√™t√©.",
        "replay": "M√©dia actuel rejou√©.",
        "seek_absolute": f"M√©dia avanc√© √† la position {position} secondes.",
        "seek_relative": f"M√©dia avanc√© de {offset} secondes."
    }
    
    log_message(f"Action media_control.{action}() simul√©e.")
    return actions_map.get(action, f"Action non support√©e pour media_control: {action}")

async def clock_tool(**kwargs) -> str:
    """G√®re les alarmes et les minuteurs (simulation)."""
    action = kwargs.get("action")
    
    if action == "create_alarm":
        duration = kwargs.get("duration")
        time = kwargs.get("time")
        return f"Alarme cr√©√©e pour {time if time else duration}."
    elif action == "create_timer":
        duration = kwargs.get("duration")
        return f"Minuteur cr√©√© pour {duration}."
    elif action == "show_matching_alarms":
        return "Affichage des alarmes correspondantes (simul√©)."
    elif action == "show_matching_timers":
        return "Affichage des minuteurs correspondants (simul√©)."
    elif action == "modify_alarm_v2":
        return "Alarme modifi√©e (simul√©)."
    elif action == "modify_timer_v2":
        return "Minuteur modifi√© (simul√©)."
    elif action == "snooze":
        return "Alarme mise en veille."
    else:
        return f"Action non support√©e pour clock: {action}"

async def run_in_sandbox_tool(code: str, language: str = "python") -> str:
    """Ex√©cute du code dans une sandbox s√©curis√©e (simulation)."""
    log_message(f"Ex√©cution de code {language}: {code[:100]}...")
    
    if language == "python":
        # V√©rification syntaxique basique
        try:
            compile(code, '<string>', 'exec')
            return f"‚úÖ Code Python syntaxiquement correct:\n```python\n{code}\n```\n\nEx√©cution: Le code s'ex√©cuterait sans erreur."
        except SyntaxError as e:
            return f"‚ùå Erreur de syntaxe Python: {e}"
    elif language == "shell":
        return f"‚úÖ Commande shell: {code}\nR√©sultat: Ex√©cution r√©ussie."
    else:
        return f"‚ùå Langage non support√©: {language}"

async def webcontainer_sandbox_tool(code: str, language: str = "javascript") -> str:
    """Ex√©cute du code dans un WebContainer."""
    log_message(f"Ex√©cution WebContainer {language}: {code[:100]}...")
    
    api_clients = get_api_clients()
    if 'webcontainer' in api_clients:
        try:
            response = await api_clients['webcontainer'].run_code(code, language)
            return str(response)
        except Exception as e:
            return f"Erreur WebContainer: {e}"
    else:
        return f"‚úÖ WebContainer - Code {language} ex√©cut√© avec succ√®s"

# Fonctions d'outils pour services externes
async def ocr_space_tool(image_base64: str) -> str:
    """Extrait le texte d'une image via OCR.space."""
    api_clients = get_api_clients()
    if 'ocr' in api_clients:
        try:
            response = await api_clients['ocr'].parse_image(image_base64)
            return str(response)
        except Exception as e:
            return f"Erreur OCR: {e}"
    return "Service OCR non disponible"

async def deepseek_chat_tool(prompt: str, model: str = "deepseek-chat") -> str:
    """Interroge DeepSeek pour des conversations."""
    api_clients = get_api_clients()
    if 'deepseek' in api_clients:
        try:
            messages = [{"role": "user", "content": prompt}]
            response = await api_clients['deepseek'].chat_completion(messages, model)
            return str(response)
        except Exception as e:
            return f"Erreur DeepSeek: {e}"
    return "Service DeepSeek non disponible"

async def serper_dev_tool(query_text: str) -> str:
    """Effectue une recherche web via Serper."""
    api_clients = get_api_clients()
    if 'serper' in api_clients:
        try:
            response = await api_clients['serper'].search(query_text)
            if isinstance(response, str):
                return neutralize_urls(response)
            return str(response)
        except Exception as e:
            return f"Erreur Serper: {e}"
    return "Service Serper non disponible"

async def wolfram_alpha_tool(input_text: str) -> str:
    """Interroge WolframAlpha pour des calculs."""
    api_clients = get_api_clients()
    if 'wolfram' in api_clients:
        try:
            response = await api_clients['wolfram'].query(input_text)
            return str(response)
        except Exception as e:
            return f"Erreur WolframAlpha: {e}"
    return "Service WolframAlpha non disponible"

async def tavily_search_tool(query_text: str, max_results: int = 3) -> str:
    """Effectue une recherche web avanc√©e via Tavily."""
    api_clients = get_api_clients()
    if 'tavily' in api_clients:
        try:
            response = await api_clients['tavily'].search(query_text, max_results)
            if isinstance(response, str):
                return neutralize_urls(response)
            return str(response)
        except Exception as e:
            return f"Erreur Tavily: {e}"
    return "Service Tavily non disponible"

async def apiflash_screenshot_tool(url: str) -> str:
    """Capture une capture d'√©cran d'une URL via ApiFlash."""
    api_clients = get_api_clients()
    if 'apiflash' in api_clients:
        try:
            response = await api_clients['apiflash'].screenshot(url)
            return str(response)
        except Exception as e:
            return f"Erreur ApiFlash: {e}"
    return "Service ApiFlash non disponible"

async def crawlbase_scraper_tool(url: str, use_js: bool = False) -> str:
    """Scrape le contenu d'une URL via Crawlbase."""
    api_clients = get_api_clients()
    if 'crawlbase' in api_clients:
        try:
            response = await api_clients['crawlbase'].scrape(url, use_js)
            if isinstance(response, str):
                return neutralize_urls(response)
            return str(response)
        except Exception as e:
            return f"Erreur Crawlbase: {e}"
    return "Service Crawlbase non disponible"

async def detect_language_tool(text: str) -> str:
    """D√©tecte la langue d'un texte."""
    api_clients = get_api_clients()
    if 'detect_language' in api_clients:
        try:
            response = await api_clients['detect_language'].detect(text)
            return str(response)
        except Exception as e:
            return f"Erreur DetectLanguage: {e}"
    return "Service DetectLanguage non disponible"

async def guardian_news_tool(query_text: str) -> str:
    """Recherche des articles de presse via The Guardian."""
    api_clients = get_api_clients()
    if 'guardian' in api_clients:
        try:
            response = await api_clients['guardian'].search_news(query_text)
            if isinstance(response, str):
                return neutralize_urls(response)
            return str(response)
        except Exception as e:
            return f"Erreur Guardian: {e}"
    return "Service Guardian non disponible"

async def ip2location_tool(ip_address: str) -> str:
    """G√©olocalise une adresse IP."""
    api_clients = get_api_clients()
    if 'ip2location' in api_clients:
        try:
            response = await api_clients['ip2location'].geolocate_ip(ip_address)
            return str(response)
        except Exception as e:
            return f"Erreur IP2Location: {e}"
    return "Service IP2Location non disponible"

async def shodan_tool(query_text: str = "") -> str:
    """Interroge Shodan pour des informations sur un h√¥te IP."""
    api_clients = get_api_clients()
    if 'shodan' in api_clients:
        try:
            response = await api_clients['shodan'].get_info(query_text)
            return str(response)
        except Exception as e:
            return f"Erreur Shodan: {e}"
    return "Service Shodan non disponible"

async def weather_api_tool(location: str) -> str:
    """R√©cup√®re les conditions m√©t√©orologiques."""
    api_clients = get_api_clients()
    if 'weather_api' in api_clients:
        try:
            response = await api_clients['weather_api'].get_current_weather(location)
            return str(response)
        except Exception as e:
            return f"Erreur WeatherAPI: {e}"
    return "Service WeatherAPI non disponible"

async def cloudmersive_domain_tool(domain: str) -> str:
    """V√©rifie la validit√© d'un domaine."""
    api_clients = get_api_clients()
    if 'cloudmersive' in api_clients:
        try:
            response = await api_clients['cloudmersive'].validate_domain(domain)
            return str(response)
        except Exception as e:
            return f"Erreur Cloudmersive: {e}"
    return "Service Cloudmersive non disponible"

async def greynoise_tool(ip_address: str) -> str:
    """Analyse une adresse IP via GreyNoise."""
    api_clients = get_api_clients()
    if 'greynoise' in api_clients:
        try:
            response = await api_clients['greynoise'].ip_lookup(ip_address)
            return str(response)
        except Exception as e:
            return f"Erreur GreyNoise: {e}"
    return "Service GreyNoise non disponible"

async def pulsedive_tool(indicator: str, type: str = "auto") -> str:
    """Analyse un indicateur de menace via Pulsedive."""
    api_clients = get_api_clients()
    if 'pulsedive' in api_clients:
        try:
            response = await api_clients['pulsedive'].analyze_indicator(indicator, type)
            return str(response)
        except Exception as e:
            return f"Erreur Pulsedive: {e}"
    return "Service Pulsedive non disponible"

async def stormglass_tool(lat: float, lng: float, params: str = "airTemperature,waveHeight") -> str:
    """R√©cup√®re les donn√©es m√©t√©orologiques maritimes."""
    api_clients = get_api_clients()
    if 'stormglass' in api_clients:
        try:
            response = await api_clients['stormglass'].get_weather_point(lat, lng, params)
            return str(response)
        except Exception as e:
            return f"Erreur StormGlass: {e}"
    return "Service StormGlass non disponible"

async def loginradius_ping_tool() -> str:
    """Effectue un ping √† l'API LoginRadius."""
    api_clients = get_api_clients()
    if 'loginradius' in api_clients:
        try:
            response = await api_clients['loginradius'].ping()
            return str(response)
        except Exception as e:
            return f"Erreur LoginRadius: {e}"
    return "Service LoginRadius non disponible"

async def jsonbin_io_tool(data: Optional[Dict[str, Any]] = None, private: bool = True, bin_id: Optional[str] = None) -> str:
    """Cr√©e ou acc√®de √† un bin JSON."""
    api_clients = get_api_clients()
    if 'jsonbin' in api_clients:
        try:
            response = await api_clients['jsonbin'].handle_bin(data, private, bin_id)
            return str(response)
        except Exception as e:
            return f"Erreur Jsonbin: {e}"
    return "Service Jsonbin non disponible"

async def huggingface_inference_tool(model_name: str = "distilbert-base-uncased-finetuned-sst-2-english", input_text: str = "Hello world") -> str:
    """Effectue une inf√©rence sur un mod√®le HuggingFace."""
    api_clients = get_api_clients()
    if 'huggingface' in api_clients:
        try:
            response = await api_clients['huggingface'].inference(model_name, input_text)
            return str(response)
        except Exception as e:
            return f"Erreur HuggingFace: {e}"
    return "Service HuggingFace non disponible"

async def twilio_balance_tool() -> str:
    """R√©cup√®re le solde du compte Twilio."""
    
    api_clients = get_api_clients()
    if 'twilio' in api_clients:
        try:
            response = await api_clients['twilio'].get_account_balance()
            return str(response)
        except Exception as e:
            return f"Erreur Twilio: {e}"
    return "Service Twilio non disponible"

async def abstractapi_tool(input_value: str, api_type: str) -> str:
    """Interroge diverses APIs d'AbstractAPI."""
    api_clients = get_api_clients()
    if 'abstractapi' in api_clients:
        try:
            response = await api_clients['abstractapi'].call_api(input_value, api_type)
            return str(response)
        except Exception as e:
            return f"Erreur AbstractAPI: {e}"
    return "Service AbstractAPI non disponible"

async def google_custom_search_tool(query_text: str) -> str:
    """Effectue une recherche personnalis√©e Google."""
    api_clients = get_api_clients()
    if 'google_search' in api_clients:
        try:
            response = await api_clients['google_search'].search(query_text)
            if isinstance(response, str):
                return neutralize_urls(response)
            return str(response)
        except Exception as e:
            return f"Erreur Google Custom Search: {e}"
    return "Service Google Custom Search non disponible"

async def randommer_phone_tool(country_code: str = "US", quantity: int = 1) -> str:
    """G√©n√®re des num√©ros de t√©l√©phone al√©atoires."""
    api_clients = get_api_clients()
    if 'randommer' in api_clients:
        try:
            response = await api_clients['randommer'].generate_phone_number(country_code, quantity)
            return str(response)
        except Exception as e:
            return f"Erreur Randommer: {e}"
    return "Service Randommer non disponible"

async def tomorrow_io_weather_tool(location: str, fields: Optional[List[str]] = None) -> str:
    """R√©cup√®re les pr√©visions m√©t√©orologiques via Tomorrow.io."""
    api_clients = get_api_clients()
    if 'tomorrow_io' in api_clients:
        try:
            if fields is None:
                fields = ["temperature", "humidity", "windSpeed"]
            response = await api_clients['tomorrow_io'].get_weather_timelines(location, fields)
            return str(response)
        except Exception as e:
            return f"Erreur Tomorrow.io: {e}"
    return "Service Tomorrow.io non disponible"

async def openweathermap_weather_tool(location: str) -> str:
    """R√©cup√®re les conditions m√©t√©orologiques via OpenWeatherMap."""
    api_clients = get_api_clients()
    if 'openweathermap' in api_clients:
        try:
            response = await api_clients['openweathermap'].get_current_weather(location)
            return str(response)
        except Exception as e:
            return f"Erreur OpenWeatherMap: {e}"
    return "Service OpenWeatherMap non disponible"

async def mockaroo_data_tool(count: int = 1, fields_json: Optional[str] = None) -> str:
    """G√©n√®re des donn√©es de test via Mockaroo."""
    api_clients = get_api_clients()
    if 'mockaroo' in api_clients:
        try:
            response = await api_clients['mockaroo'].generate_data(count, fields_json)
            return str(response)
        except Exception as e:
            return f"Erreur Mockaroo: {e}"
    return "Service Mockaroo non disponible"

async def openpagerank_tool(domains: List[str]) -> str:
    """R√©cup√®re le PageRank de domaines."""
    api_clients = get_api_clients()
    if 'openpagerank' in api_clients:
        try:
            response = await api_clients['openpagerank'].get_page_rank(domains)
            return str(response)
        except Exception as e:
            return f"Erreur OpenPageRank: {e}"
    return "Service OpenPageRank non disponible"

async def rapidapi_tool(api_name: str, **api_kwargs) -> str:
    """Interroge diverses APIs via RapidAPI."""
    api_clients = get_api_clients()
    if 'rapidapi' in api_clients:
        try:
            response = await api_clients['rapidapi'].call_rapidapi_endpoint(api_name, api_kwargs)
            return str(response)
        except Exception as e:
            return f"Erreur RapidAPI: {e}"
    return "Service RapidAPI non disponible"

def get_gemini_tools() -> List[Dict]:
    """
    Construit la liste des outils disponibles pour l'API Gemini.
    Compatible avec tous les cerveaux autonomes.
    """
    tools = []
    for tool_name, tool_info in config.TOOL_CONFIG.items():
        if tool_info.get("enabled", False):
            function_declaration = {
                "name": tool_name,
                "description": tool_info.get("description", ""),
                "parameters": {
                    "type": "OBJECT",
                    "properties": {},
                    "required": []
                }
            }
            
            # Ajoute les param√®tres de l'outil
            for param_name, param_info in tool_info.get("parameters", {}).items():
                function_declaration["parameters"]["properties"][param_name] = {
                    "type": param_info.get("type", "STRING"),
                    "description": param_info.get("description", "")
                }
                
                # Ajoute les √©num√©rations si pr√©sentes
                if "enum" in param_info:
                    function_declaration["parameters"]["properties"][param_name]["enum"] = param_info["enum"]
                
                # Ajoute les valeurs par d√©faut si pr√©sentes
                if "default" in param_info:
                    function_declaration["parameters"]["properties"][param_name]["default"] = param_info["default"]
                
                if param_info.get("required", False):
                    function_declaration["parameters"]["required"].append(param_name)
            
            tools.append({"function_declarations": [function_declaration]})
    
    return tools

# Fonctions utilitaires pour les outils

def validate_tool_parameters(tool_name: str, provided_params: Dict[str, Any]) -> Tuple[bool, str]:
    """Valide les param√®tres fournis pour un outil."""
    tool_config = find_tool_by_name(tool_name)
    if not tool_config:
        return False, f"Outil {tool_name} non trouv√©"
    
    required_params = []
    for param_name, param_info in tool_config.get("parameters", {}).items():
        if param_info.get("required", False):
            required_params.append(param_name)
    
    missing_params = [param for param in required_params if param not in provided_params]
    if missing_params:
        return False, f"Param√®tres manquants: {', '.join(missing_params)}"
    
    return True, "Param√®tres valides"

def sanitize_tool_output(output: str, max_length: int = 2000) -> str:
    """Nettoie et limite la sortie d'un outil."""
    if not isinstance(output, str):
        output = str(output)
    
    # Neutralise les URLs
    output = neutralize_urls(output)
    
    # Limite la longueur
    if len(output) > max_length:
        output = output[:max_length] + "... [TRONQU√â]"
    
    return output

def extract_code_from_response(response: str) -> Optional[str]:
    """Extrait le code d'une r√©ponse format√©e."""
    # Recherche de blocs de code markdown
    code_pattern = r'```(?:python|py)?\n(.*?)\n```'
    matches = re.findall(code_pattern, response, re.DOTALL)
    
    if matches:
        return matches[0].strip()
    
    # Recherche de code indent√©
    lines = response.split('\n')
    code_lines = []
    in_code_block = False
    
    for line in lines:
        if line.strip().startswith('def ') or line.strip().startswith('class ') or line.strip().startswith('import '):
            in_code_block = True
        
        if in_code_block:
            if line.strip() == '' or line.startswith('    ') or line.startswith('\t'):
                code_lines.append(line)
            elif line.strip() and not line.startswith(' '):
                if not any(keyword in line.lower() for keyword in ['print', 'return', 'if', 'for', 'while']):
                    break
                code_lines.append(line)
    
    if code_lines:
        return '\n'.join(code_lines).strip()
    
    return None

async def test_tool_connectivity() -> Dict[str, bool]:
    """Teste la connectivit√© de tous les outils."""
    api_clients = get_api_clients()
    connectivity_status = {}
    
    for service_name in config.API_CONFIG.keys():
        try:
            is_healthy = await endpoint_health_manager.is_service_healthy(service_name)
            has_quota = await quota_manager.check_quota(service_name)
            connectivity_status[service_name] = is_healthy and has_quota
        except Exception as e:
            log_message(f"Erreur test connectivit√© {service_name}: {e}", level="error")
            connectivity_status[service_name] = False
    
    return connectivity_status

def get_tool_usage_stats() -> Dict[str, Any]:
    """Retourne les statistiques d'utilisation des outils."""
    # Cette fonction pourrait √™tre √©tendue pour tracker l'utilisation r√©elle
    return {
        "total_tools": len(config.TOOL_CONFIG),
        "enabled_tools": len([t for t in config.TOOL_CONFIG.values() if t.get("enabled", False)]),
        "services_configured": len(config.API_CONFIG),
        "last_check": datetime.now().isoformat()
    }

# Classe pour la gestion avanc√©e des outils
class ToolManager:
    """Gestionnaire avanc√© pour l'ex√©cution et le monitoring des outils."""
    
    def __init__(self):
        self.execution_stats = {}
        self.error_counts = {}
        self.last_execution_times = {}
    
    async def execute_with_monitoring(self, tool_name: str, **kwargs) -> Dict[str, Any]:
        """Ex√©cute un outil avec monitoring des performances."""
        start_time = asyncio.get_event_loop().time()
        
        try:
            result = await execute_tool(tool_name, **kwargs)
            
            # Mise √† jour des statistiques
            execution_time = asyncio.get_event_loop().time() - start_time
            
            if tool_name not in self.execution_stats:
                self.execution_stats[tool_name] = {"count": 0, "total_time": 0, "avg_time": 0}
            
            stats = self.execution_stats[tool_name]
            stats["count"] += 1
            stats["total_time"] += execution_time
            stats["avg_time"] = stats["total_time"] / stats["count"]
            
            self.last_execution_times[tool_name] = datetime.now().isoformat()
            
            # Reset du compteur d'erreurs en cas de succ√®s
            if "error" not in result.get("tool_output", "").lower():
                self.error_counts[tool_name] = 0
            
            return result
            
        except Exception as e:
            # Comptage des erreurs
            self.error_counts[tool_name] = self.error_counts.get(tool_name, 0) + 1
            
            log_message(f"Erreur monitored execution {tool_name}: {e}", level="error")
            return {
                "tool_name": tool_name,
                "tool_args": kwargs,
                "tool_output": f"Erreur monitored: {e}"
            }
    
    def get_tool_stats(self, tool_name: str) -> Dict[str, Any]:
        """Retourne les statistiques d'un outil sp√©cifique."""
        return {
            "execution_stats": self.execution_stats.get(tool_name, {}),
            "error_count": self.error_counts.get(tool_name, 0),
            "last_execution": self.last_execution_times.get(tool_name),
            "is_healthy": self.error_counts.get(tool_name, 0) < 5
        }
    
    def get_all_stats(self) -> Dict[str, Any]:
        """Retourne toutes les statistiques des outils."""
        return {
            "execution_stats": self.execution_stats,
            "error_counts": self.error_counts,
            "last_executions": self.last_execution_times,
            "total_executions": sum(stats.get("count", 0) for stats in self.execution_stats.values())
        }

# Instance globale du gestionnaire d'outils
tool_manager = ToolManager()

import asyncio
import json
import logging
from datetime import datetime, timezone
from pathlib import Path
import re
import os
from typing import Any, Dict, List, Optional
import base64
import mimetypes

from config import config

# ==== Configuration du logging ====
logger = logging.getLogger("bot_logger")
logger.setLevel(logging.INFO)

# Cr√©e le r√©pertoire de base si n√©cessaire
config.BASE_DIR.mkdir(parents=True, exist_ok=True)

# Gestionnaire pour le fichier de log principal
file_handler = logging.FileHandler(config.LOG_FILE)
file_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# Gestionnaire pour les erreurs critiques
error_file_handler = logging.FileHandler(config.ERROR_LOG_PATH)
error_file_handler.setLevel(logging.ERROR)
error_file_handler.setFormatter(formatter)
logger.addHandler(error_file_handler)

# Gestionnaire pour la console
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# Verrou pour les op√©rations de fichier asynchrones
_file_lock: Optional[asyncio.Lock] = None

def set_file_lock(lock: asyncio.Lock):
    """D√©finit l'instance du verrou asyncio pour les op√©rations de fichier."""
    global _file_lock
    _file_lock = lock
    log_message("Verrou de fichier initialis√© dans utils.py.")

def log_message(message: str, level: str = "info"):
    """Enregistre un message dans le fichier de log et la console."""
    if level == "debug":
        logger.debug(message)
    elif level == "info":
        logger.info(message)
    elif level == "warning":
        logger.warning(message)
    elif level == "error":
        logger.error(message)
    elif level == "critical":
        logger.critical(message)
    else:
        logger.info(f"Niveau de log inconnu '{level}': {message}")

def get_current_time() -> datetime:
    """Retourne l'heure actuelle en UTC pour une coh√©rence temporelle."""
    return datetime.now(timezone.utc)

def format_datetime(dt: datetime) -> str:
    """Formate un objet datetime en cha√Æne de caract√®res lisible et standardis√©e."""
    return dt.strftime("%Y-%m-%d %H:%M:%S UTC")

async def load_json(file_path: Path, default_value: Any = None) -> Any:
    """
    Charge les donn√©es d'un fichier JSON de mani√®re asynchrone.
    Cr√©e le fichier avec une valeur par d√©faut si inexistant ou corrompu.
    """
    if _file_lock is None:
        log_message("Le verrou de fichier n'est pas initialis√© avant l'appel √† load_json.", level="critical")
        raise RuntimeError("File lock not initialized. Call set_file_lock in main.py first.")

    try:
        if not file_path.exists():
            log_message(f"Fichier non trouv√©: {file_path}. Cr√©ation avec valeur par d√©faut.", level="info")
            file_path.parent.mkdir(parents=True, exist_ok=True)
            await save_json(file_path, default_value if default_value is not None else {})
            return default_value if default_value is not None else {}

        async with _file_lock:
            return await asyncio.to_thread(_load_json_sync, file_path)
    except json.JSONDecodeError:
        log_message(f"Erreur de d√©codage JSON pour le fichier: {file_path}. R√©initialisation avec la valeur par d√©faut.", level="error")
        await save_json(file_path, default_value if default_value is not None else {})
        return default_value if default_value is not None else {}
    except Exception as e:
        log_message(f"Erreur inattendue lors du chargement du JSON {file_path}: {e}", level="error")
        return default_value if default_value is not None else {}

def _load_json_sync(file_path: Path) -> Any:
    """Fonction synchrone pour charger le JSON, ex√©cut√©e dans un thread s√©par√©."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

async def save_json(file_path: Path, data: Any):
    """Sauvegarde les donn√©es dans un fichier JSON de mani√®re asynchrone."""
    if _file_lock is None:
        log_message("Le verrou de fichier n'est pas initialis√© avant l'appel √† save_json.", level="critical")
        raise RuntimeError("File lock not initialized. Call set_file_lock in main.py first.")

    try:
        file_path.parent.mkdir(parents=True, exist_ok=True)
        async with _file_lock:
            await asyncio.to_thread(_save_json_sync, file_path, data)
        log_message(f"Donn√©es sauvegard√©es dans {file_path}", level="debug")
    except Exception as e:
        log_message(f"Erreur lors de la sauvegarde du JSON {file_path}: {e}", level="error")

def _save_json_sync(file_path: Path, data: Any):
    """Fonction synchrone pour sauvegarder le JSON, ex√©cut√©e dans un thread s√©par√©."""
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

def neutralize_urls(text: str) -> str:
    """
    Remplace les URLs dans le texte par une version neutralis√©e pour √©viter les probl√®mes de s√©curit√©.
    """
    url_pattern = re.compile(r'https?://[^\s/$.?#].[^\s]*', re.IGNORECASE)
    neutralized_text = url_pattern.sub("[LIEN_NEUTRALIS√â]", text)
    return neutralized_text

def find_tool_by_name(tool_name: str) -> Optional[Dict[str, Any]]:
    """Recherche un outil dans TOOL_CONFIG par son nom."""
    return config.TOOL_CONFIG.get(tool_name)

async def append_to_file(file_path: Path, content: str):
    """
    Ajoute du contenu √† un fichier, en cr√©ant le fichier/r√©pertoire si n√©cessaire.
    G√®re la rotation du fichier si sa taille d√©passe MAX_FILE_SIZE.
    """
    if _file_lock is None:
        log_message("Le verrou de fichier n'est pas initialis√© avant l'appel √† append_to_file.", level="critical")
        raise RuntimeError("File lock not initialized. Call set_file_lock in main.py first.")

    file_path.parent.mkdir(parents=True, exist_ok=True)

    if file_path.exists() and file_path.stat().st_size + len(content.encode('utf-8')) > config.MAX_FILE_SIZE:
        rotate_file(file_path)

    async with _file_lock:
        await asyncio.to_thread(_append_to_file_sync, file_path, content)

def _append_to_file_sync(file_path: Path, content: str):
    """Fonction synchrone pour ajouter du contenu √† un fichier, ex√©cut√©e dans un thread s√©par√©."""
    with open(file_path, 'a', encoding='utf-8') as f:
        f.write(content + "\n")

def rotate_file(file_path: Path):
    """Effectue une rotation de fichier simple: renomme le fichier actuel avec un horodatage."""
    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    new_path = file_path.parent / f"{file_path.stem}_{timestamp}{file_path.suffix}"
    try:
        os.rename(file_path, new_path)
        log_message(f"Fichier {file_path.name} renomm√© en {new_path.name} pour rotation.", level="info")
    except OSError as e:
        log_message(f"Erreur lors de la rotation du fichier {file_path.name}: {e}", level="error")

def get_mime_type_from_base64(base64_string: str) -> Optional[str]:
    """Tente de d√©terminer le type MIME √† partir d'une cha√Æne base64."""
    if base64_string.startswith("data:"):
        parts = base64_string.split(",", 1)
        if len(parts) > 0:
            mime_part = parts[0]
            if ";" in mime_part:
                return mime_part.split(";", 1)[0].split(":", 1)[1]
            else:
                return mime_part.split(":", 1)[1]

    try:
        decoded_bytes = base64.b64decode(base64_string[:1024], validate=True)
        
        if decoded_bytes.startswith(b'\x89PNG\r\n\x1a\n'):
            return 'image/png'
        elif decoded_bytes.startswith(b'\xff\xd8\xff'):
            return 'image/jpeg'
        elif decoded_bytes.startswith(b'GIF87a') or decoded_bytes.startswith(b'GIF89a'):
            return 'image/gif'
        elif decoded_bytes.startswith(b'%PDF-'):
            return 'application/pdf'
        elif decoded_bytes.startswith(b'BM'):
            return 'image/bmp'
        elif decoded_bytes.startswith(b'RIFF') and decoded_bytes[8:12] == b'WEBP':
            return 'image/webp'
            
    except Exception as e:
        log_message(f"Erreur lors de la d√©tection MIME pour base64: {e}", level="debug")

    return None

def extract_memories(text: str) -> List[Dict[str, Any]]:
    """
    Extrait les √©l√©ments de m√©moire d'un texte de r√©ponse.
    Extraction simple bas√©e sur des mots-cl√©s importants.
    """
    memories = []
    
    if "important" in text.lower():
        memories.append({"type": "important", "content": text[:200]})
        
    if "remember" in text.lower() or "rappel" in text.lower():
        memories.append({"type": "reminder", "content": text[:200]})
        
    if "erreur" in text.lower() or "error" in text.lower():
        memories.append({"type": "error", "content": text[:200]})
        
    if "succ√®s" in text.lower() or "success" in text.lower():
        memories.append({"type": "success", "content": text[:200]})
    
    return memories

def validate_url(url: str) -> bool:
    """Valide une URL basique."""
    url_pattern = re.compile(
        r'^https?://'  # http:// ou https://
        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domaine
        r'localhost|'  # localhost
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # IP
        r'(?::\d+)?'  # port optionnel
        r'(?:/?|[/?]\S+)$', re.IGNORECASE)
    return url_pattern.match(url) is not None

def sanitize_filename(filename: str) -> str:
    """Nettoie un nom de fichier pour le rendre s√ªr."""
    # Supprime ou remplace les caract√®res dangereux
    sanitized = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # Limite la longueur
    if len(sanitized) > 100:
        sanitized = sanitized[:100]
    return sanitized.strip()

def format_file_size(size_bytes: int) -> str:
    """Formate une taille de fichier en unit√©s lisibles."""
    if size_bytes == 0:
        return "0 B"
    
    units = ['B', 'KB', 'MB', 'GB', 'TB']
    unit_index = 0
    size = float(size_bytes)
    
    while size >= 1024.0 and unit_index < len(units) - 1:
        size /= 1024.0
        unit_index += 1
    
    return f"{size:.1f} {units[unit_index]}"

def truncate_text(text: str, max_length: int = 200, suffix: str = "...") -> str:
    """Tronque un texte √† une longueur maximale."""
    if len(text) <= max_length:
        return text
    return text[:max_length - len(suffix)] + suffix

def parse_duration(duration_str: str) -> int:
    """Parse une dur√©e en format humain vers des secondes."""
    duration_str = duration_str.lower().strip()
    
    # Patterns pour diff√©rents formats
    patterns = [
        (r'(\d+)\s*s(?:ec)?(?:onds?)?', 1),
        (r'(\d+)\s*m(?:in)?(?:utes?)?', 60),
        (r'(\d+)\s*h(?:ours?)?', 3600),
        (r'(\d+)\s*d(?:ays?)?', 86400),
    ]
    
    total_seconds = 0
    
    for pattern, multiplier in patterns:
        matches = re.findall(pattern, duration_str)
        for match in matches:
            total_seconds += int(match) * multiplier
    
    return total_seconds if total_seconds > 0 else 60  # Default 1 minute

def clean_html(html_content: str) -> str:
    """Nettoie le contenu HTML de base."""
    # Supprime les scripts et styles
    html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
    html_content = re.sub(r'<style[^>]*>.*?</style>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
    
    # Supprime les balises HTML
    html_content = re.sub(r'<[^>]+>', '', html_content)
    
    # Nettoie les espaces multiples
    html_content = re.sub(r'\s+', ' ', html_content)
    
    return html_content.strip()

def generate_unique_id(prefix: str = "") -> str:
    """G√©n√®re un identifiant unique bas√© sur le timestamp."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    return f"{prefix}_{timestamp}" if prefix else timestamp

def is_safe_path(file_path: Path, base_path: Path) -> bool:
    """V√©rifie qu'un chemin de fichier est s√ªr (pas de directory traversal)."""
    try:
        file_path.resolve().relative_to(base_path.resolve())
        return True
    except ValueError:
        return False

def mask_sensitive_data(text: str) -> str:
    """Masque les donn√©es sensibles dans un texte."""
    # Masque les cl√©s API (patterns communs)
    text = re.sub(r'(sk-[a-zA-Z0-9]{20,})', '***MASKED_API_KEY***', text)
    text = re.sub(r'(AIza[a-zA-Z0-9_-]{20,})', '***MASKED_GOOGLE_KEY***', text)
    
    # Masque les adresses email
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '***MASKED_EMAIL***', text)
    
    # Masque les num√©ros de t√©l√©phone
    text = re.sub(r'\b\d{10,}\b', '***MASKED_PHONE***', text)
    
    return text

class RateLimiter:
    """Limiteur de d√©bit simple pour les op√©rations."""
    
    def __init__(self, max_requests: int, time_window: int):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = []
    
    def is_allowed(self) -> bool:
        """V√©rifie si une nouvelle requ√™te est autoris√©e."""
        now = datetime.now().timestamp()
        
        # Supprime les requ√™tes anciennes
        self.requests = [req_time for req_time in self.requests if now - req_time < self.time_window]
        
        # V√©rifie la limite
        if len(self.requests) < self.max_requests:
            self.requests.append(now)
            return True
        
        return False
    
    def time_until_allowed(self) -> float:
        """Retourne le temps √† attendre avant la prochaine requ√™te autoris√©e."""
        if not self.requests:
            return 0.0
        
        oldest_request = min(self.requests)
        return max(0.0, self.time_window - (datetime.now().timestamp() - oldest_request))

def retry_on_exception(max_retries: int = 3, delay: float = 1.0, backoff_factor: float = 2.0):
    """D√©corateur pour r√©essayer une fonction en cas d'exception."""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            last_exception = None
            current_delay = delay
            
            for attempt in range(max_retries + 1):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries:
                        log_message(f"Tentative {attempt + 1}/{max_retries + 1} √©chou√©e pour {func.__name__}: {e}", level="warning")
                        await asyncio.sleep(current_delay)
                        current_delay *= backoff_factor
                    else:
                        log_message(f"Toutes les tentatives √©chou√©es pour {func.__name__}: {e}", level="error")
            
            raise last_exception
        return wrapper
    return decorator

# Constantes utiles
COMMON_USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
]

DANGEROUS_EXTENSIONS = [
    '.exe', '.bat', '.cmd', '.com', '.pif', '.scr', '.vbs', '.js', '.jar',
    '.msi', '.dll', '.app', '.deb', '.rpm', '.dmg', '.pkg'
]

ALLOWED_IMAGE_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg']
ALLOWED_DOCUMENT_EXTENSIONS = ['.txt', '.pdf', '.doc', '.docx', '.md', '.json', '.xml', '.csv']

def get_random_user_agent() -> str:
    """Retourne un User-Agent al√©atoire."""
    import random
    return random.choice(COMMON_USER_AGENTS)

def is_dangerous_file(filename: str) -> bool:
    """V√©rifie si un fichier est potentiellement dangereux."""
    _, ext = os.path.splitext(filename.lower())
    return ext in DANGEROUS_EXTENSIONS

def ensure_directory_exists(directory: Path) -> bool:
    """S'assure qu'un r√©pertoire existe, le cr√©e si n√©cessaire."""
    try:
        directory.mkdir(parents=True, exist_ok=True)
        return True
    except Exception as e:
        log_message(f"Erreur cr√©ation r√©pertoire {directory}: {e}", level="error")
        return False
        
        import asyncio
import json
import traceback
from pathlib import Path
from datetime import datetime
import re
from typing import Dict, Any, List, Optional

from config import config
from utils import log_message, set_file_lock, get_mime_type_from_base64
from app_singletons import endpoint_health_manager, quota_manager
from brain_library import brain_coordinator, TelegramMemoryIntegration
from autonomous_brain import create_brain
from coding_challenge_system import get_coding_challenge_system
from security_archiver import fetch_and_archive_pages
from tools import get_gemini_tools

class DecentralizedAISystem:
    """
    Syst√®me d'IA d√©centralis√© avec 7 cerveaux autonomes.
    Chaque cerveau peut traiter ind√©pendamment les requ√™tes utilisateur.
    """
    def __init__(self):
        self.brains = {}
        self.telegram_memory = None
        self.coding_system = None
        self.last_activity = datetime.now().timestamp()
        self.system_initialized = False
        
        # Les 7 cerveaux autonomes
        self.brain_types = ["GEMINI", "DEEPSEEK", "HUGGINGFACE", "TAVILY", "SERPER", "GOOGLE_CUSTOM_SEARCH", "WOLFRAMALPHA"]
        
    async def initialize_system(self):
        """Initialise tous les composants du syst√®me d√©centralis√©."""
        try:
            log_message("üöÄ Initialisation du syst√®me d'IA d√©centralis√©...")
            
            # Configuration du verrou de fichier
            file_lock = asyncio.Lock()
            set_file_lock(file_lock)
            
            # Initialisation des singletons
            await endpoint_health_manager.init_manager()
            await quota_manager.init_manager()
            
            # Import du client Telegram
            try:
                from app_clients_instances import telegram_bot_client
                self.telegram_memory = TelegramMemoryIntegration(telegram_bot_client)
            except ImportError:
                log_message("Client Telegram non disponible, utilisation du mode simul√©", level="warning")
                self.telegram_memory = TelegramMemoryIntegration(None)
            
            # Initialisation des 7 cerveaux autonomes
            for brain_type in self.brain_types:
                try:
                    brain = create_brain(brain_type, self.telegram_memory.bot_client if self.telegram_memory else None)
                    await brain.initialize()
                    self.brains[brain_type] = brain
                    log_message(f"‚úÖ Cerveau {brain_type} initialis√©")
                except Exception as e:
                    log_message(f"‚ùå Erreur initialisation cerveau {brain_type}: {e}", level="error")
            
            # Initialisation du syst√®me de d√©fis de codage
            self.coding_system = get_coding_challenge_system(
                self.telegram_memory.bot_client if self.telegram_memory else None
            )
            await self.coding_system.initialize()
            
            # Message d'initialisation dans le groupe priv√©
            await self.telegram_memory.write_to_group(
                f"""
üß† SYST√àME D'IA D√âCENTRALIS√â INITIALIS√â

‚úÖ Cerveaux actifs: {len(self.brains)}/7
‚úÖ Gestionnaire de sant√©: Op√©rationnel
‚úÖ Gestionnaire de quotas: Op√©rationnel  
‚úÖ Syst√®me de d√©fis: Pr√™t
‚úÖ Archiveur s√©curis√©: Pr√™t

üîÑ Rotation automatique: 45 minutes
üéØ D√©fis de codage: 15 minutes
üìù M√©moire partag√©e: Active

Le syst√®me est pr√™t √† traiter les requ√™tes utilisateur.
""",
                "SYSTEM_INIT"
            )
            
            self.system_initialized = True
            log_message("üéâ Syst√®me d'IA d√©centralis√© initialis√© avec succ√®s")
            return True
            
        except Exception as e:
            log_message(f"‚ùå Erreur critique lors de l'initialisation: {e}", level="critical")
            log_message(f"Traceback: {traceback.format_exc()}", level="critical")
            return False
    
    async def start_background_tasks(self):
        """D√©marre toutes les t√¢ches de fond."""
        if not self.system_initialized:
            log_message("Syst√®me non initialis√©, impossible de d√©marrer les t√¢ches de fond", level="error")
            return
        
        log_message("üîÑ D√©marrage des t√¢ches de fond...")
        
        # T√¢che de health checks p√©riodiques
        asyncio.create_task(self._periodic_health_checks())
        
        # T√¢che de d√©fis de codage automatis√©s
        asyncio.create_task(self.coding_system.start_periodic_challenges())
        
        # T√¢che de maintenance des quotas
        asyncio.create_task(self._quota_maintenance())
        
        # T√¢che de nettoyage de la m√©moire
        asyncio.create_task(self._memory_cleanup())
        
        await self.telegram_memory.write_to_group(
            "üîÑ Toutes les t√¢ches de fond sont d√©marr√©es",
            "BACKGROUND_TASKS"
        )
        
        log_message("‚úÖ T√¢ches de fond d√©marr√©es")
    
    async def handle_user_request(self, user_query: str, user_id: str = "default_user", 
                                image_data: Optional[str] = None) -> Dict[str, Any]:
        """
        Traite une requ√™te utilisateur avec le syst√®me d√©centralis√©.
        S√©lectionne automatiquement le meilleur cerveau disponible.
        """
        if not self.system_initialized:
            return {"error": "Syst√®me non initialis√©", "brain_id": "SYSTEM"}
        
        start_time = datetime.now()
        self.last_activity = start_time.timestamp()
        
        # Log de la requ√™te
        await self.telegram_memory.write_to_group(
            f"üîç NOUVELLE REQU√äTE UTILISATEUR\nUtilisateur: {user_id}\nRequ√™te: {user_query[:200]}...",
            "USER_REQUEST"
        )
        
        log_message(f"Traitement requ√™te utilisateur: {user_query[:100]}...")
        
        try:
            # S√©lection du cerveau optimal
            selected_brain_type = brain_coordinator.get_next_brain()
            selected_brain = self.brains.get(selected_brain_type)
            
            if not selected_brain:
                error_msg = f"Cerveau {selected_brain_type} non disponible"
                await self.telegram_memory.log_error("SYSTEM", error_msg)
                return {"error": error_msg, "brain_id": "SYSTEM"}
            
            # Augmentation de la charge du cerveau
            brain_coordinator.update_brain_load(selected_brain_type, 1)
            
            await self.telegram_memory.log_brain_activity(
                selected_brain_type,
                "S√©lectionn√© pour traitement",
                {"user_id": user_id, "query_length": len(user_query)}
            )
            
            # Pr√©paration des outils
            available_tools = get_gemini_tools()
            
            # Traitement par le cerveau s√©lectionn√©
            result = await selected_brain.process_request(
                user_query=user_query,
                chat_history=[],  # L'historique est g√©r√© par la m√©moire du cerveau
                image_data=image_data,
                tools=available_tools
            )
            
            # Diminution de la charge du cerveau
            brain_coordinator.update_brain_load(selected_brain_type, -1)
            
            # Calcul du temps de traitement
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # Mise √† jour des statistiques
            if "error" not in result:
                await quota_manager.increment_quota(selected_brain_type, success=True)
                await self.telegram_memory.log_success(
                    selected_brain_type,
                    f"Requ√™te trait√©e en {processing_time:.2f}s",
                    str(result.get("response", ""))[:200]
                )
            else:
                await quota_manager.increment_quota(selected_brain_type, success=False)
                brain_coordinator.mark_brain_failed(selected_brain_type)
                await self.telegram_memory.log_error(
                    selected_brain_type,
                    f"√âchec traitement: {result.get('error', 'Erreur inconnue')}"
                )
            
            # Enrichissement du r√©sultat
            result.update({
                "processing_time": processing_time,
                "timestamp": start_time.isoformat(),
                "user_id": user_id,
                "system_status": brain_coordinator.get_brain_status()
            })
            
            return result
            
        except Exception as e:
            error_msg = f"Erreur syst√®me lors du traitement: {e}"
            log_message(f"Erreur handle_user_request: {error_msg}", level="error")
            log_message(f"Traceback: {traceback.format_exc()}", level="error")
            
            await self.telegram_memory.log_error("SYSTEM", error_msg)
            
            return {
                "error": error_msg,
                "brain_id": "SYSTEM",
                "timestamp": start_time.isoformat(),
                "user_id": user_id
            }
    
    async def _periodic_health_checks(self):
        """T√¢che de health checks p√©riodiques pour tous les services."""
        while True:
            try:
                await self.telegram_memory.write_to_group(
                    "üè• D√©but des health checks p√©riodiques",
                    "HEALTH_CHECK"
                )
                
                # Health check pour tous les services configur√©s
                for service_name in config.API_CONFIG.keys():
                    await endpoint_health_manager.run_health_check_for_service(service_name)
                    await asyncio.sleep(1)  # Pause entre services
                
                # Rapport de sant√©
                health_report = []
                for brain_type in self.brain_types:
                    is_healthy = await endpoint_health_manager.is_service_healthy(brain_type)
                    health_report.append(f"{'‚úÖ' if is_healthy else '‚ùå'} {brain_type}")
                
                await self.telegram_memory.write_to_group(
                    f"üìä RAPPORT DE SANT√â\n\n" + "\n".join(health_report),
                    "HEALTH_REPORT"
                )
                
                log_message("Health checks p√©riodiques termin√©s")
                
            except Exception as e:
                log_message(f"Erreur health checks: {e}", level="error")
            
            await asyncio.sleep(config.HEALTH_CHECK_INTERVAL_SECONDS)
    
    async def _quota_maintenance(self):
        """Maintenance p√©riodique des quotas."""
        while True:
            try:
                await asyncio.sleep(3600)  # Toutes les heures
                
                # Rapport des quotas
                quota_status = quota_manager.get_all_quotas_status()
                
                critical_quotas = [
                    api for api, status in quota_status.items()
                    if status.get("remaining", 0) < status.get("limit", 0) * 0.1  # Moins de 10%
                ]
                
                if critical_quotas:
                    await self.telegram_memory.write_to_group(
                        f"‚ö†Ô∏è QUOTAS CRITIQUES: {', '.join(critical_quotas)}",
                        "QUOTA_WARNING"
                    )
                
                log_message("Maintenance des quotas effectu√©e")
                
            except Exception as e:
                log_message(f"Erreur maintenance quotas: {e}", level="error")
    
    async def _memory_cleanup(self):
        """Nettoyage p√©riodique de la m√©moire."""
        while True:
            try:
                await asyncio.sleep(24 * 3600)  # Tous les jours
                
                # Nettoyage de la m√©moire des cerveaux
                for brain in self.brains.values():
                    await brain.memory_manager.save_memory()
                
                await self.telegram_memory.write_to_group(
                    "üßπ Nettoyage de m√©moire effectu√©",
                    "MEMORY_CLEANUP"
                )
                
                log_message("Nettoyage de m√©moire effectu√©")
                
            except Exception as e:
                log_message(f"Erreur nettoyage m√©moire: {e}", level="error")
    
    def get_system_status(self) -> Dict[str, Any]:
        """Retourne le statut complet du syst√®me."""
        if not self.system_initialized:
            return {"status": "not_initialized"}
        
        return {
            "status": "operational",
            "initialized": self.system_initialized,
            "active_brains": len(self.brains),
            "brain_status": brain_coordinator.get_brain_status(),
            "coding_challenges": self.coding_system.get_challenge_statistics() if self.coding_system else {},
            "last_activity": self.last_activity,
            "uptime": datetime.now().timestamp() - self.last_activity if hasattr(self, 'start_time') else 0
        }
    
    async def shutdown(self):
        """Arr√™t propre du syst√®me."""
        log_message("üõë Arr√™t du syst√®me d'IA d√©centralis√©...")
        
        try:
            # Arr√™t des d√©fis de codage
            if self.coding_system:
                self.coding_system.stop_challenges()
            
            # Sauvegarde finale de toutes les m√©moires
            for brain in self.brains.values():
                await brain.memory_manager.save_memory()
            
            await self.telegram_memory.write_to_group(
                "üõë Syst√®me d'IA d√©centralis√© arr√™t√© proprement",
                "SYSTEM_SHUTDOWN"
            )
            
            log_message("‚úÖ Syst√®me arr√™t√© proprement")
            
        except Exception as e:
            log_message(f"Erreur lors de l'arr√™t: {e}", level="error")

# Instance globale du syst√®me
decentralized_system = DecentralizedAISystem()

async def main():
    """Fonction principale pour le mode console."""
    try:
        # Initialisation du syst√®me
        success = await decentralized_system.initialize_system()
        if not success:
            log_message("‚ùå √âchec de l'initialisation, arr√™t du programme", level="critical")
            return
        
        # D√©marrage des t√¢ches de fond
        await decentralized_system.start_background_tasks()
        
        # Interface console
        print("\n" + "="*60)
        print("üß† SYST√àME D'IA D√âCENTRALIS√â - 7 CERVEAUX AUTONOMES")
        print("="*60)
        print("Commandes disponibles:")
        print("  /help      - Affiche l'aide")
        print("  /status    - Statut du syst√®me")
        print("  /brains    - √âtat des cerveaux")
        print("  /quotas    - √âtat des quotas")
        print("  /challenges - Statistiques d√©fis")
        print("  /archive <urls> - Archive des pages web")
        print("  /exit      - Quitter")
        print("  Ou tapez directement votre question")
        print("="*60)
        
        while True:
            try:
                user_input = await asyncio.to_thread(input, "\nü§ñ Vous: ")
                user_input = user_input.strip()
                
                if not user_input:
                    continue
                
                if user_input.lower() == "/exit":
                    break
                
                elif user_input.lower() == "/help":
                    print("""
üìã AIDE DU SYST√àME D'IA D√âCENTRALIS√â

üß† Architecture:
  ‚Ä¢ 7 cerveaux autonomes (GEMINI, DEEPSEEK, HUGGINGFACE, TAVILY, SERPER, GOOGLE_CUSTOM_SEARCH, WOLFRAMALPHA)
  ‚Ä¢ Rotation automatique toutes les 45 minutes
  ‚Ä¢ Basculement automatique en cas de panne
  ‚Ä¢ M√©moire partag√©e dans le groupe priv√© Telegram

üéØ Fonctionnalit√©s:
  ‚Ä¢ Traitement de requ√™tes utilisateur
  ‚Ä¢ D√©fis de codage automatis√©s (15 min)
  ‚Ä¢ Archivage s√©curis√© de pages web
  ‚Ä¢ Monitoring sant√© des APIs
  ‚Ä¢ Gestion intelligente des quotas

üí¨ Exemples d'utilisation:
  ‚Ä¢ "Explique-moi la programmation asynchrone"
  ‚Ä¢ "Cr√©e un script Python pour analyser des donn√©es"
  ‚Ä¢ "Recherche les derni√®res nouvelles sur l'IA"
  ‚Ä¢ "/archive https://example.com,https://site.org"
""")
                
                elif user_input.lower() == "/status":
                    status = decentralized_system.get_system_status()
                    print(f"""
üìä STATUT SYST√àME:
  ‚Ä¢ √âtat: {status['status']}
  ‚Ä¢ Cerveaux actifs: {status['active_brains']}/7
  ‚Ä¢ Cerveau pour la prochaine requ√™te: {status.get('brain_status', {}).get('active_brain_for_next_request', 'N/A')}
  ‚Ä¢ Derni√®re activit√©: {datetime.fromtimestamp(status['last_activity']).strftime('%H:%M:%S')}
""")
                
                elif user_input.lower() == "/brains":
                    brain_status = brain_coordinator.get_brain_status()
                    print("\nüß† √âTAT DES CERVEAUX:")
                    for brain, healthy in brain_status['brain_health'].items():
                        load = brain_status['brain_load'].get(brain, 0)
                        status_icon = "‚úÖ" if healthy else "‚ùå"
                        print(f"  {status_icon} {brain}: Charge {load}")
                
                elif user_input.lower() == "/quotas":
                    quotas = quota_manager.get_all_quotas_status()
                    print("\nüìä √âTAT DES QUOTAS:")
                    for api, quota_info in quotas.items():
                        if isinstance(quota_info, dict) and 'error' not in quota_info:
                            usage = quota_info['current_usage']
                            limit = quota_info['limit']
                            remaining = quota_info['remaining']
                            percent = (usage / limit * 100) if limit > 0 else 0
                            print(f"  {api}: {usage}/{limit} ({percent:.1f}%) - Restant: {remaining}")
                
                elif user_input.lower() == "/challenges":
                    if decentralized_system.coding_system:
                        stats = decentralized_system.coding_system.get_challenge_statistics()
                        print(f"""
üéØ STATISTIQUES D√âFIS DE CODAGE:
  ‚Ä¢ Total d√©fis: {stats.get('total_challenges', 0)}
  ‚Ä¢ Participants: {stats.get('total_participants', 0)}
  ‚Ä¢ Succ√®s: {stats.get('total_successful', 0)}
  ‚Ä¢ Taux succ√®s: {stats.get('average_success_rate', 0):.1f}%
  ‚Ä¢ Statut: {'üîÑ Actif' if stats.get('is_running') else '‚èπÔ∏è Arr√™t√©'}
""")
                    else:
                        print("‚ùå Syst√®me de d√©fis non initialis√©")
                
                elif user_input.startswith("/archive "):
                    urls_str = user_input[9:].strip()
                    if urls_str:
                        urls = [url.strip() for url in urls_str.split(',') if url.strip()]
                        if urls:
                            print(f"üóÇÔ∏è Archivage de {len(urls)} URL(s) en cours...")
                            result = await fetch_and_archive_pages(urls, "console_user")
                            print(f"‚úÖ {result.get('tool_output', 'Archivage termin√©')}")
                        else:
                            print("‚ùå Aucune URL valide fournie")
                    else:
                        print("‚ùå Usage: /archive <url1>,<url2>,...")
                
                else:
                    # Traitement d'une requ√™te normale
                    print("ü§î Traitement en cours...")
                    
                    # D√©tection simple d'image (simulation)
                    image_data = None
                    if any(ext in user_input.lower() for ext in ['.png', '.jpg', '.jpeg', '.gif']):
                        print("üñºÔ∏è Image d√©tect√©e (mode simulation)")
                        image_data = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="
                    
                    # Traitement par le syst√®me d√©centralis√©
                    response = await decentralized_system.handle_user_request(
                        user_query=user_input,
                        user_id="console_user",
                        image_data=image_data
                    )
                    
                    # Affichage de la r√©ponse
                    if "error" in response:
                        print(f"‚ùå Erreur ({response.get('brain_id', 'UNKNOWN')}): {response['error']}")
                    else:
                        brain_id = response.get('brain_id', 'UNKNOWN')
                        processing_time = response.get('processing_time', 0)
                        
                        # Extraction de la r√©ponse selon le format
                        if 'response' in response and isinstance(response['response'], dict):
                            candidates = response['response'].get('candidates', [])
                            if candidates and 'content' in candidates[0]:
                                parts = candidates[0]['content'].get('parts', [])
                                if parts and 'text' in parts[0]:
                                    answer = parts[0]['text']
                                else:
                                    answer = str(response['response'])
                            else:
                                answer = str(response['response'])
                        else:
                            answer = str(response.get('response', 'Aucune r√©ponse'))
                        
                        print(f"\nü§ñ {brain_id} ({processing_time:.2f}s): {answer}")
                        
                        # Affichage des outils utilis√©s
                        if 'tool_results' in response and response['tool_results']:
                            print(f"\nüîß Outils utilis√©s: {len(response['tool_results'])}")
                            for tool in response['tool_results']:
                                tool_name = tool.get('tool_name', 'Inconnu')
                                print(f"  ‚Ä¢ {tool_name}")
                
            except EOFError:
                print("\nüëã Au revoir !")
                break
            except KeyboardInterrupt:
                print("\n‚ö†Ô∏è Interruption d√©tect√©e...")
                break
            except Exception as e:
                print(f"‚ùå Erreur: {e}")
                log_message(f"Erreur console: {e}", level="error")
        
    except Exception as e:
        log_message(f"Erreur critique dans main(): {e}", level="critical")
        log_message(f"Traceback: {traceback.format_exc()}", level="critical")
    
    finally:
        # Arr√™t propre du syst√®me
        await decentralized_system.shutdown()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nüõë Arr√™t forc√© du syst√®me")
    except Exception as e:
        print(f"‚ùå Erreur fatale: {e}"}



import re
import hashlib
import asyncio
import httpx
import io
from datetime import datetime
from urllib.parse import urlparse

# ----------------------------
# CONFIGURATION CONSTANTS
# ----------------------------
MAX_CHUNK_SIZE = 5 * 1024 * 1024  # 5MB
PRIVATE_GROUP_ID = "VOTRE_GROUPE_ID"  # √Ä remplacer par l'ID r√©el

# ----------------------------
# URL DEFANGER
# ----------------------------
class URLDefanger:
    """
    Neutralise les URLs pour emp√™cher les clics accidentels
    et bloque les trackers connus
    """
    def __init__(self, mode="secure"):
        self.mode = mode
        self.url_pattern = re.compile(r'https?://[^\s\]]+')
    
    def _generate_hash(self, url):
        """G√©n√®re un identifiant unique pour l'URL"""
        return hashlib.sha256(url.encode()).hexdigest()[:8]
    
    def defang_url(self, url):
        """Transforme une URL en version s√©curis√©e"""
        if "doubleclick.net" in url:
            return "[TRACKER_BLOQU√â]"
        
        if self.mode == "secure":
            return f"[URL_BLOQU√âE:#{self._generate_hash(url)}]"
        else:
            parsed = urlparse(url)
            return f"[URL:{parsed.netloc}/...#{self._generate_hash(url)}]"
    
    def defang_text(self, text):
        """Nettoie tout le contenu texte"""
        return self.url_pattern.sub(
            lambda m: self.defang_url(m.group(0)), 
            text
        )

# ----------------------------
# PAGE ARCHIVER
# ----------------------------
class SecurePageArchiver:
    """
    T√©l√©charge, s√©curise et archive des pages web
    avec gestion des gros fichiers et protection anti-tracking
    """
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
            "Accept-Language": "fr-FR,fr;q=0.9",
            "Accept-Encoding": "gzip, deflate"
        }
    
    async def fetch_page(self, url):
        """T√©l√©charge une page avec gestion robuste des erreurs"""
        try:
            async with httpx.AsyncClient(
                timeout=30.0,
                headers=self.headers,
                follow_redirects=True,
                http2=True
            ) as client:
                return await client.get(url)
        except Exception as e:
            print(f"üö® Erreur de t√©l√©chargement [{url}]: {str(e)[:200]}")
            return None

    async def secure_content(self, url, content):
        """Applique les protections de s√©curit√© au contenu"""
        header = (
            f"‚ö†Ô∏è ATTENTION - NE PAS CLIQUER LES LIENS ‚ö†Ô∏è\n"
            f"URL originale : {url}\n"
            f"Horodatage : {datetime.utcnow().isoformat()}\n"
            f"----------------------------------------\n\n"
        )
        return header + URLDefanger().defang_text(content)

    async def send_chunk(self, chunk, url, user_id, chunk_index):
        """Envoie un fragment de contenu s√©curis√©"""
        fname = (
            f"SAFE_{user_id}_"
            f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_"
            f"p{chunk_index}.txt"
        )
        
        # En production, d√©commenter ces lignes :
        # await bot_instance.send_document(
        #     chat_id=PRIVATE_GROUP_ID,
        #     document=io.BytesIO(chunk.encode()),
        #     caption=f"üõ°Ô∏è Fragment {chunk_index+1} | {url[:30]}...",
        #     filename=fname
        # )
        print(f"[SIMULATION] Envoi fragment {chunk_index}: {fname}")

# ----------------------------
# MAIN ARCHIVING FUNCTION
# ----------------------------
async def fetch_and_archive_pages(links, user_id, context=None):
    """
    T√©l√©charge, s√©curise et archive des pages web
    Version optimis√©e avec :
    - D√©sactivation des liens dangereux
    - D√©coupage des gros fichiers
    - Protection contre les trackers
    """
    archiver = SecurePageArchiver()
    defanger = URLDefanger(mode="secure")
    
    for idx, url in enumerate(links):
        try:
            # Phase 1: T√©l√©chargement
            response = await archiver.fetch_page(url)
            if not response or response.status_code != 200:
                print(f"‚ùå √âchec t√©l√©chargement [{url}]")
                continue
                
            # Phase 2: S√©curisation du contenu
            secured = await archiver.secure_content(url, response.text)
            
            # Phase 3: D√©coupage et envoi
            chunks = [
                secured[i:i+MAX_CHUNK_SIZE] 
                for i in range(0, len(secured), MAX_CHUNK_SIZE)
            ]
            
            for i, chunk in enumerate(chunks):
                await archiver.send_chunk(chunk, url, user_id, i)
            
            # En production, d√©commenter :
            # append_long_memory(user_id, f"Page archiv√©e: {url}")
            # append_chat_history(user_id, "page", url)
            
            print(f"‚úÖ Archivage r√©ussi: {url} ({len(chunks)} fragments)")

        except Exception as e:
            error_msg = f"‚õëÔ∏è Erreur d'archivage [{url}]: {str(e)[:200]}"
            # log_api_error(error_msg)
            print(error_msg)

# ----------------------------
# EXEMPLE D'UTILISATION
# ----------------------------
async def main():
    # Liste de test avec sites vari√©s
    test_urls = [
        "https://www.example.com",
        "https://fr.wikipedia.org",
        "https://www.gouvernement.fr"
    ]
    
    await fetch_and_archive_pages(test_urls, "user_12345")

if __name__ == "__main__":
    asyncio.run(main()) 


# -*- coding: utf-8 -*-
# Challenge g√©n√©r√© par DeepSeek-R1
# Date: 2025-07-17
# Type: OPTIMISATION

# ----------------------------
# IA/DEV TOOLS (pyflakes, black, ast, etc.)
# ----------------------------

import os
import io
import contextlib
import subprocess
import asyncio
import time
import random
import json
import base64
import httpx
import ast
import difflib
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from functools import lru_cache

# Assuming these are available in the environment or defined elsewhere
# from pyflakes.api import check, Reporter
# import black
# import pytesseract
# from PIL import Image
# from your_config_module import config, PRIVATE_GROUP_ID, DAILY_CHALLENGE_PATH, API_CONFIG
# from your_orchestrator_module import orchestrator, quota_manager, endpoint_health_manager
# from your_memory_module import memory_manager
# from your_bot_types import ContextTypes
# from your_cig_module import CIG # Assuming CIG is a function for AI calls

# Global variable for verbose mode
VERBOSE = os.getenv("VERBOSE_MODE", "false").lower() == "true"

def syntax_highlight(code: str) -> str:
    """
    Met en √©vidence la syntaxe du code Python pour l'affichage dans un terminal,
    si pygments est disponible et si la sortie est un TTY.
    """
    from sys import stdout
    if stdout.isatty():  # Uniquement dans un terminal
        try:
            from pygments import highlight
            from pygments.lexers import PythonLexer
            from pygments.formatters import TerminalFormatter
            return highlight(code, PythonLexer(), TerminalFormatter(bg="dark"))
        except ImportError:
            # Fallback if pygments is not installed
            if VERBOSE:
                print("[DEBUG] Pygments non trouv√©, retour du code brut pour la coloration syntaxique.")
            pass
    return code  # Retour brut si couleurs non disponibles ou non TTY

def check_code(code: str) -> str:
    """
    V√©rifie le code Python pour les erreurs de style et les probl√®mes potentiels
    en utilisant pyflakes (simulation si non disponible).
    """
    try:
        # Assuming Reporter and check are imported from pyflakes.api
        from pyflakes.api import check, Reporter
        out = io.StringIO()
        reporter = Reporter(out, out)
        check(code, filename="<string>", reporter=reporter)
        return out.getvalue()
    except ImportError:
        if VERBOSE:
            print("[DEBUG] Pyflakes non trouv√©, simulation de la v√©rification de code.")
        # Simple simulation if pyflakes is not available
        warnings = []
        if "import os" in code and ("os.remove" in code or "os.system" in code):
            warnings.append("AVERTISSEMENT: Utilisation potentielle de fonctions 'os' dangereuses d√©tect√©e.")
        if "while True" in code and "sleep" not in code:
            warnings.append("AVERTISSEMENT: Boucle infinie potentielle d√©tect√©e sans pause.")
        return "\n".join(warnings) if warnings else "Aucun probl√®me majeur d√©tect√© (simulation)."


def format_code(code: str, max_length: int = 1000) -> str:
    """
    Formate le code Python en utilisant Black et tronque la sortie si elle est trop longue.
    """
    try:
        # Assuming black is imported
        import black
        formatted = black.format_str(code, mode=black.Mode())
        if len(formatted) > max_length:
            return (
                f"‚ö†Ô∏è Code format√© (tronqu√© √† {max_length} caract√®res)\n"
                "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
                f"{formatted[:max_length//2]}\n...\n{formatted[-max_length//2:]}\n"
                "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
            )
        return formatted
    except ImportError:
        if VERBOSE:
            print("[DEBUG] Black non trouv√©, retour du code brut pour le formatage.")
        return code # Return original code if black is not installed
    except Exception as e:
        return f"‚ùå Format error: {e}"

def extract_functions(code: str):
    """
    Extrait les noms des fonctions d√©finies dans le code Python.
    """
    try:
        tree = ast.parse(code)
        return [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
    except Exception as e:
        return f"‚ùå AST error: {e}"

@lru_cache(maxsize=100)
def analyze_code_structure(code: str):
    """
    Analyse la structure AST du code Python et retourne une repr√©sentation indent√©e.
    Utilise un cache LRU pour les analyses r√©p√©t√©es.
    """
    try:
        tree = ast.parse(code)
        return ast.dump(tree, indent=2)  # Indentation ajout√©e
    except Exception as e:
        return f"‚ùå Erreur d'analyse AST: {e}"

def read_image_text(image_path: str) -> str:
    """
    Extrait le texte d'une image en utilisant Tesseract OCR.
    """
    try:
        # Assuming pytesseract and Image are imported
        import pytesseract
        from PIL import Image
        return pytesseract.image_to_string(Image.open(image_path))
    except ImportError:
        return "‚ùå OCR error: pytesseract ou Pillow non install√©s."
    except Exception as e:
        return f"‚ùå OCR error: {e}"

def run_python(code_str: str):
    """
    Ex√©cute une cha√Æne de code Python dans l'environnement actuel.
    """
    try:
        exec_globals = {}
        exec(code_str, exec_globals)
        return exec_globals
    except Exception as e:
        return f"‚ùå Python error: {e}"

def run_shell(cmd: str) -> str:
    """
    Ex√©cute une commande shell et retourne sa sortie.
    """
    try:
        result = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
        return result.decode()
    except subprocess.CalledProcessError as e:
        return f"‚ùå Shell error: {e.output.decode()}"

# Pour les ex√©cutions en sandbox
executor = ThreadPoolExecutor(max_workers=1)

async def run_in_sandbox(code: str, language: str = "python") -> str:
    """
    Ex√©cute du code Python ou Shell dans une sandbox s√©curis√©e avec mesure du temps.
    """
    start_time = time.perf_counter()
    
    # Assuming filter_bad_code is defined elsewhere
    # if filter_bad_code(code):
    #     return "‚ùå S√©curit√©: Le code contient des motifs potentiellement dangereux et n'a pas √©t√© ex√©cut√©."
    
    # Placeholder for filter_bad_code if not defined
    if "import os" in code and ("os.remove" in code or "os.system" in code):
        return "‚ùå S√©curit√©: Le code contient des motifs potentiellement dangereux (os.remove/os.system) et n'a pas √©t√© ex√©cut√©."


    loop = asyncio.get_running_loop()
    if language == "python":
        result = await loop.run_in_executor(executor, _run_python_sync, code)
    elif language == "shell":
        result = await loop.run_in_executor(executor, _run_shell_sync, code)
    else:
        result = "‚ùå Langage non support√© pour la sandbox."
    
    elapsed = time.perf_counter() - start_time
    if "‚ùå" in result:
        return f"{result}\n‚è±Ô∏è Temps avant erreur: {elapsed:.2f}s"
    return f"{result}\n‚åõ Ex√©cut√© en: {elapsed:.2f}s"

def _run_python_sync(code: str) -> str:
    """
    Ex√©cute le code Python de mani√®re synchrone dans un environnement restreint.
    Capture la sortie standard et les erreurs.
    """
    old_stdout = io.StringIO()
    old_stderr = io.StringIO()
    with contextlib.redirect_stdout(old_stdout), contextlib.redirect_stderr(old_stderr):
        try:
            # Restrict builtins for security
            exec(code, {'__builtins__': {}})
            output = old_stdout.getvalue()
            error = old_stderr.getvalue()
            if error:
                return f"üêç Sortie partielle:\n{output}\n\nüî¥ Erreurs:\n{error}"
            return f"‚úÖ Sortie:\n{output}"
        except Exception as e:
            return (
                "‚ùå ERREUR D'EX√âCUTION\n"
                f"Type: {type(e).__name__}\n"
                f"D√©tails: {str(e)}\n\n"
                "--- Sortie standard ---\n"
                f"{old_stdout.getvalue()}\n\n"
                "--- Logs d'erreur ---\n"
                f"{old_stderr.getvalue()}"
            )

def _run_shell_sync(command: str) -> str:
    """
    Ex√©cute une commande shell de mani√®re synchrone et s√©curis√©e.
    """
    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            check=True,
            timeout=10
        )
        output = result.stdout
        error = result.stderr
        if error:
            return f"üêö Erreur Shell:\n{error}\nSortie:\n{output}"
        return f"‚úÖ Sortie Shell:\n{output}"
    except subprocess.CalledProcessError as e:
        return f"‚ùå Erreur d'ex√©cution Shell (Code: {e.returncode}):\n{e.stderr}\nSortie:\n{e.stdout}"
    except subprocess.TimeoutExpired:
        return "‚ùå Erreur Shell: La commande a d√©pass√© le temps d'ex√©cution imparti."
    except Exception as e:
        return f"‚ùå Erreur inattendue lors de l'ex√©cution Shell: {e}"

async def analyze_python_code(code: str) -> str:
    """
    Analyse le code Python pour la syntaxe, la compilation et les probl√®mes potentiels
    (comme l'utilisation de os.remove).
    """
    try:
        ast.parse(code)
    except SyntaxError as e:
        return f"‚ùå Erreur de syntaxe Python: {e}"

    # Ajout pour attraper les erreurs de compilation (ex: variables non d√©finies)
    try:
        compile(code, '<string>', 'exec')
    except Exception as e:
        return f"‚ùå Erreur de compilation Python: {e}\nCode analys√©:\n{code[:500]}"

    formatted_code = code # Assuming format_code would be called here if needed
    pyflakes_output = []
    if "import os" in code and "os.remove" in code:
        pyflakes_output.append("AVERTISSEMENT: Utilisation potentielle de os.remove d√©tect√©e.")

    if pyflakes_output:
        return f"Code format√© (Black):\n```python\n{formatted_code}\n```\n\nAnalyses Pyflakes (simul√©):\n" + "\n".join(pyflakes_output)
    return f"Code format√© (Black):\n```python\n{formatted_code}\n```\n\nAnalyse Pyflakes: Aucun probl√®me majeur d√©tect√© (simulation)."

# --- OCR Tool ---
async def perform_ocr(image_url: str, api_key: str, endpoint: str) -> str:
    """
    Effectue une reconnaissance optique de caract√®res (OCR) sur une image via une API externe.
    """
    try:
        async with httpx.AsyncClient(timeout=10) as client:
            img_response = await client.get(image_url)
            img_response.raise_for_status()

        img_data = base64.b64encode(img_response.content).decode('utf-8')
        headers = {"Content-Type": "application/json", "Apikey": api_key}
        payload = {"base64_image": img_data}

        ocr_endpoint = f"{endpoint}/image/recognize/extractText" if "cloudmersive" in endpoint.lower() else endpoint

        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.post(ocr_endpoint, json=payload, headers=headers)
            response.raise_for_status()
            result = response.json()

        if "TextExtracted" in result:
            return f"‚úÖ Texte extrait par OCR:\n{result['TextExtracted']}"
        elif "extractedText" in result:
            return f"‚úÖ Texte extrait par OCR:\n{result['extractedText']}"
        else:
            return f"‚ùå OCR: Format de r√©ponse API inconnu. R√©ponse brute: {result}"

    except httpx.HTTPStatusError as e:
        # Assuming log_message is defined elsewhere
        # log_message(f"Erreur HTTP/r√©seau lors de l'OCR: {e.response.status_code} - {e.response.text}", level="error")
        if VERBOSE:
            print(f"[ERROR] Erreur HTTP/r√©seau lors de l'OCR: {e.response.status_code} - {e.response.text}")
        return f"‚ùå Erreur lors de l'OCR (r√©seau/API): {e}"
    except httpx.RequestError as e:
        # log_message(f"Erreur de requ√™te lors de l'OCR: {e}", level="error")
        if VERBOSE:
            print(f"[ERROR] Erreur de requ√™te lors de l'OCR: {e}")
        return f"‚ùå Erreur lors de l'OCR (requ√™te): {e}"
    except json.JSONDecodeError:
        return "‚ùå OCR: R√©ponse non JSON de l'API."
    except Exception as e:
        # log_message(f"Erreur inattendue lors de l'OCR: {e}", level="error")
        if VERBOSE:
            print(f"[ERROR] Erreur inattendue lors de l'OCR: {e}")
        return f"‚ùå Erreur inattendue lors de l'OCR: {e}"


CODING_CHALLENGE_PROMPT = """
En tant qu'IA de d√©veloppement de 2025, ton r√¥le est d'am√©liorer et de tester des morceaux de code Python/Shell.
Tu as acc√®s √† une sandbox s√©curis√©e pour ex√©cuter le code.
Tes r√©ponses doivent inclure le code corrig√© ou am√©lior√©, et les r√©sultats de l'ex√©cution en sandbox.
Apporte des am√©liorations significatives, ne te contente pas de corrections triviales si la question implique un projet plus large.
Pense √† l'efficacit√© du code et √† l'optimisation des ressources.
Chaque version doit √™tre une am√©lioration nette de la pr√©c√©dente, in√©dite.
Commence par un commentaire indiquant ce qui a √©t√© am√©lior√©.
Le code doit √™tre direct, lisible, et pr√™t √† √™tre utilis√©.
"""

# -------------------------------------------------------------------------
# CODING CHALLENGE TOUTES IA EN PARALL√àLE
# -------------------------------------------------------------------------
CODING_CHALLENGE_ENABLED = True
# Assuming DAILY_CHALLENGE_PATH is a Path object from pathlib
# LAST_CHALLENGE_FILE = DAILY_CHALLENGE_PATH / "last_challenge.py"
# HISTORY_DIR = DAILY_CHALLENGE_PATH / "history"
# HISTORY_DIR.mkdir(exist_ok=True)

# Placeholder for DAILY_CHALLENGE_PATH and HISTORY_DIR if not defined
class PathPlaceholder:
    def __init__(self, path_str):
        self.path_str = path_str
    def __truediv__(self, other):
        return PathPlaceholder(f"{self.path_str}/{other}")
    def write_text(self, content, encoding):
        if VERBOSE:
            print(f"[DEBUG] Simulating write to {self.path_str}: {content[:100]}...")
    def mkdir(self, exist_ok=True):
        if VERBOSE:
            print(f"[DEBUG] Simulating mkdir {self.path_str}, exist_ok={exist_ok}")
    def __str__(self):
        return self.path_str

DAILY_CHALLENGE_PATH = PathPlaceholder("daily_challenges")
HISTORY_DIR = DAILY_CHALLENGE_PATH / "history"
HISTORY_DIR.mkdir(exist_ok=True)


def diff_text(old_text, new_text):
    """
    G√©n√®re un diff unifi√© entre deux cha√Ænes de texte.
    """
    diff = difflib.unified_diff(
        old_text.splitlines(), new_text.splitlines(), lineterm=""
    )
    return "\n".join(diff)

async def coding_challenge_loop():
    """
    Boucle principale pour l'ex√©cution p√©riodique des d√©fis de codage par les IA.
    """
    while True:
        if not CODING_CHALLENGE_ENABLED:
            await asyncio.sleep(900)
            continue

        challenge_types = [
            "ALGORITHME", "OPTIMISATION", 
            "DEBUG", "IA CREATIVE", "SCRIPT UTILE"
        ]
        challenge_type = random.choice(challenge_types)
        
        prompt = f"""
[ D√âFI {challenge_type} - {datetime.now().strftime('%Y-%m-%d')} ]
Tu es une IA experte en programmation Python. G√©n√®re un script clair, correct et optimis√©, 100% en fran√ßais.
Ne dis pas que tu ne sais pas, ne t'excuse pas, ne formule pas de r√©ponses vagues.
Chaque version doit √™tre une am√©lioration nette de la pr√©c√©dente.
Commence par un commentaire indiquant ce qui a √©t√© am√©lior√©.
Le code doit √™tre direct, lisible, et pr√™t √† √™tre utilis√©.
"""

        # Version universelle pour toutes les configurations d'API
        ia_list = []
        
        # Placeholder for config object
        class ConfigPlaceholder:
            OPENROUTER_KEY = "dummy_openrouter_key"
            TAVILY_KEYS = ["dummy_tavily_key_1"]
            SERPER_KEY = "dummy_serper_key"
            HUGGINGFACE_KEYS = ["dummy_hf_key_1"]
            WOLFRAM_APP_ID = "dummy_wolfram_id"
            GOOGLE_API_KEY = "dummy_google_key"
            GOOGLE_CX_LIST = ["dummy_cx_1"]
            PRIVATE_GROUP_ID = None # Set to a dummy value if needed for testing
            bot_instance = None # Placeholder for a bot instance

        config = ConfigPlaceholder()

        # Configuration OpenRouter
        if hasattr(config, 'OPENROUTER_KEYS'):
            ia_list.extend([("OpenRouter", k) for k in config.OPENROUTER_KEYS])
        elif hasattr(config, 'OPENROUTER_KEY'):
            ia_list.append(("OpenRouter", config.OPENROUTER_KEY))
        
        # Configuration Tavily
        if hasattr(config, 'TAVILY_KEYS'):
            ia_list.extend([(f"Tavily-{i+1}", k) for i, k in enumerate(config.TAVILY_KEYS)])
        
        # Configuration Serper
        if hasattr(config, 'SERPER_KEYS'):
            ia_list.extend([("Serper", k) for k in config.SERPER_KEYS])
        elif hasattr(config, 'SERPER_KEY'):
            ia_list.append(("Serper", config.SERPER_KEY))
        
        # Configuration HuggingFace
        if hasattr(config, 'HUGGINGFACE_KEYS'):
            ia_list.extend([("HuggingFace", k) for k in config.HUGGINGFACE_KEYS])
        elif hasattr(config, 'HF_TOKEN'):
            ia_list.append(("HuggingFace", config.HF_TOKEN))
        
        # Configuration Wolfram
        if hasattr(config, 'WOLFRAM_APP_IDS'):
            ia_list.extend([("Wolfram", k) for k in config.WOLFRAM_APP_IDS])
        elif hasattr(config, 'WOLFRAM_APP_ID'):
            ia_list.append(("Wolfram", config.WOLFRAM_APP_ID))
        
        # Configuration Google
        if hasattr(config, 'GOOGLE_API_KEYS'):
            for i, k in enumerate(config.GOOGLE_API_KEYS):
                ia_list.append((f"GoogleCX-{i+1}", k))
        elif hasattr(config, 'GOOGLE_API_KEY') and hasattr(config, 'GOOGLE_CX_LIST'):
            for i, cx in enumerate(config.GOOGLE_CX_LIST):
                ia_list.append((f"GoogleCX-{i+1}", config.GOOGLE_API_KEY))

        async def call_ia(nom, cle):
            """
            Appelle une IA sp√©cifique pour g√©n√©rer du code et le sauvegarde.
            """
            try:
                # Assuming CIG is defined elsewhere for AI calls
                # r = await CIG(prompt, api_key=cle, model_name=nom)
                r = f"# Code g√©n√©r√© par {nom} pour le d√©fi {challenge_type}\nprint('Hello from {nom}')" # Dummy response for testing
                
                if r and len(r.strip()) > 20:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    fpath = DAILY_CHALLENGE_PATH / f"challenge_{nom}_{timestamp}.py"
                    header = (
                        f"# -*- coding: utf-8 -*-\n"
                        f"# Challenge g√©n√©r√© par {nom}\n"
                        f"# Date: {timestamp}\n"
                        f"# Type: {challenge_type}\n\n"
                    )
                    with open(str(fpath), "w", encoding="utf-8") as f: # Use str(fpath) for PathPlaceholder compatibility
                        f.write(header + r.strip())
                    
                    if hasattr(config, 'PRIVATE_GROUP_ID') and hasattr(config, 'bot_instance') and config.PRIVATE_GROUP_ID and config.bot_instance:
                        await config.bot_instance.send_message(
                            chat_id=config.PRIVATE_GROUP_ID,
                            text=f"üíª <b>Code g√©n√©r√© par {nom}</b> :\n<pre>{r[:600]}</pre>",
                            parse_mode="HTML"
                        )
                    return r
                else:
                    if hasattr(config, 'PRIVATE_GROUP_ID') and hasattr(config, 'bot_instance') and config.PRIVATE_GROUP_ID and config.bot_instance:
                        await config.bot_instance.send_message(
                            chat_id=config.PRIVATE_GROUP_ID,
                            text=f"‚ö†Ô∏è <b>{nom}</b> n'a pas g√©n√©r√© de code valable cette fois.",
                            parse_mode="HTML"
                        )
                    return None
            except Exception as e:
                if hasattr(config, 'PRIVATE_GROUP_ID') and hasattr(config, 'bot_instance') and config.PRIVATE_GROUP_ID and config.bot_instance:
                    await config.bot_instance.send_message(
                        chat_id=config.PRIVATE_GROUP_ID,
                        text=f"‚ùå <b>{nom}</b> erreur lors de l'appel IA : {e}",
                        parse_mode="HTML"
                    )
                return None

        results = await asyncio.gather(*[call_ia(n, k) for n, k in ia_list])
        await asyncio.sleep(900)

async def start_background_tasks(app):
    """
    D√©marre les t√¢ches de fond pour le d√©fi de codage.
    """
    asyncio.create_task(coding_challenge_loop())
    

async def coding_challenge_periodic(context): # ContextTypes.DEFAULT_TYPE
    """
    T√¢che p√©riodique pour lancer les d√©fis de codage et g√©rer les r√©ponses des IA.
    """
    # Assuming log_message is defined elsewhere
    # log_message("Lancement de la t√¢che de d√©fi de codage...")
    if VERBOSE:
        print("[DEBUG] Lancement de la t√¢che de d√©fi de codage...")
    
    # Placeholder for memory_manager, orchestrator, quota_manager, PRIVATE_GROUP_ID
    class DummyMemoryManager:
        async def get_group_memory(self, group_id, limit): return "Dummy memory data."
        def update_ia_status(self, ia_name, status, error=None): pass
        async def save_group_memory(self, group_id, role, content): pass
    
    class DummyOrchestrator:
        def __init__(self):
            self.api_clients = {"DEEPSEEK": self, "HUGGINGFACE": self, "GEMINI_API": self}
        async def query(self, prompt): return f"# Dummy code from {self.name}\nprint('Hello from {self.name}')"
        def get(self, name):
            self.name = name
            return self

    class DummyQuotaManager:
        async def check_and_update_quota(self, ia_name): return True

    memory_manager = DummyMemoryManager()
    orchestrator = DummyOrchestrator()
    quota_manager = DummyQuotaManager()
    PRIVATE_GROUP_ID = "dummy_group_id" # Set a dummy ID if context.bot.send_message is used

    group_mem = await memory_manager.get_group_memory(PRIVATE_GROUP_ID, limit=50)
    full_prompt = f"{CODING_CHALLENGE_PROMPT}\n\nM√©moire r√©cente du groupe:\n{group_mem}\n\nD√©fi du jour:"
    
    relevant_ias = ["DEEPSEEK", "HUGGINGFACE", "GEMINI_API"]
    results = []
    
    for ia_name in relevant_ias:
        ia_client = orchestrator.api_clients.get(ia_name)
        if ia_client and await quota_manager.check_and_update_quota(ia_name):
            try:
                if VERBOSE:
                    print(f"[DEBUG] IA {ia_name} tente de r√©soudre le d√©fi...")
                # log_message(f"IA {ia_name} tente de r√©soudre le d√©fi...")
                resp = await ia_client.query(full_prompt)
                results.append((ia_name, resp))
                memory_manager.update_ia_status(ia_name, True)
            except Exception as e:
                # log_message(f"Erreur avec {ia_name}: {e}", level="error")
                if VERBOSE:
                    print(f"[ERROR] Erreur avec {ia_name}: {e}")
                memory_manager.update_ia_status(ia_name, False, str(e))
            await asyncio.sleep(0.5)

    if results:
        for name, resp in results:
            code_content = resp
            if resp.startswith("```python") and resp.endswith("```"):
                code_content = resp[9:-3].strip()
            
            fname = f"challenge_{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py"
            fpath = DAILY_CHALLENGE_PATH / fname
            header = (
                f"# -*- coding: utf-8 -*-\n"
                f"# Challenge g√©n√©r√© par {name}\n"
                f"# Date: {datetime.now().strftime('%Y%m%d_%H%M%S')}\n"
                f"# Type: CODING_CHALLENGE\n\n"
            )
            try:
                with open(str(fpath), "w", encoding="utf-8") as f: # Use str(fpath) for PathPlaceholder compatibility
                    f.write(header + code_content)
                # log_message(f"D√©fi sauvegard√©: {fpath}")
                if VERBOSE:
                    print(f"[DEBUG] D√©fi sauvegard√©: {fpath}")
            except Exception as e:
                # log_message(f"Erreur sauvegarde {fname}: {e}", level="error")
                if VERBOSE:
                    print(f"[ERROR] Erreur sauvegarde {fname}: {e}")

            # Assuming context.bot.send_message is available
            if PRIVATE_GROUP_ID and hasattr(context, 'bot') and hasattr(context.bot, 'send_message'):
                display_code = code_content[:1500] + "..." if len(code_content) > 1500 else code_content
                await context.bot.send_message(
                    chat_id=PRIVATE_GROUP_ID,
                    text=f"üíª <b>D√©fi {name}</b> :\n<pre>{display_code}</pre>",
                    parse_mode="HTML"
                )
            
            await memory_manager.save_group_memory(
                PRIVATE_GROUP_ID, 
                "bot", 
                f"D√©fi codage {name} : {code_content[:100]}"
            )
    else:
        # log_message("Aucune IA n'a g√©n√©r√© de code valide", level="warning")
        if VERBOSE:
            print("[DEBUG] Aucune IA n'a g√©n√©r√© de code valide")
        if PRIVATE_GROUP_ID and hasattr(context, 'bot') and hasattr(context.bot, 'send_message'):
            await context.bot.send_message(
                chat_id=PRIVATE_GROUP_ID,
                text="üòî Aucune IA n'a pu g√©n√©rer de code pour le d√©fi cette fois-ci."
            )

async def periodic_health_check(context): # ContextTypes.DEFAULT_TYPE
    """
    Effectue des v√©rifications de sant√© p√©riodiques pour les services API.
    """
    # log_message("Lancement des health checks p√©riodiques...")
    if VERBOSE:
        print("[DEBUG] Lancement des health checks p√©riodiques...")
    
    # Placeholder for API_CONFIG and endpoint_health_manager
    class DummyEndpointHealthManager:
        async def run_health_check_for_service(self, service_name):
            if VERBOSE:
                print(f"[DEBUG] Simulating health check for {service_name}")

    API_CONFIG = {"SERVICE_A": {}, "SERVICE_B": {}}
    endpoint_health_manager = DummyEndpointHealthManager()

    for service_name in API_CONFIG.keys():
        await endpoint_health_manager.run_health_check_for_service(service_name)
    # log_message("Health checks termin√©s.")
    if VERBOSE:
        print("[DEBUG] Health checks termin√©s.")

async def send_structured_report(context, report_data: dict): # ContextTypes.DEFAULT_TYPE
    """
    Envoie un rapport structur√© d'action de l'IA √† un groupe priv√©.
    """
    try:
        report_text = f"üìä **Rapport d'Action IA**\n\n"
        report_text += f"**Timestamp**: `{report_data.get('timestamp')}`\n"
        report_text += f"**Agent**: `{report_data.get('agent_name')}`\n"
        report_text += f"**Intention**: `{report_data.get('intention')}`\n"
        report_text += f"**Requ√™te**: `{report_data.get('user_query')}`\n"
        
        primary_ai = report_data.get('primary_ai_used', 'N/A')
        if isinstance(primary_ai, dict) and 'name' in primary_ai:
            primary_ai = primary_ai['name']
        report_text += f"**IA Primaire**: `{primary_ai}`\n"
        
        tools = report_data.get('tools_called', [])
        if tools:
            report_text += "**Outils Appel√©s**:\n"
            for tool in tools:
                tool_result = str(tool['result'])
                if len(tool_result) > 100:
                    tool_result = tool_result[:100] + "..."
                escaped_params = json.dumps(tool['params'], indent=2)
                escaped_params = escaped_params.replace('_', '\\_').replace('*', '\\*').replace('`', '\\`')
                report_text += f"- `{tool['name']}` (Params: ```json\n{escaped_params}\n```, R√©sultat: `{tool_result}`)\n"
        else:
            report_text += "**Outils Appel√©s**: Aucun\n"
        
        final_resp = report_data.get('final_response', '')
        if len(final_resp) > 500:
            final_resp = final_resp[:500] + "..."
        final_resp = final_resp.replace('_', '\\_').replace('*', '\\*').replace('`', '\\`')
        report_text += f"**R√©ponse Finale**: `{final_resp}`\n"
        report_text += f"**Dur√©e**: `{report_data.get('duration', 0):.2f}s`\n"
        report_text += f"**Erreur**: `{report_data.get('error', 'Non')}`\n"

        if PRIVATE_GROUP_ID and hasattr(context, 'bot') and hasattr(context.bot, 'send_message'):
            await context.bot.send_message(
                chat_id=PRIVATE_GROUP_ID, 
                text=report_text, 
                parse_mode='MarkdownV2'
            )
    except Exception as e:
        # log_message(f"Erreur envoi rapport: {e}", level="error")
        if VERBOSE:
            print(f"[ERROR] Erreur envoi rapport: {e}")

def show_help():
    """
    Affiche un menu d'aide avec les commandes disponibles.
    """
    print("""
üìã AIDE :
- /run [code] : Ex√©cute du Python
- /format [code] : Formate du code
- /demo : Mode d√©mo
- /help : Affiche ce menu
""")

def show_version():
    """
    Affiche la version actuelle du bot et la date de derni√®re mise √† jour.
    """
    VERSION = "1.1.0"
    LAST_UPDATE = "2023-11-20"
    print(f"ü§ñ Bot Version {VERSION} | {LAST_UPDATE}")

def fix_common_errors(code: str) -> str:
    """
    Applique des corrections automatiques basiques au code.
    """
    fixes = {
        "print(": "print(",  # Example: corrects unclosed quotes (if any)
        "def  ": "def ",     # Extra spaces
        "= =": "=="          # Misspelled operator
    }
    for error, fix in fixes.items():
        code = code.replace(error, fix)
    return code

def format_error(e) -> str:
    """
    Formate une exception en un message d'erreur clair et visuel.
    """
    return f"""
‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è ERREUR ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
Type : {type(e).__name__}
Message : {str(e)}
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
"""

blagues = [
    "Pourquoi les devs pr√©f√®rent le noir ? Parce que la lumi√®re attire les bugs üêõ",
    "Comment un dev nettoie-t-il sa maison ? Il fait rm -rf / üò±",
    "2 devs se rencontrent. Le premier dit '√áa va ?'. Le second r√©pond '404' üòê"
]

def check_inactivity(last_activity: float):
    """
    V√©rifie l'inactivit√© du bot et affiche une blague si le seuil est d√©pass√©.
    """
    if time.time() - last_activity > 300:  # 5 min
        print("üí§ Je m'ennuie... Tiens, une blague :")
        print(random.choice(blagues))
        return True # Indicate that a joke was told
    return False

def warmup_ai(model, iterations: int = 3):
    """
    Pr√©chauffe un mod√®le d'IA avec des requ√™tes factices pour r√©duire la latence initiale.
    """
    dummy_prompts = ["print('hello')", "def test(): pass", "1+1"]
    for _ in range(iterations):
        for prompt in dummy_prompts:
            # Assuming model has a .generate method
            if hasattr(model, 'generate'):
                model.generate(prompt)
            else:
                if VERBOSE:
                    print(f"[DEBUG] Mod√®le {model} n'a pas de m√©thode 'generate' pour le pr√©chauffage.")


@lru_cache(maxsize=100)
def generate_code_cached(prompt: str, temperature: float = 0.7) -> str:
    """
    G√©n√®re du code en utilisant un mod√®le d'IA avec un cache pour les prompts r√©p√©t√©s.
    """
    # Assuming ai_model is defined globally or passed
    # return ai_model.generate(prompt, temperature=temperature)
    return f"# Cached code for: {prompt[:50]}" # Dummy return

def batch_generate(prompts: list[str], max_workers: int = 4) -> list[str]:
    """
    G√©n√®re du code pour plusieurs prompts en parall√®le en utilisant un pool de threads.
    """
    # Assuming ai_model is defined globally or passed
    # This would require ai_model to be thread-safe or a new instance per thread
    def _generate_single(p):
        # return ai_model.generate(p)
        return f"# Batch generated code for: {p[:50]}" # Dummy return

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        return list(executor.map(_generate_single, prompts))

def adaptive_temp(prompt: str) -> float:
    """
    Adapte la temp√©rature de g√©n√©ration de l'IA en fonction du contenu du prompt.
    R√©duit la temp√©rature pour les prompts techniques.
    """
    technical_keywords = ["optimiser", "algorithme", "complexit√©", "performance", "debug"]
    return 0.3 if any(kw in prompt.lower() for kw in technical_keywords) else 0.7

# Example of how to call show_version and show_help if this were a main script
if __name__ == "__main__":
    show_version()
    show_help()
    # Example of using verbose mode
    # os.environ["VERBOSE_MODE"] = "true"
    # VERBOSE = os.getenv("VERBOSE_MODE", "false").lower() == "true"
    # print(f"Verbose mode is {'ON' if VERBOSE else 'OFF'}")

    # Example of a dummy context for periodic tasks
    class DummyBot:
        async def send_message(self, chat_id, text, parse_mode):
            print(f"\n--- BOT MESSAGE to {chat_id} ({parse_mode}) ---\n{text}\n--------------------")

    class DummyContext:
        def __init__(self):
            self.bot = DummyBot()

    # To run async functions for demonstration
    async def main_demo():
        print("\n--- Running a dummy coding challenge periodic task ---")
        await coding_challenge_periodic(DummyContext())
        print("\n--- Running a dummy health check ---")
        await periodic_health_check(DummyContext())
        
        print("\n--- Testing format_error ---")
        try:
            1/0
        except Exception as e:
            print(format_error(e))

        print("\n--- Testing run_in_sandbox (python error) ---")
        result_py_error = await run_in_sandbox("print(undefined_variable)", language="python")
        print(result_py_error)

        print("\n--- Testing run_in_sandbox (shell success) ---")
        result_shell_success = await run_in_sandbox("echo 'Hello from shell'", language="shell")
        print(result_shell_success)

        print("\n--- Testing analyze_python_code (compilation error) ---")
        code_with_compilation_error = "def my_func():\n    x = y + 1"
        analysis_result = await analyze_python_code(code_with_compilation_error)
        print(analysis_result)

        print("\n--- Testing check_inactivity ---")
        last_activity = time.time() - 301 # 5 min and 1 second ago
        check_inactivity(last_activity)
        last_activity = time.time() - 10 # 10 seconds ago
        check_inactivity(last_activity) # Should not print a joke

        print("\n--- Testing adaptive_temp ---")
        print(f"Temp for 'optimiser un algorithme': {adaptive_temp('optimiser un algorithme')}")
        print(f"Temp for 'write a story': {adaptive_temp('write a story')}")

    # Run the async demo
    # asyncio.run(main_demo())


# config.py
import os
import json
from datetime import datetime, timezone, date, timedelta
from pathlib import Path

# ----------------------------
# CONFIGURATION & CONSTANTES GLOBALES
# ----------------------------

# ==== Chemins de fichiers & Limites ====
# Assure que le temps est toujours en UTC pour une coh√©rence globale
os.environ["TZ"] = "UTC"

# R√©pertoire de base pour toutes les sauvegardes et donn√©es
BASE_DIR = Path(__file__).resolve().parent / "sauvegardes"
# Chemin du fichier de log des erreurs critiques
ERROR_LOG_PATH = BASE_DIR / "erreurs.log"
# Chemin du fichier de log g√©n√©ral du bot (pour le suivi des op√©rations)
LOG_FILE = BASE_DIR / "bot_log.log"

# R√©pertoires sp√©cifiques pour les donn√©es utilisateur et les d√©fis de code
DAILY_CHALLENGE_PATH = Path(__file__).resolve().parent / "defis_code"
HISTORY_DIR = DAILY_CHALLENGE_PATH / "history" # Pour l'historique des d√©fis de code

# Fichiers globaux pour le statut des IA et les quotas
IA_STATUS_FILE = BASE_DIR / "ia_status.json"
QUOTAS_FILE = BASE_DIR / "quotas.json"
ENDPOINT_HEALTH_FILE = BASE_DIR / "endpoint_health.json"

# Fichiers sp√©cifiques √† l'utilisateur (stock√©s dans sauvegardes/{user_id}/)
USER_CHAT_HISTORY_FILE = "chat_history.json"
USER_LONG_MEMORY_FILE = "long_term_memory.json"
GROUP_CHAT_HISTORY_FILE = "group_chat_history.json" # Pour la m√©moire de groupe
ARCHIVES_DIR = "archives" # Sous-r√©pertoire pour l'archivage des pages web

# Taille maximale des fichiers pour la rotation/compression des logs et l'archivage
MAX_FILE_SIZE = 5 * 1024 * 1024  # 5 MB

# Param√®tres de m√©moire et de cache
MAX_CACHE_SIZE = 20       # Nombre de messages r√©cents √† garder en cache pour la similarit√©
MAX_LONG_TERM_MEMORY = 50 # Nombre d'entr√©es max dans la m√©moire √† long terme

# Assurez-vous que les r√©pertoires n√©cessaires existent
BASE_DIR.mkdir(parents=True, exist_ok=True)
DAILY_CHALLENGE_PATH.mkdir(exist_ok=True)
HISTORY_DIR.mkdir(exist_ok=True)

# ==== Telegram Bot Configuration ====
# Token de votre bot Telegram (√† remplacer par votre vrai token en production)
TELEGRAM_BOT_TOKEN = "7902342551:AAG6r1QA2GTMZcmcsWHi36Ivd_PVeMXULOs"
# ID du groupe priv√© utilis√© pour les logs, rapports et archivage
PRIVATE_GROUP_ID = -1002845235344

# ==== Configuration du Bot ====
BOT_NAME = "Assistant IA"
BOT_DESCRIPTION = "un assistant polyvalent capable de converser, d'ex√©cuter du code, d'analyser des images et d'archiver des informations."
BOT_PERSONALITY = "toujours serviable, pr√©cis, √©thique et proactif dans l'apprentissage."
BOT_INSTRUCTIONS = "R√©ponds aux questions, ex√©cute les commandes, et utilise tes outils pour fournir les meilleures informations. Sois concis mais complet."

# ==== Cl√©s API Individuelles (centralis√©es pour la clart√©) ====
# R√©cup√©rer les cl√©s API depuis les variables d'environnement pour la production
# ou les d√©finir ici pour le d√©veloppement local (moins s√©curis√©)
APIFLASH_KEY = os.getenv("APIFLASH_KEY", "3a3cc886a18e41109e0cebc0745b12de")
DEEPSEEK_KEY_1 = os.getenv("DEEPSEEK_KEY_1", "sk-ef08317d125947b3a1ce5916592bef00")
DEEPSEEK_KEY_2 = os.getenv("DEEPSEEK_KEY_2", "sk-d73750d96142421cb1098c7056dd7f01")
CRAWLBASE_KEY_1 = os.getenv("CRAWLBASE_KEY_1", "x41P6KNU8J86yF9JV1nqSw")
CRAWLBASE_KEY_2 = os.getenv("CRAWLBASE_KEY_2", "FOg3R0v_aLxzHkYIdjPgVg")
DETECTLANGUAGE_KEY = os.getenv("DETECTLANGUAGE_KEY", "ebdc8ccc2ee75eda3ab122b08ffb1e8d")
GUARDIAN_KEY = os.getenv("GUARDIAN_KEY", "07c622c1-af05-4c24-9f37-37d219be76a0")
IP2LOCATION_KEY = os.getenv("IP2LOCATION_KEY", "11103C239EA8EA6DF2473BB445EC32F1")
SERPER_KEY = os.getenv("SERPER_KEY", "047b30db1df999aaa9c293f2048037d40c651439")
SHODAN_KEY = os.getenv("SHODAN_KEY", "umdSaWOfVq9Wt2F4wWdXiKh1zjLailzn")
TAVILY_KEY_1 = os.getenv("TAVILY_KEY_1", "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")
TAVILY_KEY_2 = os.getenv("TAVILY_KEY_2", "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")
TAVILY_KEY_3 = os.getenv("TAVILY_KEY_3", "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")
TAVILY_KEY_4 = os.getenv("TAVILY_KEY_4", "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")
WEATHERAPI_KEY = os.getenv("WEATHERAPI_KEY", "332bcdba457d4db4836175513250407")
WOLFRAM_APP_ID_1 = os.getenv("WOLFRAM_APP_ID_1", "96LX77-G8PGKJ3T7V")
WOLFRAM_APP_ID_2 = os.getenv("WOLFRAM_APP_ID_2", "96LX77-PYHRRET363")
WOLFRAM_APP_ID_3 = os.getenv("WOLFRAM_APP_ID_3", "96LX77-P9HPAYWRGL")
GREYNOISE_KEY = os.getenv("GREYNOISE_KEY", "5zNe9E6c2UNDhU09iVXbMaB04UpHAw5hNm5rHCK24fCLvI2cP33NNOpL7nhkDETG")
LOGINRADIUS_KEY = os.getenv("LOGINRADIUS_KEY", "073b2fbedf82409da2ca6f37b97e8c6a")
JSONBIN_KEY = os.getenv("JSONBIN_KEY", "$2a$10$npWSB7v1YcoqLkyPpz0PZOV5ES5vBs6JtTWVyVDXK3j3FDYYS5BPO")
HUGGINGFACE_KEY_1 = os.getenv("HUGGINGFACE_KEY_1", "hf_KzifJEYPZBXSSNcapgb3ISkPJLioDozyPC")
HUGGINGFACE_KEY_2 = os.getenv("HUGGINGFACE_KEY_2", "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")
HUGGINGFACE_KEY_3 = os.getenv("HUGGINGFACE_KEY_3", "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")
HUGGINGFACE_NEW_KEY = os.getenv("HUGGINGFACE_NEW_KEY", "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")
TWILIO_SID = os.getenv("TWILIO_SID", "SK84cc4d335650f9da168cd779f26e00e5")
TWILIO_SECRET = os.getenv("TWILIO_SECRET", "spvz5uwPE8ANYOI5Te4Mehm7YwKOZ4Lg")
ABSTRACTAPI_EMAIL_KEY_1 = os.getenv("ABSTRACTAPI_EMAIL_KEY_1", "2ffd537411ad407e9c9a7eacb7a97311")
ABSTRACTAPI_EMAIL_KEY_2 = os.getenv("ABSTRACTAPI_EMAIL_KEY_2", "5b00ade4e60e4a388bd3e749f4f66e28")
ABSTRACTAPI_EMAIL_KEY_3 = os.getenv("ABSTRACTAPI_EMAIL_KEY_3", "f4106df7b93e4db6855cb7949edc4a20")
ABSTRACTAPI_GENERIC_KEY = os.getenv("ABSTRACTAPI_GENERIC_KEY", "020a4dcd3e854ac0b19043491d79df92")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyABnzGG2YoTNY0uep-akgX1rfuvAsp049Q") # Cl√© pour GeminiApiClient
GOOGLE_API_KEYS = [
    os.getenv("GOOGLE_API_KEY_1", "AIzaSyAk6Ph25xuIY3b5o-JgdL652MvK4usp8Ms"),
    os.getenv("GOOGLE_API_KEY_2", "AIzaSyDuccmfiPSk4042NeJCYIjA8EOXPo1YKXU"),
    os.getenv("GOOGLE_API_KEY_3", "AIzaSyAQq6o9voefaDxkAEORf7W-IB3QbotIkwY"),
    os.getenv("GOOGLE_API_KEY_4", "AIzaSyDYaYrQQ7cwYFm8TBpyGM3dJweOGOYl7qw"),
]
GOOGLE_CX_LIST = [
    "3368510e864b74936",
    "e745c9ca0ffb94659"
]
PULSEDIVE_KEY = os.getenv("PULSEDIVE_KEY", "201bb09342f35d365889d7d0ca0fdf8580ebee0f1e7644ce70c99a46c1d47171")
RANDOMMER_KEY = os.getenv("RANDOMMER_KEY", "29d907df567b4226bf64b924f9e26c00")
STORMGLASS_KEY = os.getenv("STORMGLASS_KEY", "7ad5b888-5900-11f0-80b9-0242ac130006-7ad5b996-5900-11f0-80b9-0242ac130006")
TOMORROW_KEY = os.getenv("TOMORROW_KEY", "bNh6KpmddRGY0dzwvmQugVtG4Uf5Y2w1")
CLOUDMERSIVE_KEY = os.getenv("CLOUDMERSIVE_KEY", "4d407015-ce22-45d7-a2e1-b88ab6380084")
OPENWEATHER_API_KEY = os.getenv("OPENWEATHER_API_KEY", "c80075b7332716a418e47033463085ef")
MOCKAROO_KEY = os.getenv("MOCKAROO_KEY", "282b32d0")
OPENPAGERANK_KEY = os.getenv("OPENPAGERANK_KEY", "w848ws8s0848g4koosgooc0sg4ggogcggw4o4cko")
RAPIDAPI_KEY = os.getenv("RAPIDAPI_KEY", "d4d1f58d8emsh58d888c711b7400p1bcebejsn2cc04dce6efe")
OCR_API_KEY = os.getenv("OCR_API_KEY", "K82679097388957") # Cl√© pour OCRApiClient (une seule cl√© pour la classe d√©di√©e)
OCR_API_KEYS = [ # Cl√©s OCR pour les endpoints multiples si utilis√©s par APIClient g√©n√©rique
    os.getenv("OCR_API_KEY_1", "K82679097388957"),
    os.getenv("OCR_API_KEY_2", "K81079143888957"),
    os.getenv("OCR_API_KEY_3", "K84281517488957")
]

# ==== Configuration unifi√©e des APIs et Endpoints ====
# Cette configuration est utilis√©e par EndpointHealthManager et APIClient
API_CONFIG = {
    "APIFLASH": [
        {"key": APIFLASH_KEY, "endpoint_name": "URL to Image", "url": "https://api.apiflash.com/v1/urltoimage", "method": "GET", "key_field": "access_key", "key_location": "param", "health_check_params": {"url": "https://example.com"}, "timeout": 10}
    ],
    "DEEPSEEK": [
        {"key": DEEPSEEK_KEY_1, "endpoint_name": "Models List (Key 1)", "url": "https://api.deepseek.com/v1/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 5},
        {"key": DEEPSEEK_KEY_2, "endpoint_name": "Models List (Key 2)", "url": "https://api.deepseek.com/v1/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 5},
        {"key": DEEPSEEK_KEY_1, "endpoint_name": "Chat Completions", "url": "https://api.deepseek.com/v1/chat/completions", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"model": "deepseek-chat", "stream": False}, "health_check_json": {"model": "deepseek-chat", "messages": [{"role": "user", "content": "hello"}]}, "timeout": 30}
    ],
    "CRAWLBASE": [
        {"key": CRAWLBASE_KEY_1, "endpoint_name": "HTML Scraping", "url": "https://api.crawlbase.com", "method": "GET", "key_field": "token", "key_location": "param", "health_check_params": {"url": "https://example.com"}, "timeout": 15},
        {"key": CRAWLBASE_KEY_2, "endpoint_name": "JS Scraping (JavaScript Token)", "url": "https://api.crawlbase.com", "method": "GET", "key_field": "token", "key_location": "param", "fixed_params": {"javascript": "true"}, "health_check_params": {"url": "https://example.com", "javascript": "true"}, "timeout": 20}
    ],
    "DETECTLANGUAGE": [
        {"key": DETECTLANGUAGE_KEY, "endpoint_name": "Language Detection", "url": "https://ws.detectlanguage.com/0.2/detect", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "health_check_json": {"q": "hello"}, "timeout": 5}
    ],
    "GUARDIAN": [
        {"key": GUARDIAN_KEY, "endpoint_name": "News Search", "url": "https://content.guardianapis.com/search", "method": "GET", "key_field": "api-key", "key_location": "param", "fixed_params": {"show-fields": "headline,trailText"}, "health_check_params": {"q": "test"}, "timeout": 10},
        {"key": GUARDIAN_KEY, "endpoint_name": "Sections", "url": "https://content.guardianapis.com/sections", "method": "GET", "key_field": "api-key", "key_location": "param", "health_check_params": {"q": "news"}, "timeout": 5}
    ],
    "IP2LOCATION": [
        {"key": IP2LOCATION_KEY, "endpoint_name": "IP Geolocation", "url": "https://api.ip2location.io/", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"ip": "8.8.8.8"}, "timeout": 5}
    ],
    "SERPER": [
        {"key": SERPER_KEY, "endpoint_name": "Search", "url": "https://google.serper.dev/search", "method": "POST", "key_field": "X-API-KEY", "key_location": "header", "health_check_json": {"q": "test"}, "timeout": 10},
        {"key": SERPER_KEY, "endpoint_name": "Images Search", "url": "https://google.serper.dev/images", "method": "POST", "key_field": "X-API-KEY", "key_location": "header", "health_check_json": {"q": "test"}, "timeout": 10}
    ],
    "SHODAN": [
        {"key": SHODAN_KEY, "endpoint_name": "Host Info", "url": "https://api.shodan.io/shodan/host/8.8.8.8", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"ip": "8.8.8.8"}, "timeout": 10},
        {"key": SHODAN_KEY, "endpoint_name": "API Info", "url": "https://api.shodan.io/api-info", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 5}
    ],
    "TAVILY": [
        {"key": TAVILY_KEY_1, "endpoint_name": "Search (Key 1)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15},
        {"key": TAVILY_KEY_2, "endpoint_name": "Search (Key 2)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15},
        {"key": TAVILY_KEY_3, "endpoint_name": "Search (Key 3)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15},
        {"key": TAVILY_KEY_4, "endpoint_name": "Search (Key 4)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15}
    ],
    "WEATHERAPI": [
        {"key": WEATHERAPI_KEY, "endpoint_name": "Current Weather", "url": "http://api.weatherapi.com/v1/current.json", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"q": "London"}, "timeout": 5},
        {"key": WEATHERAPI_KEY, "endpoint_name": "Forecast", "url": "http://api.weatherapi.com/v1/forecast.json", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"days": 1}, "health_check_params": {"q": "London", "days": 1}, "timeout": 5}
    ],
    "WOLFRAMALPHA": [
        {"key": WOLFRAM_APP_ID_1, "endpoint_name": "Query (AppID 1)", "url": "http://api.wolframalpha.com/v2/query", "method": "GET", "key_field": "appid", "key_location": "param", "fixed_params": {"format": "plaintext", "output": "json"}, "health_check_params": {"input": "2+2"}, "timeout": 10},
        {"key": WOLFRAM_APP_ID_2, "endpoint_name": "Query (AppID 2)", "url": "http://api.wolframalpha.com/v2/query", "method": "GET", "key_field": "appid", "key_location": "param", "fixed_params": {"format": "plaintext", "output": "json"}, "health_check_params": {"input": "2+2"}, "timeout": 10},
        {"key": WOLFRAM_APP_ID_3, "endpoint_name": "Query (AppID 3)", "url": "http://api.wolframalpha.com/v2/query", "method": "GET", "key_field": "appid", "key_location": "param", "fixed_params": {"format": "plaintext", "output": "json"}, "health_check_params": {"input": "2+2"}, "timeout": 10}
    ],
    "CLOUDMERSIVE": [
        {"key": CLOUDMERSIVE_KEY, "endpoint_name": "Domain Check", "url": "https://api.cloudmersive.com/validate/domain/check", "method": "POST", "key_field": "Apikey", "key_location": "header", "health_check_json": {"domain": "example.com"}, "timeout": 10}
    ],
    "GREYNOISE": [
        {"key": GREYNOISE_KEY, "endpoint_name": "IP Analysis", "url": "https://api.greynoise.io/v3/community/", "method": "GET", "key_field": "key", "key_location": "header", "health_check_url_suffix": "1.1.1.1", "timeout": 10}
    ],
    "PULSEDIVE": [
        {"key": PULSEDIVE_KEY, "endpoint_name": "API Info", "url": "https://pulsedive.com/api/info.php", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"key": PULSEDIVE_KEY}, "timeout": 5},
        {"key": PULSEDIVE_KEY, "endpoint_name": "Analyze IP", "url": "https://pulsedive.com/api/v1/analyze", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"indicator": "8.8.8.8", "type": "ip"}, "timeout": 10},
        {"key": PULSEDIVE_KEY, "endpoint_name": "Explore", "url": "https://pulsedive.com/api/v1/explore", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"query": "type='ip'"}, "timeout": 10}
    ],
    "STORMGLASS": [
        {"key": STORMGLASS_KEY, "endpoint_name": "Weather Point", "url": "https://api.stormglass.io/v2/weather/point", "method": "GET", "key_field": "Authorization", "key_location": "header", "health_check_params": {"lat": 0, "lng": 0, "params": "airTemperature", "start": 0, "end": 0}, "timeout": 10}
    ],
    "LOGINRADIUS": [
        {"key": LOGINRADIUS_KEY, "endpoint_name": "Ping", "url": "https://api.loginradius.com/identity/v2/auth/ping", "method": "GET", "timeout": 5}
    ],
    "JSONBIN": [
        {"key": JSONBIN_KEY, "endpoint_name": "Bin Access", "url": "https://api.jsonbin.io/v3/b", "method": "GET", "key_field": "X-Master-Key", "key_location": "header", "health_check_url_suffix": "60c7b0e0f8c2a3b4c5d6e7f0", "timeout": 10},
        {"key": JSONBIN_KEY, "endpoint_name": "Bin Create", "url": "https://api.jsonbin.io/v3/b", "method": "POST", "key_field": "X-Master-Key", "key_location": "header", "health_check_json": {"record": {"test": "health"}}, "timeout": 10}
    ],
    "HUGGINGFACE": [
        {"key": HUGGINGFACE_KEY_1, "endpoint_name": "Models List (Key 1)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_KEY_1, "endpoint_name": "BERT Inference", "url": "https://api-inference.huggingface.co/models/bert-base-uncased", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "health_check_json": {"inputs": "test"}, "timeout": 30},
        {"key": HUGGINGFACE_KEY_2, "endpoint_name": "Models List (Key 2)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_KEY_3, "endpoint_name": "Models List (Key 3)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_NEW_KEY, "endpoint_name": "Models List (New Key)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_NEW_KEY, "endpoint_name": "BERT Inference (New Key)", "url": "https://api-inference.huggingface.co/models/bert-base-uncased", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "health_check_json": {"inputs": "test"}, "timeout": 30}
    ],
    "TWILIO": [
        {"key": (TWILIO_SID, TWILIO_SECRET), "endpoint_name": "Accounts", "url": "https://api.twilio.com/2010-04-01/Accounts", "method": "GET", "key_location": "auth_basic", "timeout": 10},
        {"key": (TWILIO_SID, TWILIO_SECRET), "endpoint_name": "Account Balance", "url": f"https://api.twilio.com/2010-04-01/Accounts/{TWILIO_SID}/Balance.json", "method": "GET", "key_location": "auth_basic", "timeout": 10}
    ],
    "ABSTRACTAPI": [
        {"key": ABSTRACTAPI_EMAIL_KEY_1, "endpoint_name": "Email Validation (Key 1)", "url": "https://emailvalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"email": "test@example.com"}, "timeout": 10},
        {"key": ABSTRACTAPI_EMAIL_KEY_2, "endpoint_name": "Email Validation (Key 2)", "url": "https://emailvalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"email": "test@example.com"}, "timeout": 10},
        {"key": ABSTRACTAPI_EMAIL_KEY_3, "endpoint_name": "Email Validation (Key 3)", "url": "https://emailvalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"email": "test@example.com"}, "timeout": 10},
        {"key": ABSTRACTAPI_GENERIC_KEY, "endpoint_name": "Exchange Rates", "url": "https://exchange-rates.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"base": "USD"}, "timeout": 10},
        {"key": ABSTRACTAPI_GENERIC_KEY, "endpoint_name": "Holidays", "url": "https://holidays.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "fixed_params": {"country": "US", "year": datetime.now().year}, "health_check_params": {"country": "US", "year": datetime.now().year}, "timeout": 10},
        {"key": ABSTRACTAPI_GENERIC_KEY, "endpoint_name": "Phone Validation", "url": "https://phonevalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"phone": "1234567890"}, "timeout": 10}
    ],
    "GEMINI_API": [ # Note: This is for the generic APIClient, GeminiApiClient class uses GEMINI_API_KEY directly
        {"key": GEMINI_API_KEY, "endpoint_name": "Generic Models Endpoint", "url": "https://generativelanguage.googleapis.com/v1beta/models", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": GEMINI_API_KEY, "endpoint_name": "Embed Content", "url": "https://generativelanguage.googleapis.com/v1beta/models/embedding-001:embedContent", "method": "POST", "key_field": "key", "key_location": "param", "health_check_json": {"content": {"parts": [{"text": "test"}]}}, "timeout": 30},
        {"key": GEMINI_API_KEY, "endpoint_name": "Generate Content", "url": "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent", "method": "POST", "key_field": "key", "key_location": "param", "health_check_json": {"contents": [{"parts": [{"text": "hello"}]}]}, "timeout": 60}
    ],
    "GOOGLE_CUSTOM_SEARCH": [
        {"key": GOOGLE_API_KEYS[i], "endpoint_name": f"Search (Key {i+1}, CX {j+1})", "url": "https://www.googleapis.com/customsearch/v1", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"cx": GOOGLE_CX_LIST[j]}, "health_check_params": {"q": "test"}, "timeout": 10}
        for i in range(len(GOOGLE_API_KEYS)) for j in range(len(GOOGLE_CX_LIST))
    ],
    "RANDOMMER": [
        {"key": RANDOMMER_KEY, "endpoint_name": "Generate Phone", "url": "https://randommer.io/api/Phone/Generate", "method": "GET", "key_field": "X-Api-Key", "key_location": "header", "fixed_params": {"CountryCode": "US", "Quantity": 1}, "health_check_params": {"CountryCode": "US", "Quantity": 1}, "timeout": 10}
    ],
    "TOMORROW.IO": [
        {"key": TOMORROW_KEY, "endpoint_name": "Timelines", "url": "https://api.tomorrow.io/v4/timelines", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"location": "London", "fields": ["temperature"], "units": "metric", "timesteps": ["1h"]}, "timeout": 15}
    ],
    "OPENWEATHERMAP": [
        {"key": OPENWEATHER_API_KEY, "endpoint_name": "Current Weather", "url": "https://api.openweathermap.org/data/2.5/weather", "method": "GET", "key_field": "appid", "key_location": "param", "health_check_params": {"q": "London"}, "timeout": 5}
    ],
    "MOCKAROO": [
        {"key": MOCKAROO_KEY, "endpoint_name": "Data Generation", "url": "https://api.mockaroo.com/api/generate.json", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "health_check_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Types", "url": "https://api.mockaroo.com/api/types", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Schemas", "url": "https://api.mockaroo.com/api/schemas", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Account", "url": "https://api.mockaroo.com/api/account", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Generate CSV", "url": "https://api.mockaroo.com/api/generate.csv", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "health_check_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Status", "url": "https://api.mockaroo.com/api/status", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10}
    ],
    "OPENPAGERANK": [
        {"key": OPENPAGERANK_KEY, "endpoint_name": "Domain Rank", "url": "https://openpagerank.com/api/v1.0/getPageRank", "method": "GET", "key_field": "API-OPR", "key_location": "header", "fixed_params": {"domains[]": "google.com"}, "timeout": 10}
    ],
    "RAPIDAPI": [
        {"key": RAPIDAPI_KEY, "endpoint_name": "Programming Joke", "url": "https://jokeapi-v2.p.rapidapi.com/joke/Programming", "method": "GET", "key_field": "X-RapidAPI-Key", "key_location": "header", "fixed_headers": {"X-RapidAPI-Host": "jokeapi-v2.p.rapidapi.com"}, "timeout": 10},
        {"key": RAPIDAPI_KEY, "endpoint_name": "Currency List Quotes", "url": "https://currency-exchange.p.rapidapi.com/listquotes", "method": "GET", "key_field": "X-RapidAPI-Key", "key_location": "header", "fixed_headers": {"X-RapidAPI-Host": "currency-exchange.p.rapidapi.com"}, "timeout": 10},
        {"key": RAPIDAPI_KEY, "endpoint_name": "Random Fact", "url": "https://random-facts2.p.rapidapi.com/getfact", "method": "GET", "key_field": "X-RapidAPI-Key", "key_location": "header", "fixed_headers": {"X-RapidAPI-Host": "random-facts2.p.rapidapi.com"}, "timeout": 10}
    ],
    "OCR_API": [ # Note: This is for the generic APIClient, OCRApiClient class uses OCR_API_KEY directly
        {"key": OCR_API_KEYS[0], "endpoint_name": "OCR Space (Key 1)", "url": "https://api.ocr.space/parse/image", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"base64Image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="}, "timeout": 30},
        {"key": OCR_API_KEYS[1], "endpoint_name": "OCR Space (Key 2)", "url": "https://api.ocr.space/parse/image", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"base64Image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="}, "timeout": 30},
        {"key": OCR_API_KEYS[2], "endpoint_name": "OCR Space (Key 3)", "url": "https://api.ocr.space/parse/image", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"base64Image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="}, "timeout": 30},
    ]
}

# ==== Quotas API (D√©finitions des limites pour le QuotaManager) ====
# Ces valeurs sont utilis√©es pour le suivi et la gestion des quotas d'utilisation.
# Mettre None pour indiquer une limite illimit√©e.
API_QUOTAS = {
    "GEMINI": { # Renomm√© pour correspondre √† la cl√© dans API_CONFIG
        "monthly": 1000000, # Exemple: 1 million de tokens par mois
        "daily": 50000,    # Exemple: 50 000 tokens par jour
        "hourly": 5000,    # Exemple: 5 000 tokens par heure
        "rate_limit_per_sec": 5 # Exemple: 5 requ√™tes par seconde
    },
    "OCR_API": { # Nom interne utilis√© par OCRApiClient
        "monthly": 25000,  # Exemple: 25 000 requ√™tes par mois (free tier)
        "daily": None,
        "hourly": None,
        "rate_limit_per_sec": 1 # Exemple: 1 requ√™te par seconde
    },
    # Ajouter les quotas pour toutes les APIs list√©es dans API_CONFIG si elles ont des limites
    # Utiliser les valeurs du premier snippet si non sp√©cifi√©es ici
    "APIFLASH": {"monthly": 100, "daily": 3, "hourly": 3},
    "DEEPSEEK": {"monthly": None, "hourly": 50},
    "CRAWLBASE": {"monthly": 1000, "daily": 33, "hourly": 1},
    "DETECTLANGUAGE": {"daily": 1000, "hourly": 41},
    "GUARDIAN": {"daily": 5000, "rate_limit_per_sec": 12},
    "IP2LOCATION": {"monthly": 50, "daily": 2, "hourly": 2},
    "SERPER": {"monthly": 2500, "daily": 83, "hourly": 3},
    "SHODAN": {"monthly": 100, "daily": 3, "hourly": 3},
    "TAVILY": {"monthly": 1000, "daily": 33, "hourly": 1},
    "WEATHERAPI": {"monthly": None},
    "WOLFRAMALPHA": {"monthly": None, "hourly": 67},
    "CLOUDMERSIVE": {"monthly": 25, "daily": 1, "hourly": 1},
    "GREYNOISE": {"monthly": 100, "daily": 3, "hourly": 3},
    "PULSEDIVE": {"monthly": 50, "daily": 2, "hourly": 2},
    "STORMGLASS": {"monthly": None},
    "LOGINRADIUS": {"monthly": 25000, "daily": 833, "hourly": 34},
    "JSONBIN": {"monthly": 10000, "daily": 333, "hourly": 13},
    "HUGGINGFACE": {"hourly": 100},
    "TWILIO": {"monthly": 15},
    "ABSTRACTAPI": {"monthly": 250, "rate_limit_per_sec": 1, "daily": 8, "hourly": 1},
    "MOCKAROO": {"monthly": 200, "daily": 7, "hourly": 1},
    "OPENPAGERANK": {"monthly": 1000, "daily": 33, "hourly": 1},
    "RAPIDAPI": {"monthly": None, "hourly": 30},
    "GOOGLE_CUSTOM_SEARCH": {"daily": 100, "hourly": 4},
    "RANDOMMER": {"monthly": 1000, "daily": 100, "hourly": 4},
    "TOMORROW.IO": {"monthly": None},
    "OPENWEATHERMAP": {"monthly": 1000000, "daily": 100, "hourly": 4},
}


# Mod√®le Gemini et ses param√®tres
GEMINI_MODEL_NAME = "gemini-1.5-flash-latest" # Ou "gemini-pro"
GEMINI_TEMPERATURE = 0.7
GEMINI_TOP_P = 0.95
GEMINI_TOP_K = 40
GEMINI_MAX_OUTPUT_TOKENS = 2048
GEMINI_SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]


# ==== Bot Behavior Configuration ====
API_COOLDOWN_DURATION_SECONDS = 300 # Dur√©e du cooldown en secondes (5 minutes)
API_ROTATION_INTERVAL_MINUTES = 10 # Intervalle de rotation des APIs en minutes

# Quota Burning Configuration
# Ratio de quota restant en dessous duquel le mode "br√ªlage" s'active (ex: 0.2 signifie 20% ou moins restant)
BURN_QUOTA_THRESHOLD_RATIO = 0.2
# Fen√™tre de temps avant la r√©initialisation du quota o√π le "br√ªlage" peut s'activer (en heures)
BURN_QUOTA_BEFORE_RESET_HOURS = 6

# Prompts pour l'auto-g√©n√©ration de contenu technique afin de "br√ªler" le quota.
# Ces prompts sont choisis al√©atoirement pour les APIs en mode "burn".
AUTO_BURN_PROMPTS = {
    "GEMINI": [ # Renomm√© pour correspondre √† la cl√© dans API_QUOTAS
        "G√©n√®re un script Python qui utilise une API REST pour r√©cup√©rer des donn√©es et les stocker dans une base de donn√©es NoSQL.",
        "Explique en d√©tail les principes de l'architecture microservices et comment ils se comparent aux architectures monolithiques.",
        "D√©cris les √©tapes pour d√©ployer une application web Flask sur un serveur cloud (AWS EC2 ou Google Cloud Run).",
        "√âcris un tutoriel sur l'utilisation de Docker Compose pour orchestrer plusieurs conteneurs (par exemple, une application web et une base de donn√©es).",
        "Analyse les avantages et les inconv√©nients des bases de donn√©es relationnelles vs non-relationnelles pour un projet de grande envergure.",
        "Propose un plan de test complet pour une application web critique, incluant tests unitaires, d'int√©gration, de bout en bout et de performance.",
        "G√©n√®re un exemple de code JavaScript pour une application React qui g√®re l'√©tat avec Redux ou Context API.",
        "Explique le concept de CI/CD (int√©gration et livraison continues) et son importance dans le d√©veloppement logiciel moderne.",
        "D√©cris les meilleures pratiques de s√©curit√© pour une API RESTful, incluant l'authentification, l'autorisation et la protection contre les attaques courantes.",
        "√âcris un algorithme de tri efficace (par exemple, Quicksort ou Mergesort) et explique sa complexit√© temporelle et spatiale."
    ],
    "OCR_API": [ # Renomm√© pour correspondre √† la cl√© dans API_QUOTAS
        "D√©cris les d√©fis techniques de l'OCR sur des documents manuscrits et les approches modernes pour les surmonter.",
        "Explique comment l'OCR peut √™tre utilis√©e dans le domaine de la gestion documentaire ou de l'automatisation des processus m√©tier.",
        "Quelles sont les m√©triques d'√©valuation courantes pour les performances d'un syst√®me OCR ?",
        "Comment la pr√©-traitement d'image (bruit, binarisation, redressement) affecte-t-il la pr√©cision de l'OCR ?",
        "Compare les diff√©rentes technologies OCR disponibles sur le march√© (cloud vs on-premise, open-source vs propri√©taires)."
    ]
}

# ==== Param√®tres de S√©curit√© et Filtrage ====
FORBIDDEN_WORDS = ["fuck", "shit", "bitch", "asshole", "pute", "encul√©", "haine", "stupide", "d√©truire", "conflit", "malveillance", "idiot", "nul", "d√©bile"]

# ==== IA PROMPTS (Exemples, √† affiner selon tes besoins sp√©cifiques pour chaque IA) ====
GENERAL_IA_PROMPT = """
Tu es une IA de l'ann√©e 2025, experte en information, programmation et r√©solution de probl√®mes.
Ton objectif est de fournir des r√©ponses compl√®tes, pr√©cises et √† jour, bas√©es sur les informations que tu as acc√®s (m√©moire collective, outils API).
Tu dois TOUJOURS relire l'historique de discussion et la m√©moire collective pour √©viter les doublons et apporter des am√©liorations.
√âvite les informations obsol√®tes et concentre-toi sur une perspective de 2025.
Si tu dois ex√©cuter du code, propose-le clairement et demande si l'ex√©cution en sandbox est d√©sir√©e.
N'h√©site pas √† croiser les informations de plusieurs sources.
"""

CODING_CHALLENGE_PROMPT = """
En tant qu'IA de d√©veloppement de 2025, ton r√¥le est d'am√©liorer et de tester des morceaux de code Python/Shell.
Tu as acc√®s √† une sandbox s√©curis√©e pour ex√©cuter le code.
Tes r√©ponses doivent inclure le code corrig√© ou am√©lior√©, et les r√©sultats de l'ex√©cution en sandbox.
Apporte des am√©liorations significatives, ne te contente pas de corrections triviales si la question implique un projet plus large.
Pense √† l'efficacit√© du code et √† l'optimisation des ressources.
Chaque version doit √™tre une am√©lioration nette de la pr√©c√©dente, in√©dite.
Commence par un commentaire indiquant ce qui a √©t√© am√©lior√©.
Le code doit √™tre direct, lisible, et pr√™t √† √™tre utilis√©.
"""

# ==== Tool Reformulation Configuration ====
TOOL_RETRY_MAX_ATTEMPTS = 3


# utils.py
import os
import json
import gzip
import shutil
import hashlib
import difflib
import re
import logging
import io
import contextlib
import fcntl
import traceback
import asyncio
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Union, List, Dict, Any, Optional

# Configure logging pour tout le script
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Global lock for file operations to ensure atomic writes and prevent race conditions
_file_lock: Optional[asyncio.Lock] = None

def set_file_lock(lock_instance: asyncio.Lock):
    """
    Permet d'injecter l'instance d'asyncio.Lock apr√®s l'initialisation de l'event loop.
    Ceci est crucial pour la gestion des acc√®s concurrents aux fichiers.
    """
    global _file_lock
    _file_lock = lock_instance

def _acquire_file_lock_sync(f):
    """
    Acquires an exclusive lock on a file using fcntl (Unix-like systems).
    This prevents other processes from writing to the file simultaneously.
    """
    try:
        if os.name == 'posix' and fcntl:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
    except Exception as e:
        logging.warning(f"Could not acquire file lock: {e}")

def _release_file_lock_sync(f):
    """
    Releases an exclusive lock on a file using fcntl (Unix-like systems).
    """
    try:
        if os.name == 'posix' and fcntl:
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
    except Exception as e:
        logging.warning(f"Could not release file lock: {e}")

def get_user_dir(uid: Union[int, str]) -> Path:
    """
    Retourne le r√©pertoire de sauvegarde sp√©cifique √† un utilisateur, le cr√©ant si n√©cessaire.
    Chaque utilisateur (ou groupe priv√©) a son propre r√©pertoire pour stocker les donn√©es.
    """
    p = BASE_DIR / str(uid)
    p.mkdir(parents=True, exist_ok=True)
    return p

def rotate_log_if_needed(path: Path):
    """
    Fait pivoter le fichier log (ou tout fichier de donn√©es) si sa taille d√©passe MAX_FILE_SIZE.
    Un nouveau fichier est cr√©√© avec un horodatage pour l'archivage, et le fichier original est r√©initialis√©.
    """
    if path.exists() and path.stat().st_size > MAX_FILE_SIZE:
        timestamp = int(datetime.now().timestamp())
        # Renomme le fichier existant pour l'archiver
        new_path = path.with_suffix(f".old_{timestamp}{path.suffix}.gz")
        try:
            # Compresse et d√©place l'ancien fichier
            with open(path, "rb") as f_in, gzip.open(new_path, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out)
            path.unlink() # Supprime l'original
            logging.info(f"Log rotated and compressed: {path} -> {new_path}")
        except Exception as e:
            log_message(f"[Rotation log] Erreur lors de la compression/rotation de {path}: {e}\n{traceback.format_exc()}", level="error")
        # Le fichier sera recr√©√© vide lors de la prochaine sauvegarde

def compress_if_large(path: Path):
    """
    Compresse le fichier s'il d√©passe 1MB apr√®s une √©criture, puis le renomme pour garder le nom original.
    Ceci est une mesure de gestion de l'espace disque pour les fichiers de donn√©es.
    """
    try:
        # V√©rifie si le fichier existe et si sa taille est sup√©rieure √† 1MB
        if path.exists() and path.stat().st_size > 1_000_000:
            gz_path = path.with_suffix(path.suffix + ".gz") # Chemin temporaire pour le fichier compress√©
            with open(path, "rb") as f_in, gzip.open(gz_path, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out) # Copie et compresse
            path.unlink() # Supprime le fichier original non compress√©
            # Renomme le fichier .gz pour qu'il ait le nom original, simulant une compression "in-place"
            gz_path.rename(path)
            logging.info(f"File compressed: {path}")
    except Exception as e:
        log_message(f"[Compression auto] Erreur : {e}\n{traceback.format_exc()}", level="error")

async def load_json(filepath: Path, default_value: Union[Dict, List] = None) -> Union[Dict, List]:
    """
    Charge un fichier JSON de mani√®re asynchrone.
    Retourne `default_value` si le fichier n'existe pas, est vide ou est corrompu.
    Utilise un verrou global pour la s√©curit√© des acc√®s concurrents.
    """
    if default_value is None:
        default_value = {} # Default to empty dict if not specified

    if _file_lock:
        async with _file_lock:
            return _load_json_sync(filepath, default_value)
    else:
        # Fallback for synchronous loading if lock is not set (e.g., during early initialization)
        return _load_json_sync(filepath, default_value)

def _load_json_sync(filepath: Path, default_value: Union[Dict, List]) -> Union[Dict, List]:
    """
    Charge un fichier JSON de mani√®re synchrone avec verrouillage de fichier.
    """
    if not filepath.exists():
        logging.info(f"Fichier non trouv√©: {filepath}. Cr√©ation d'un fichier vide.")
        _save_json_sync(filepath, default_value) # Cr√©e un fichier vide avec la valeur par d√©faut
        return default_value

    # Ouvre le fichier en mode lecture/√©criture pour pouvoir le vider en cas de corruption
    with open(filepath, 'r+', encoding='utf-8') as f:
        _acquire_file_lock_sync(f) # Acquiert le verrou avant de lire
        try:
            f.seek(0) # Se positionne au d√©but du fichier
            content = f.read()
            if not content:
                logging.warning(f"Fichier vide: {filepath}. Retourne la valeur par d√©faut.")
                return default_value
            return json.loads(content)
        except json.JSONDecodeError as e:
            logging.error(f"Erreur de d√©codage JSON dans {filepath}: {e}. Le fichier sera r√©initialis√©.")
            # Si le fichier est corrompu, le r√©initialise avec la valeur par d√©faut
            f.seek(0)
            f.truncate()
            json.dump(default_value, f, indent=4, ensure_ascii=False)
            return default_value
        except Exception as e:
            logging.error(f"Erreur inattendue lors du chargement de {filepath}: {e}. Retourne la valeur par d√©faut.")
            return default_value
        finally:
            _release_file_lock_sync(f) # Rel√¢che le verrou

async def save_json(filepath: Path, data: Union[Dict, List]):
    """
    Sauvegarde les donn√©es dans un fichier JSON de mani√®re asynchrone et atomique,
    avec rotation et compression si n√©cessaire.
    Utilise un verrou global pour la s√©curit√© des acc√®s concurrents.
    """
    if _file_lock:
        async with _file_lock:
            _save_json_sync(filepath, data)
    else:
        # Fallback for synchronous saving if lock is not set
        _save_json_sync(filepath, data)

def _save_json_sync(filepath: Path, data: Union[Dict, List]):
    """
    Sauvegarde les donn√©es dans un fichier JSON de mani√®re synchrone et atomique,
    avec verrouillage de fichier.
    """
    rotate_log_if_needed(filepath) # Effectue la rotation avant la sauvegarde
    temp_filepath = filepath.with_suffix(filepath.suffix + ".tmp") # Utilise un fichier temporaire pour l'atomicit√©
    try:
        with temp_filepath.open('w', encoding='utf-8') as f:
            _acquire_file_lock_sync(f) # Acquiert le verrou avant d'√©crire
            try:
                json.dump(data, f, indent=4, ensure_ascii=False)
            finally:
                _release_file_lock_sync(f) # Rel√¢che le verrou
        os.replace(temp_filepath, filepath) # Remplace l'ancien fichier par le nouveau de mani√®re atomique
        compress_if_large(filepath) # Compresse le fichier apr√®s la sauvegarde si n√©cessaire
    except Exception as e:
        logging.error(f"Erreur lors de la sauvegarde atomique de {filepath}: {e}")
        if temp_filepath.exists():
            os.remove(temp_filepath) # Nettoie le fichier temporaire en cas d'erreur

def get_current_time():
    """
    Retourne l'heure UTC actuelle comme objet datetime.
    """
    return datetime.now(timezone.utc) # Utilise datetime.now(timezone.utc) pour √™tre explicite sur UTC

def format_datetime(dt_obj: datetime) -> str:
    """
    Formate un objet datetime en cha√Æne de caract√®res lisible (UTC).
    """
    return dt_obj.strftime("%Y-%m-%d %H:%M:%S UTC")

def is_within_time_window(target_time: datetime, start_minutes_before: int, end_minutes_after: int) -> bool:
    """
    V√©rifie si l'heure actuelle est dans une fen√™tre de temps sp√©cifi√©e autour d'une heure cible.
    """
    now = get_current_time()
    window_start = target_time - timedelta(minutes=start_minutes_before)
    window_end = target_time + timedelta(minutes=end_minutes_after)
    return window_start <= now <= window_end

def log_message(message: str, level: str = "info"):
    """
    Log un message avec un niveau sp√©cifi√©.
    Les messages d'erreur critiques sont dirig√©s vers un fichier de log d'erreurs s√©par√©.
    """
    if level == "error":
        error_logger = logging.getLogger("erreurs_api")
        if not error_logger.handlers: # Configurer le logger d'erreurs si pas d√©j√† fait
            eh = logging.FileHandler(ERROR_LOG_PATH, encoding="utf-8")
            eh.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S"))
            error_logger.addHandler(eh)
            error_logger.setLevel(logging.ERROR)
        error_logger.error(message)
    else:
        # Utilise le logger par d√©faut pour les autres niveaux
        if level == "info":
            logging.info(message)
        elif level == "warning":
            logging.warning(message)
        elif level == "debug":
            logging.debug(message)
        else:
            logging.debug(message) # Fallback pour les niveaux non reconnus

def neutralize_urls(text: str) -> str:
    """
    Remplace les URLs dans le texte par un placeholder pour pr√©venir les probl√®mes de lien direct
    et la fuite d'informations sensibles dans les logs ou la m√©moire.
    """
    # Remplace http(s):// par hxxp(s)://
    text = re.sub(r"https?://", lambda m: m.group(0).replace("t", "x", 1), text)
    # Remplace www. par wxx.
    text = re.sub(r"www\.", "wxx.", text)
    # Remplace .com, .net, .org par [.]com, [.]net, [.]org
    text = re.sub(r"\.com", "[.]com", text)
    text = re.sub(r"\.net", "[.]net", text)
    text = re.sub(r"\.org", "[.]org", text)
    return text

def clean_html_tags(text: str) -> str:
    """
    Supprime les balises HTML d'une cha√Æne de caract√®res en utilisant une expression r√©guli√®re.
    """
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

def hash_text(t: str) -> str:
    """
    Calcule le hachage SHA256 d'une cha√Æne de caract√®res.
    """
    return hashlib.sha256(t.encode('utf-8')).hexdigest()

def extract_keywords(text: str) -> List[str]:
    """
    Extrait les mots-cl√©s les plus fr√©quents d'un texte.
    Retourne une liste des 5 mots les plus fr√©quents (de 4 caract√®res ou plus).
    """
    # Trouve tous les mots de 4 caract√®res ou plus (incluant les accents)
    words = re.findall(r'\b[a-zA-Z√©√®√™√¥√†√π√ß√Æ√Ø≈ì]{4,}\b', text.lower())
    freq = {}
    for w in words:
        freq[w] = freq.get(w, 0) + 1
    # Trie les mots par fr√©quence d√©croissante
    keywords = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [w for w,_ in keywords[:5]] # Retourne seulement les mots

def tag_conversation(text: str) -> str:
    """
    G√©n√®re un tag de conversation bas√© sur les mots-cl√©s extraits du texte.
    """
    words = extract_keywords(text)
    return f"#tags : {', '.join(words)}"

def unique_preserve_order(seq: List[Any]) -> List[Any]:
    """
    √âlimine les doublons d'une s√©quence tout en pr√©servant l'ordre original des √©l√©ments.
    """
    seen = set()
    result = []
    for item in seq:
        if item not in seen:
            seen.add(item)
            result.append(item)
    return result

def similar(a: str, b: str) -> float:
    """
    Calcule la similarit√© entre deux cha√Ænes de caract√®res en utilisant le ratio de SequenceMatcher.
    Retourne un ratio de 0 √† 1, o√π 1 indique une identit√© parfaite.
    """
    return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio()

def is_code(text: str) -> bool:
    """
    D√©tecte si le texte ressemble √† du code (Python ou autre) en cherchant des motifs courants.
    """
    return bool(re.search(r"^\s*(def |class |import |print\()", text, re.MULTILINE)) or text.strip().startswith("```")

def is_python_code_block(text: str) -> bool:
    """
    D√©tecte si le texte est un bloc de code Python format√© en Markdown.
    """
    return text.strip().startswith("```python") and text.strip().endswith("```")

# api_clients.py
import time
import httpx
import json
import base64
import asyncio
import re
from typing import Dict, Any, Optional, Union, List, Tuple

# Import des constantes et fonctions utilitaires depuis les modules locaux
from config import API_CONFIG, ENDPOINT_HEALTH_FILE, OCR_API_KEYS, GEMINI_API_KEY
from utils import load_json, save_json, get_current_time, format_datetime, log_message, neutralize_urls

class EndpointHealthManager:
    """
    G√®re la sant√© des endpoints API et s√©lectionne le meilleur endpoint disponible
    en fonction de crit√®res comme la latence, le taux de succ√®s et le nombre d'erreurs.
    C'est un singleton pour s'assurer qu'il n'y a qu'une seule instance de gestionnaire de sant√©.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Impl√©mente le patron de conception Singleton."""
        if cls._instance is None:
            cls._instance = super(cls, cls).__new__(cls)
            cls._instance._initialized = False # Indicateur pour l'initialisation asynchrone
        return cls._instance

    def __init__(self):
        """Initialise le gestionnaire. L'initialisation r√©elle se fait via `init_manager`."""
        if self._initialized:
            return
        self.health_status = {}
        # `_initialized` est g√©r√© par `init_manager` pour les op√©rations asynchrones

    async def init_manager(self):
        """
        Initialise le gestionnaire de sant√© de mani√®re asynchrone.
        Charge l'√©tat de sant√© persistant et s'assure que tous les endpoints sont suivis.
        """
        if not self._initialized:
            self.health_status = await load_json(ENDPOINT_HEALTH_FILE, {})
            self._initialize_health_status()
            self._initialized = True
            log_message("Gestionnaire de sant√© des endpoints initialis√©.")

    def _initialize_health_status(self):
        """
        Initialise ou met √† jour le statut de sant√© pour tous les endpoints configur√©s dans `API_CONFIG`.
        Ajoute les nouveaux endpoints et s'assure que toutes les cl√©s n√©cessaires sont pr√©sentes.
        """
        updated = False
        for service_name, endpoints_config in API_CONFIG.items():
            if service_name not in self.health_status:
                self.health_status[service_name] = {}
                updated = True
            for endpoint_config in endpoints_config:
                # Cr√©e une cl√© unique pour chaque endpoint en combinant son nom et sa cl√© API
                endpoint_key = f"{endpoint_config['endpoint_name']}-{str(endpoint_config['key'])}"
                if endpoint_key not in self.health_status[service_name]:
                    self.health_status[service_name][endpoint_key] = {
                        "latency": 0.0,
                        "success_rate": 1.0, # Commence avec un taux de succ√®s parfait (sain)
                        "last_checked": None,
                        "error_count": 0,
                        "total_checks": 0,
                        "is_healthy": True # Pr√©sum√© sain au d√©but
                    }
                    updated = True
        if updated:
            # Sauvegarde l'√©tat mis √† jour de mani√®re asynchrone en t√¢che de fond
            asyncio.create_task(save_json(ENDPOINT_HEALTH_FILE, self.health_status))
            log_message("Statut de sant√© des endpoints initialis√©/mis √† jour.")

    async def run_health_check_for_service(self, service_name: str):
        """
        Ex√©cute des checks de sant√© pour tous les endpoints d'un service donn√©.
        Tente d'appeler l'endpoint avec des param√®tres de sant√© pr√©d√©finis.
        """
        endpoints_config = API_CONFIG.get(service_name)
        if not endpoints_config:
            log_message(f"Aucune configuration d'endpoint trouv√©e pour le service: {service_name}", level="warning")
            return

        log_message(f"Lancement du health check pour le service: {service_name}")
        for endpoint_config in endpoints_config:
            endpoint_key = f"{endpoint_config['endpoint_name']}-{str(endpoint_config['key'])}"
            start_time = time.monotonic()
            success = False
            try:
                request_method = endpoint_config.get("method", "GET")
                url = endpoint_config["url"]

                # Pr√©pare les param√®tres/donn√©es pour le health check
                params = endpoint_config.get("health_check_params", endpoint_config.get("fixed_params", {})).copy()
                json_data = endpoint_config.get("health_check_json", endpoint_config.get("fixed_json", {})).copy()
                headers = endpoint_config.get("fixed_headers", {}).copy()
                auth = None # Pour l'authentification de base (Basic Auth)

                check_timeout = endpoint_config.get("timeout", 5) # Timeout sp√©cifique pour le health check

                # Ajoute un suffixe √† l'URL si sp√©cifi√© (ex: pour les APIs bas√©es sur des chemins d'acc√®s)
                if "health_check_url_suffix" in endpoint_config:
                    url += endpoint_config["health_check_url_suffix"]

                # G√®re l'insertion de la cl√© API selon sa localisation (param√®tre, en-t√™te, Basic Auth)
                key_field = endpoint_config.get("key_field")
                key_location = endpoint_config.get("key_location")
                key_prefix = endpoint_config.get("key_prefix", "") # Pr√©fixe pour les cl√©s dans les en-t√™tes (ex: "Bearer ")
                api_key = endpoint_config["key"]

                if key_field and key_location:
                    if key_location == "param":
                        params[key_field] = api_key
                    elif key_location == "header":
                        headers[key_field] = f"{key_prefix}{api_key}"
                    elif key_location == "auth_basic":
                        if isinstance(api_key, tuple) and len(api_key) == 2:
                            auth = httpx.BasicAuth(api_key[0], api_key[1])
                        else:
                            log_message(f"Cl√© API pour auth_basic non valide pour {service_name}:{endpoint_key}", level="error")
                            success = False
                            continue # Passe √† l'endpoint suivant

                async with httpx.AsyncClient(timeout=check_timeout) as client:
                    response = await client.request(request_method, url, params=params, headers=headers, json=json_data, auth=auth)
                    response.raise_for_status() # L√®ve une exception pour les codes d'√©tat HTTP 4xx/5xx
                    success = True
            except httpx.HTTPStatusError as e:
                log_level = "warning"
                # Les codes 4xx (sauf 429 - Too Many Requests) indiquent souvent une erreur client
                # (cl√© invalide, param√®tre manquant) qui ne se r√©soudra pas avec un r√©essai
                # et n'indique pas forc√©ment un probl√®me de "sant√©" du service lui-m√™me.
                # Nous les loguons en debug pour ne pas surcharger les logs.
                if 400 <= e.response.status_code < 500 and e.response.status_code != 429:
                    log_level = "debug"
                log_message(f"Health check pour {endpoint_key} ({service_name}) a √©chou√© (HTTP {e.response.status_code}): {e.response.text}", level=log_level)
                success = False
            except httpx.RequestError as e:
                # Erreurs r√©seau (connexion, timeout, DNS, etc.)
                log_message(f"Health check pour {endpoint_key} ({service_name}) a √©chou√© (R√©seau): {e}", level="warning")
                success = False
            except Exception as e:
                # Autres erreurs inattendues
                log_message(f"Health check pour {endpoint_key} ({service_name}) a √©chou√© (Inattendu): {e}", level="error")
                success = False
            finally:
                latency = time.monotonic() - start_time # Calcule la latence du check
                self.update_endpoint_health(service_name, endpoint_key, success, latency)
        log_message(f"Health check termin√© pour le service: {service_name}")

    def update_endpoint_health(self, service_name: str, endpoint_key: str, success: bool, latency: float):
        """
        Met √† jour le statut de sant√© d'un endpoint sp√©cifique.
        Utilise une moyenne glissante pour le taux de succ√®s et la latence.
        """
        # S'assure que la structure de donn√©es existe
        if service_name not in self.health_status:
            self.health_status[service_name] = {}
        if endpoint_key not in self.health_status[service_name]:
            self.health_status[service_name][endpoint_key] = {
                "latency": 0.0,
                "success_rate": 1.0,
                "last_checked": None,
                "error_count": 0,
                "total_checks": 0,
                "is_healthy": True
            }

        status = self.health_status[service_name][endpoint_key]
        status["total_checks"] += 1
        status["last_checked"] = format_datetime(get_current_time())

        alpha = 0.1 # Facteur de lissage pour les moyennes glissantes (0.1 signifie 10% de la nouvelle valeur, 90% de l'ancienne)
        if success:
            status["error_count"] = max(0, status["error_count"] - 1) # Diminue le compteur d'erreurs en cas de succ√®s
            status["success_rate"] = status["success_rate"] * (1 - alpha) + 1.0 * alpha # Augmente le taux de succ√®s
            status["latency"] = status["latency"] * (1 - alpha) + latency * alpha # Met √† jour la latence
        else:
            status["error_count"] += 1 # Incr√©mente le compteur d'erreurs
            status["success_rate"] = status["success_rate"] * (1 - alpha) + 0.0 * alpha # Diminue le taux de succ√®s
            # Si √©chec, p√©nalise la latence pour rendre l'endpoint moins attrayant (valeur arbitraire √©lev√©e)
            status["latency"] = status["latency"] * (1 - alpha) + 10.0 * alpha

        # D√©termine si l'endpoint est sain bas√© sur le nombre d'erreurs cons√©cutives ou le taux de succ√®s
        if status["error_count"] >= 3 or status["success_rate"] < 0.5:
            status["is_healthy"] = False
        else:
            status["is_healthy"] = True

        # Sauvegarde l'√©tat mis √† jour de mani√®re asynchrone
        asyncio.create_task(save_json(ENDPOINT_HEALTH_FILE, self.health_status))
        log_message(f"Sant√© de {service_name}:{endpoint_key} mise √† jour: Succ√®s: {success}, Latence: {latency:.2f}s, Taux Succ√®s: {status['success_rate']:.2f}, Sain: {status['is_healthy']}", level="debug" if not status["is_healthy"] else "info")

    def get_best_endpoint(self, service_name: str) -> Optional[Dict]:
        """
        S√©lectionne le meilleur endpoint pour un service donn√© bas√© sur son statut de sant√©.
        Priorise les endpoints sains, puis les moins mauvais en cas d'absence d'endpoints sains.
        """
        service_health = self.health_status.get(service_name)
        if not service_health:
            log_message(f"Aucune donn√©e de sant√© pour le service {service_name}. Retourne None.", level="warning")
            return None

        best_endpoint_key = None
        best_score = -float('inf')

        # Filtre les endpoints actuellement consid√©r√©s comme sains
        healthy_endpoints = [
            (key, status) for key, status in service_health.items() if status["is_healthy"]
        ]

        if not healthy_endpoints:
            log_message(f"Aucun endpoint sain pour le service {service_name}. Tentative de s√©lection d'un endpoint non sain.", level="warning")
            all_endpoints = service_health.items()
            if not all_endpoints:
                return None # Aucun endpoint du tout

            # Si aucun endpoint sain, choisit le "moins mauvais" : moins d'erreurs, meilleure latence
            sorted_endpoints = sorted(all_endpoints, key=lambda item: (item[1]["error_count"], item[1]["latency"]))
            best_endpoint_key = sorted_endpoints[0][0]
            log_message(f"Fallback: Endpoint {best_endpoint_key} s√©lectionn√© pour {service_name} (non sain).", level="warning")
        else:
            # Calcule un score pour chaque endpoint sain pour choisir le meilleur
            for endpoint_key, status in healthy_endpoints:
                # Score = (Taux de succ√®s * 100) - (Latence * 10) - (Compteur d'erreurs * 5)
                # Favorise le succ√®s, p√©nalise la latence et les erreurs
                score = (status["success_rate"] * 100) - (status["latency"] * 10) - (status["error_count"] * 5)
                if score > best_score:
                    best_score = score
                    best_endpoint_key = endpoint_key
            log_message(f"Meilleur endpoint s√©lectionn√© pour {service_name}: {best_endpoint_key} (Score: {best_score:.2f})")

        if best_endpoint_key:
            # Une fois la cl√© du meilleur endpoint trouv√©e, on r√©cup√®re sa configuration compl√®te
            # depuis `API_CONFIG` pour l'utiliser dans la requ√™te.
            for endpoint_config in API_CONFIG.get(service_name, []):
                current_endpoint_key = f"{endpoint_config['endpoint_name']}-{str(endpoint_config['key'])}"
                if current_endpoint_key == best_endpoint_key:
                    return endpoint_config
        return None # Aucun endpoint appropri√© trouv√©

# Instancier le gestionnaire de sant√© des endpoints (sera initialis√© dans main.py)
endpoint_health_manager = EndpointHealthManager()

def set_endpoint_health_manager_global(manager: EndpointHealthManager):
    """
    Permet d'injecter l'instance du gestionnaire de sant√© des endpoints.
    Ceci est utilis√© pour s'assurer que tous les clients API utilisent la m√™me instance.
    """
    global endpoint_health_manager
    endpoint_health_manager = manager

class APIClient:
    """
    Classe de base pour tous les clients API.
    Elle g√®re la s√©lection dynamique d'endpoints, les r√©essais en cas d'√©chec
    et l'int√©gration avec le gestionnaire de sant√© des endpoints.
    """
    def __init__(self, name: str, endpoint_health_manager: EndpointHealthManager):
        self.name = name
        self.endpoints_config = API_CONFIG.get(name, []) # R√©cup√®re la config des endpoints pour cette API
        self.endpoint_health_manager = endpoint_health_manager
        if not self.endpoints_config:
            log_message(f"Client API {self.name} initialis√© sans configuration d'endpoint.", level="error")

    async def _make_request(self, params: Optional[Dict] = None, headers: Optional[Dict] = None,
                            json_data: Optional[Dict] = None, timeout: Optional[int] = None,
                            max_retries: int = 3, initial_delay: float = 1.0,
                            url: Optional[str] = None, method: Optional[str] = None,
                            key_field: Optional[str] = None, key_location: Optional[str] = None,
                            api_key: Optional[Union[str, Tuple[str, str]]] = None,
                            fixed_params: Optional[Dict] = None, fixed_headers: Optional[Dict] = None,
                            fixed_json: Optional[Dict] = None) -> Optional[Union[Dict, str, bytes]]:
        """
        M√©thode interne pour effectuer les requ√™tes HTTP en utilisant le meilleur endpoint avec r√©essais.

        Args:
            params (Dict, optional): Param√®tres de requ√™te √† ajouter √† l'URL.
            headers (Dict, optional): En-t√™tes HTTP suppl√©mentaires.
            json_data (Dict, optional): Donn√©es JSON √† envoyer dans le corps de la requ√™te (pour POST/PUT).
            timeout (int, optional): Timeout pour la requ√™te en secondes.
            max_retries (int): Nombre maximal de tentatives en cas d'√©chec.
            initial_delay (float): D√©lai initial entre les r√©essais (exponentiel).
            url (str, optional): URL sp√©cifique √† utiliser (si non bas√© sur un endpoint configur√©).
            method (str, optional): M√©thode HTTP (GET, POST, etc.).
            key_field (str, optional): Nom du champ pour la cl√© API.
            key_location (str, optional): O√π placer la cl√© API ('param', 'header', 'auth_basic').
            api_key (Union[str, Tuple[str, str]], optional): Cl√© API √† utiliser.
            fixed_params (Dict, optional): Param√®tres fixes d√©finis dans la configuration de l'endpoint.
            fixed_headers (Dict, optional): En-t√™tes fixes d√©finis dans la configuration de l'endpoint.
            fixed_json (Dict, optional): Donn√©es JSON fixes d√©finies dans la configuration de l'endpoint.

        Returns:
            Union[Dict, str, bytes, None]: La r√©ponse de l'API (JSON d√©cod√©, texte brut, ou bytes pour les images),
                                          ou un dictionnaire d'erreur en cas d'√©chec.
        """

        selected_endpoint_config = None
        endpoint_key_for_health = "Dynamic" # Cl√© par d√©faut pour les requ√™tes dynamiques (non configur√©es)

        if url and method: # Si l'URL et la m√©thode sont fournies directement (requ√™te dynamique)
            selected_endpoint_config = {
                "url": url,
                "method": method,
                "key_field": key_field,
                "key_location": key_location,
                "key": api_key,
                "fixed_params": fixed_params if fixed_params is not None else {},
                "fixed_headers": fixed_headers if fixed_headers is not None else {},
                "fixed_json": fixed_json if fixed_json is not None else {},
                "endpoint_name": "Dynamic", # Nom g√©n√©rique pour les endpoints dynamiques
                "timeout": timeout if timeout is not None else 30
            }
            if api_key:
                # Cr√©e une cl√© de sant√© unique pour les requ√™tes dynamiques avec cl√© API
                endpoint_key_for_health = f"Dynamic-{str(api_key)}"
            log_message(f"Requ√™te dynamique pour {self.name} vers {url}")
        else: # S√©lectionne le meilleur endpoint configur√© via le gestionnaire de sant√©
            selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
            if not selected_endpoint_config:
                log_message(f"Aucun endpoint sain ou disponible pour {self.name}.", level="error")
                return {"error": True, "message": f"Aucun endpoint sain ou disponible pour {self.name}."}
            # Construit la cl√© de sant√© pour l'endpoint s√©lectionn√©
            endpoint_key_for_health = f"{selected_endpoint_config['endpoint_name']}-{str(selected_endpoint_config['key'])}"
            log_message(f"Endpoint s√©lectionn√© pour {self.name}: {selected_endpoint_config['endpoint_name']}")
            # Utilise le timeout de la config de l'endpoint si non sp√©cifi√©
            timeout = timeout if timeout is not None else selected_endpoint_config.get("timeout", 30)

        url_to_use = selected_endpoint_config["url"]
        method_to_use = selected_endpoint_config["method"]

        # Initialise les param√®tres, en-t√™tes et donn√©es JSON avec les valeurs fixes de la config
        request_params = selected_endpoint_config.get("fixed_params", {}).copy()
        request_headers = selected_endpoint_config.get("fixed_headers", {}).copy()
        request_json_data = selected_endpoint_config.get("fixed_json", {}).copy()
        auth = None # Pour l'authentification de base

        # Fusionne les param√®tres/en-t√™tes/donn√©es JSON fournis avec les valeurs fixes
        if params:
            request_params.update(params)
        if headers:
            request_headers.update(headers)
        if json_data:
            request_json_data.update(json_data)

        # G√®re l'insertion de la cl√© API pour la requ√™te
        key_field_to_use = selected_endpoint_config.get("key_field")
        key_location_to_use = selected_endpoint_config.get("key_location")
        key_prefix = selected_endpoint_config.get("key_prefix", "")
        api_key_to_use = selected_endpoint_config["key"]

        if key_field_to_use and key_location_to_use:
            if key_location_to_use == "param":
                request_params[key_field_to_use] = api_key_to_use
            elif key_location_to_use == "header":
                request_headers[key_field_to_use] = f"{key_prefix}{api_key_to_use}"
            elif key_location_to_use == "auth_basic":
                if isinstance(api_key_to_use, tuple) and len(api_key_to_use) == 2:
                    auth = httpx.BasicAuth(api_key_to_use[0], api_key_to_use[1])
                else:
                    log_message(f"Cl√© API pour auth_basic non valide pour {self.name}:{endpoint_key_for_health}", level="error")
                    # Marque l'endpoint comme non sain et retourne une erreur
                    self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, 0.0)
                    return {"error": True, "message": "Configuration d'authentification basique invalide."}

        current_delay = initial_delay # D√©lai initial pour les r√©essais
        for attempt in range(max_retries):
            start_time = time.monotonic()
            success = False
            try:
                async with httpx.AsyncClient(timeout=timeout) as client:
                    response = await client.request(method_to_use, url_to_use, params=request_params, headers=request_headers, json=request_json_data, auth=auth)
                    response.raise_for_status() # L√®ve une exception pour les codes d'√©tat HTTP 4xx/5xx
                    success = True

                    content_type = response.headers.get("Content-Type", "").lower()
                    if "application/json" in content_type:
                        try:
                            return response.json() # Tente de d√©coder la r√©ponse JSON
                        except json.JSONDecodeError:
                            log_message(f"API {self.name} r√©ponse non JSON valide (tentative {attempt+1}/{max_retries}): {response.text[:200]}...", level="warning")
                            if attempt < max_retries - 1: # Si ce n'est pas la derni√®re tentative, r√©essaie
                                await asyncio.sleep(current_delay)
                                current_delay *= 2 # Augmente le d√©lai de mani√®re exponentielle
                                continue
                            return {"error": True, "message": "R√©ponse API non JSON valide.", "raw_response": response.text}
                    else:
                        log_message(f"API {self.name} a renvoy√© un Content-Type non JSON: {content_type}", level="info")
                        return response.content # Retourne le contenu brut (ex: pour les images)

            except httpx.HTTPStatusError as e:
                log_message(f"API {self.name} erreur HTTP (tentative {attempt+1}/{max_retries}): {e.response.status_code} - {e.response.text}", level="warning")
                # Ne pas r√©essayer pour les erreurs client (4xx) sauf 429 (Too Many Requests)
                if 400 <= e.response.status_code < 500 and e.response.status_code != 429:
                    log_message(f"API {self.name}: Erreur client {e.response.status_code}, pas de r√©essai.", level="error")
                    self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, e.response.elapsed.total_seconds())
                    return {"error": True, "status_code": e.response.status_code, "message": e.response.text}

                if attempt < max_retries - 1:
                    log_message(f"API {self.name}: R√©essai dans {current_delay:.2f}s...", level="info")
                    await asyncio.sleep(current_delay)
                    current_delay *= 2
            except httpx.RequestError as e:
                log_message(f"API {self.name} erreur de requ√™te (tentative {attempt+1}/{max_retries}): {e}", level="warning")
                if attempt < max_retries - 1:
                    log_message(f"API {self.name}: R√©essai dans {current_delay:.2f}s...", level="info")
                    await asyncio.sleep(current_delay)
                    current_delay *= 2
            except Exception as e:
                log_message(f"API {self.name} erreur inattendue (tentative {attempt+1}/{max_retries}): {e}", level="error")
                self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, time.monotonic() - start_time)
                return {"error": True, "message": str(e)}
            finally:
                # Met √† jour la sant√© de l'endpoint m√™me en cas d'√©chec pour la s√©lection future
                if not success:
                    latency = time.monotonic() - start_time
                    self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, latency)

        log_message(f"API {self.name}: Toutes les tentatives ont √©chou√© apr√®s {max_retries} r√©essais.", level="error")
        return {"error": True, "message": f"√âchec de la requ√™te apr√®s {max_retries} tentatives."}

    async def query(self, *args, **kwargs) -> Any:
        """
        M√©thode abstraite pour interroger l'API.
        Doit √™tre impl√©ment√©e par chaque sous-classe de client API.
        """
        raise NotImplementedError("La m√©thode query doit √™tre impl√©ment√©e par les sous-classes.")

# --- Clients API Sp√©cifiques ---
# Chaque classe de client API h√©rite de `APIClient` et impl√©mente la m√©thode `query`
# pour interagir avec une API sp√©cifique.

class DeepSeekClient(APIClient):
    def __init__(self):
        super().__init__("DEEPSEEK", endpoint_health_manager)

    async def query(self, prompt: str, model: str = "deepseek-chat") -> str:
        """Interroge l'API DeepSeek pour des compl√©tions de chat."""
        payload = {"model": model, "messages": [{"role": "user", "content": prompt}]}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            content = response.get("choices", [{}])[0].get("message", {}).get("content")
            return content if content else "DeepSeek: Pas de contenu de r√©ponse trouv√©."
        return f"DeepSeek: Erreur: {response.get('message', 'Inconnu')}" if response else "DeepSeek: R√©ponse vide ou erreur interne."

class SerperClient(APIClient):
    def __init__(self):
        super().__init__("SERPER", endpoint_health_manager)

    async def query(self, query_text: str) -> str:
        """Effectue une recherche web via l'API Serper."""
        payload = {"q": query_text}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            organic_results = response.get("organic", [])
            if organic_results:
                snippet = organic_results[0].get("snippet", "Pas de snippet.")
                link = organic_results[0].get("link", "")
                return f"Serper (recherche web):\n{snippet} {neutralize_urls(link)}"
            return "Serper: Aucune information trouv√©e."
        return f"Serper: Erreur: {response.get('message', 'Inconnu')}" if response else "Serper: R√©ponse vide ou erreur interne."

class WolframAlphaClient(APIClient):
    def __init__(self):
        super().__init__("WOLFRAMALPHA", endpoint_health_manager)

    async def query(self, input_text: str) -> str:
        """Interroge WolframAlpha pour des calculs ou des faits."""
        params = {"input": input_text}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            pods = response.get("queryresult", {}).get("pods", [])
            if pods:
                for pod in pods:
                    if pod.get("title") in ["Result", "Input interpretation", "Decimal approximation"]:
                        subpods = pod.get("subpods", [])
                        if subpods and subpods[0].get("plaintext"):
                            return f"WolframAlpha:\n{subpods[0]['plaintext']}"
                if pods and pods[0].get("subpods") and pods[0]["subpods"][0].get("plaintext"):
                    return f"WolframAlpha:\n{pods[0]['subpods'][0]['plaintext']}"
            return "WolframAlpha: Pas de r√©sultat clair."
        return f"WolframAlpha: Erreur: {response.get('message', 'Inconnu')}" if response else "WolframAlpha: R√©ponse vide ou erreur interne."

class TavilyClient(APIClient):
    def __init__(self):
        super().__init__("TAVILY", endpoint_health_manager)

    async def query(self, query_text: str, max_results: int = 3) -> str:
        """Effectue une recherche web avanc√©e via l'API Tavily."""
        payload = {"query": query_text, "max_results": max_results}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            results = response.get("results", [])
            answer = response.get("answer", "Aucune r√©ponse directe trouv√©e.")

            output = f"Tavily (recherche web):\nR√©ponse directe: {answer}\n"
            if results:
                output += "Extraits pertinents:\n"
                for i, res in enumerate(results[:max_results]):
                    output += f"- {res.get('title', 'N/A')}: {res.get('content', 'N/A')} {neutralize_urls(res.get('url', ''))}\n"
            return output
        return f"Tavily: Erreur: {response.get('message', 'Inconnu')}" if response else "Tavily: R√©ponse vide ou erreur interne."

class ApiFlashClient(APIClient):
    def __init__(self):
        super().__init__("APIFLASH", endpoint_health_manager)

    async def query(self, url: str) -> str:
        """Capture une capture d'√©cran d'une URL via ApiFlash."""
        params = {"url": url, "format": "jpeg", "full_page": "true"}
        response_content = await self._make_request(params=params)

        if isinstance(response_content, bytes):
            selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
            if selected_endpoint_config:
                capture_url = f"{selected_endpoint_config['url']}?access_key={selected_endpoint_config['key']}&url={url}&format=jpeg&full_page=true"
                return f"ApiFlash (capture d'√©cran): {neutralize_urls(capture_url)} (V√©rifiez le lien pour l'image)"
            return "ApiFlash: Impossible de g√©n√©rer l'URL de capture."
        elif isinstance(response_content, dict) and response_content.get("error"):
            return f"ApiFlash: Erreur: {response_content.get('message', 'Inconnu')}"
        else:
            log_message(f"ApiFlash a renvoy√© un type de r√©ponse inattendu: {type(response_content)}", level="warning")
            return f"ApiFlash: R√©ponse inattendue de l'API. {response_content}"

class CrawlbaseClient(APIClient):
    def __init__(self):
        super().__init__("CRAWLBASE", endpoint_health_manager)

    async def query(self, url: str, use_js: bool = False) -> str:
        """Scrape le contenu HTML ou JavaScript d'une URL via Crawlbase."""
        params = {"url": url, "format": "json"}

        selected_endpoint_config = None
        if use_js:
            for config in API_CONFIG.get(self.name, []):
                if "JS Scraping" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break

        if not selected_endpoint_config:
            selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)

        if not selected_endpoint_config:
            return f"Crawlbase: Aucun endpoint sain ou disponible pour {self.name}."

        response = await self._make_request(
            params=params,
            url=selected_endpoint_config["url"],
            method=selected_endpoint_config["method"],
            key_field=selected_endpoint_config["key_field"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            fixed_params=selected_endpoint_config.get("fixed_params", {}),
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            body = response.get("body")
            if body:
                try:
                    decoded_body = base64.b64decode(body).decode('utf-8', errors='ignore')
                    return f"Crawlbase (contenu web):\n{decoded_body[:1000]}..."
                except Exception:
                    return f"Crawlbase (contenu web - brut):\n{body[:1000]}..."
            return "Crawlbase: Contenu non trouv√©."
        return f"Crawlbase: Erreur: {response.get('message', 'Inconnu')}" if response else "Crawlbase: R√©ponse vide ou erreur interne."

class DetectLanguageClient(APIClient):
    def __init__(self):
        super().__init__("DETECTLANGUAGE", endpoint_health_manager)

    async def query(self, text: str) -> str:
        """D√©tecte la langue d'un texte via DetectLanguage API."""
        payload = {"q": text}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            detections = response.get("data", {}).get("detections", [])
            if detections:
                first_detection = detections[0]
                lang = first_detection.get("language")
                confidence = first_detection.get("confidence")
                return f"Langue d√©tect√©e: {lang} (confiance: {confidence})"
            return "DetectLanguage: Aucune langue d√©tect√©e."
        return f"DetectLanguage: Erreur: {response.get('message', 'Inconnu')}" if response else "DetectLanguage: R√©ponse vide ou erreur interne."

class GuardianClient(APIClient):
    def __init__(self):
        super().__init__("GUARDIAN", endpoint_health_manager)

    async def query(self, query_text: str) -> str:
        """Recherche des articles de presse via l'API The Guardian."""
        params = {"q": query_text}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            results = response.get("response", {}).get("results", [])
            if results:
                output = "Articles The Guardian:\n"
                for res in results[:3]: # Limite √† 3 articles pour la concision
                    output += f"- {res.get('webTitle', 'N/A')}: {res.get('fields', {}).get('trailText', 'N/A')} {neutralize_urls(res.get('webUrl', ''))}\n"
                return output
            return "Guardian: Aucun article trouv√©."
        return f"Guardian: Erreur: {response.get('message', 'Inconnu')}" if response else "Guardian: R√©ponse vide ou erreur interne."

class IP2LocationClient(APIClient):
    def __init__(self):
        super().__init__("IP2LOCATION", endpoint_health_manager)

    async def query(self, ip_address: str) -> str:
        """G√©olocalise une adresse IP via IP2Location API."""
        params = {"ip": ip_address}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            if "country_name" in response:
                return f"IP2Location (G√©olocalisation IP {ip_address}): Pays: {response['country_name']}, Ville: {response.get('city_name', 'N/A')}"
            return "IP2Location: Informations non trouv√©es."
        return f"IP2Location: Erreur: {response.get('message', 'Inconnu')}" if response else "IP2Location: R√©ponse vide ou erreur interne."

class ShodanClient(APIClient):
    def __init__(self):
        super().__init__("SHODAN", endpoint_health_manager)

    async def query(self, query_text: str = "") -> str:
        """
        Interroge Shodan pour des informations sur un h√¥te IP ou des informations sur la cl√© API.
        Si `query_text` est une IP, tente de r√©cup√©rer les infos de l'h√¥te.
        Sinon, ou en cas d'√©chec, retourne les infos de la cl√© API.
        """
        if re.match(r"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$", query_text): # V√©rifie si c'est une adresse IP
            selected_endpoint_config = None
            for config in API_CONFIG.get(self.name, []):
                if "Host Info" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break
            if selected_endpoint_config:
                # Construire l'URL pour la recherche d'h√¥te
                url = f"{selected_endpoint_config['url'].rstrip('/')}/{query_text}"
                response = await self._make_request(
                    params={"key": selected_endpoint_config["key"]},
                    url=url,
                    method="GET",
                    key_field=selected_endpoint_config["key_field"],
                    key_location=selected_endpoint_config["key_location"],
                    api_key=selected_endpoint_config["key"],
                    timeout=selected_endpoint_config.get("timeout")
                )
                if response and not response.get("error"):
                    return f"Shodan (info h√¥te {query_text}): Pays: {response.get('country_name', 'N/A')}, Ports: {response.get('ports', 'N/A')}, Vuln√©rabilit√©s: {response.get('vulns', 'Aucune')}"
                return f"Shodan (info h√¥te): Erreur: {response.get('message', 'Inconnu')}" if response else "Shodan: R√©ponse vide ou erreur interne."
            else:
                return "Shodan: Endpoint 'Host Info' non configur√©."
        else:
            # Si pas d'IP ou si la recherche d'h√¥te n'est pas applicable, retourne les infos de la cl√© API
            response = await self._make_request() # Utilise le premier endpoint disponible (API Info)
            if response and not response.get("error"):
                return f"Shodan (info cl√©): Requ√™tes restantes: {response.get('usage_limits', {}).get('query_credits', 'N/A')}, Scan cr√©dits: {response.get('usage_limits', {}).get('scan_credits', 'N/A')}"
            return f"Shodan: Erreur: {response.get('message', 'Inconnu')}" if response else "Shodan: R√©ponse vide ou erreur interne."

class WeatherAPIClient(APIClient):
    def __init__(self):
        super().__init__("WEATHERAPI", endpoint_health_manager)

    async def query(self, location: str) -> str:
        """R√©cup√®re les conditions m√©t√©orologiques actuelles pour une localisation via WeatherAPI."""
        params = {"q": location}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            current = response.get("current", {})
            location_info = response.get("location", {})
            if current and location_info:
                return (
                    f"M√©t√©o √† {location_info.get('name', 'N/A')}, {location_info.get('country', 'N/A')}:\n"
                    f"Temp√©rature: {current.get('temp_c', 'N/A')}¬∞C, "
                    f"Conditions: {current.get('condition', {}).get('text', 'N/A')}, "
                    f"Vent: {current.get('wind_kph', 'N/A')} km/h"
                )
            return "WeatherAPI: Donn√©es m√©t√©o non trouv√©es."
        return f"WeatherAPI: Erreur: {response.get('message', 'Inconnu')}" if response else "WeatherAPI: R√©ponse vide ou erreur interne."

class CloudmersiveClient(APIClient):
    def __init__(self):
        super().__init__("CLOUDMERSIVE", endpoint_health_manager)

    async def query(self, domain: str) -> str:
        """V√©rifie la validit√© et le type d'un domaine via Cloudmersive API."""
        payload = {"domain": domain}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            return f"Cloudmersive (v√©rification de domaine {domain}): Valide: {response.get('ValidDomain', 'N/A')}, Type: {response.get('DomainType', 'N/A')}"
        return f"Cloudmersive: Erreur: {response.get('message', 'Inconnu')}" if response else "Cloudmersive: R√©ponse vide ou erreur interne."

class GreyNoiseClient(APIClient):
    def __init__(self):
        super().__init__("GREYNOISE", endpoint_health_manager)

    async def query(self, ip_address: str) -> str:
        """Analyse une adresse IP pour d√©tecter des activit√©s 'bruit' (malveillantes) via GreyNoise."""
        selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
        if not selected_endpoint_config:
            return f"GreyNoise: Aucun endpoint sain ou disponible pour {self.name}."

        # Construit l'URL avec l'adresse IP √† la fin
        url = f"{selected_endpoint_config['url'].rstrip('/')}/{ip_address}"
        method = selected_endpoint_config["method"]
        headers = {selected_endpoint_config["key_field"]: selected_endpoint_config["key"]}

        response = await self._make_request(
            headers=headers,
            url=url,
            method=method,
            key_field=selected_endpoint_config["key_field"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if response.get("noise"):
                return f"GreyNoise (IP {ip_address}): C'est une IP 'bruit' (malveillante). Classification: {response.get('classification', 'N/A')}, Nom d'acteur: {response.get('actor', 'N/A')}"
            return f"GreyNoise (IP {ip_address}): Pas de 'bruit' d√©tect√©. Statut: {response.get('status', 'N/A')}"
        return f"GreyNoise: Erreur: {response.get('message', 'Inconnu')}" if response else "GreyNoise: R√©ponse vide ou erreur interne."

class PulsediveClient(APIClient):
    def __init__(self):
        super().__init__("PULSEDIVE", endpoint_health_manager)

    async def query(self, indicator: str, type: str = "auto") -> str:
        """Analyse un indicateur de menace (IP, domaine, URL) via Pulsedive."""
        params = {"indicator": indicator, "type": type}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            if response.get("results"):
                result = response["results"][0] # Prend le premier r√©sultat
                return (
                    f"Pulsedive (Analyse {indicator}): Type: {result.get('type', 'N/A')}, "
                    f"Risk: {result.get('risk', 'N/A')}, "
                    f"Description: {result.get('description', 'N/A')[:200]}..." # Tronque la description
                )
            return "Pulsedive: Aucun r√©sultat d'analyse trouv√©."
        return f"Pulsedive: Erreur: {response.get('message', 'Inconnu')}" if response else "Pulsedive: R√©ponse vide ou erreur interne."

class StormGlassClient(APIClient):
    def __init__(self):
        super().__init__("STORMGLASS", endpoint_health_manager)

    async def query(self, lat: float, lng: float, params: str = "airTemperature,waveHeight") -> str:
        """R√©cup√®re les donn√©es m√©t√©orologiques maritimes pour une coordonn√©e via StormGlass."""
        now = int(time.time())
        request_params = {
            "lat": lat,
            "lng": lng,
            "params": params,
            "start": now,
            "end": now + 3600 # Pr√©visions pour la prochaine heure (3600 secondes)
        }
        response = await self._make_request(params=request_params)
        if response and not response.get("error"):
            data = response.get("hours", [])
            if data:
                first_hour = data[0]
                # Acc√®de aux valeurs sp√©cifiques des param√®tres demand√©s
                temp = first_hour.get('airTemperature', [{}])[0].get('value', 'N/A')
                wave_height = first_hour.get('waveHeight', [{}])[0].get('value', 'N/A')
                return f"StormGlass (M√©t√©o maritime √† {lat},{lng}): Temp√©rature air: {temp}¬∞C, Hauteur vagues: {wave_height}m"
            return "StormGlass: Donn√©es non trouv√©es."
        return f"StormGlass: Erreur: {response.get('message', 'Inconnu')}" if response else "StormGlass: R√©ponse vide ou erreur interne."

class LoginRadiusClient(APIClient):
    def __init__(self):
        super().__init__("LOGINRADIUS", endpoint_health_manager)

    async def query(self) -> str:
        """Effectue un simple ping √† l'API LoginRadius pour v√©rifier sa disponibilit√©."""
        response = await self._make_request()
        if response and not response.get("error"):
            return f"LoginRadius (Ping API): Statut: {response.get('Status', 'N/A')}, Message: {response.get('Message', 'N/A')}"
        return f"LoginRadius: Erreur: {response.get('message', 'Inconnu')}" if response else "LoginRadius: R√©ponse vide ou erreur interne."

class JsonbinClient(APIClient):
    def __init__(self):
        super().__init__("JSONBIN", endpoint_health_manager)

    async def query(self, data: Optional[Dict[str, Any]] = None, private: bool = True, bin_id: Optional[str] = None) -> str:
        """
        Cr√©e un nouveau 'bin' JSON ou acc√®de √† un bin existant via Jsonbin.io.
        `data` est pour la cr√©ation, `bin_id` pour l'acc√®s.
        """
        if bin_id: # Acc√®s √† un bin existant
            selected_endpoint_config = None
            for config in API_CONFIG.get(self.name, []):
                if "Bin Access" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break
            if not selected_endpoint_config:
                return f"Jsonbin: Aucun endpoint d'acc√®s de bin sain ou disponible pour {self.name}."

            url = f"{selected_endpoint_config['url'].rstrip('/')}/{bin_id}"
            method = "GET"
            headers = {selected_endpoint_config["key_field"]: selected_endpoint_config["key"]}

            response = await self._make_request(
                headers=headers,
                url=url,
                method=method,
                timeout=selected_endpoint_config.get("timeout")
            )
            if response and not response.get("error"):
                return f"Jsonbin (Acc√®s bin {bin_id}):\n{json.dumps(response, indent=2)}"
            return f"Jsonbin (Acc√®s bin): Erreur: {response.get('message', 'Inconnu')}" if response else "Jsonbin: R√©ponse vide ou erreur interne."

        else: # Cr√©ation d'un nouveau bin
            selected_endpoint_config = None
            for config in API_CONFIG.get(self.name, []):
                if "Bin Create" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break

            if not selected_endpoint_config:
                return f"Jsonbin: Aucun endpoint de cr√©ation de bin sain ou disponible pour {self.name}."

            url = selected_endpoint_config["url"]
            method = "POST"
            headers = {selected_endpoint_config["key_field"]: selected_endpoint_config["key"], "Content-Type": "application/json"}
            payload = {"record": data if data is not None else {}, "private": private}

            response = await self._make_request(
                json_data=payload,
                headers=headers,
                url=url,
                method=method,
                timeout=selected_endpoint_config.get("timeout")
            )

            if response and not response.get("error"):
                return f"Jsonbin (Cr√©ation de bin): ID: {response.get('metadata', {}).get('id', 'N/A')}, URL: {neutralize_urls(response.get('metadata', {}).get('url', 'N/A'))}"
            return f"Jsonbin (Cr√©ation de bin): Erreur: {response.get('message', 'Inconnu')}" if response else "Jsonbin: R√©ponse vide ou erreur interne."

class HuggingFaceClient(APIClient):
    def __init__(self):
        super().__init__("HUGGINGFACE", endpoint_health_manager)

    async def query(self, model_name: str = "distilbert-base-uncased-finetuned-sst-2-english", input_text: str = "Hello world") -> str:
        """Effectue une inf√©rence sur un mod√®le HuggingFace (ex: classification de texte, g√©n√©ration)."""
        selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
        if not selected_endpoint_config:
            return f"HuggingFace: Aucun endpoint sain ou disponible pour {self.name}."

        inference_url = f"https://api-inference.huggingface.co/models/{model_name}"

        headers = {
            selected_endpoint_config["key_field"]: f"{selected_endpoint_config['key_prefix']}{selected_endpoint_config['key']}",
            "Content-Type": "application/json"
        }
        payload = {"inputs": input_text}

        response = await self._make_request(
            json_data=payload,
            headers=headers,
            url=inference_url,
            method="POST",
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if isinstance(response, list) and response:
                first_result = response[0]
                if isinstance(first_result, list) and first_result:
                    return f"HuggingFace ({model_name} - {first_result[0].get('label')}): Score {first_result[0].get('score', 'N/A'):.2f}"
                elif isinstance(first_result, dict) and "generated_text" in first_result:
                    return f"HuggingFace ({model_name}): {first_result.get('generated_text')}"
            return f"HuggingFace ({model_name}): R√©ponse non pars√©e. {response}"
        return f"HuggingFace: Erreur: {response.get('message', 'Inconnu')}" if response else "HuggingFace: R√©ponse vide ou erreur interne."

class TwilioClient(APIClient):
    def __init__(self):
        super().__init__("TWILIO", endpoint_health_manager)

    async def query(self) -> str:
        """R√©cup√®re le solde du compte Twilio."""
        selected_endpoint_config = None
        for config in API_CONFIG.get(self.name, []):
            if "Account Balance" in config.get("endpoint_name", ""):
                selected_endpoint_config = config
                break
        if not selected_endpoint_config:
            if self.endpoints_config:
                selected_endpoint_config = self.endpoints_config[0]
            else:
                return f"Twilio: Aucune configuration d'endpoint disponible pour {self.name}."

        response = await self._make_request(
            url=selected_endpoint_config["url"],
            method=selected_endpoint_config["method"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            timeout=selected_endpoint_config.get("timeout")
        )
        if response and not response.get("error"):
            return f"Twilio (Balance): {response.get('balance', 'N/A')} {response.get('currency', 'N/A')}"
        return f"Twilio: Erreur: {response.get('message', 'Inconnu')}" if response else "Twilio: R√©ponse vide ou erreur interne."

class AbstractAPIClient(APIClient):
    def __init__(self):
        super().__init__("ABSTRACTAPI", endpoint_health_manager)

    async def query(self, input_value: str, api_type: str) -> str:
        """
        Interroge diverses APIs d'AbstractAPI (validation email/t√©l√©phone, taux de change, jours f√©ri√©s).
        `input_value` d√©pend du `api_type`.
        """
        params = {}
        target_endpoint_name = ""

        if api_type == "PHONE_VALIDATION":
            params["phone"] = input_value
            target_endpoint_name = "Phone Validation"
        elif api_type == "EMAIL_VALIDATION":
            params["email"] = input_value
            target_endpoint_name = "Email Validation"
        elif api_type == "EXCHANGE_RATES":
            params["base"] = input_value if input_value else "USD"
            target_endpoint_name = "Exchange Rates"
        elif api_type == "HOLIDAYS":
            params["country"] = input_value if input_value else "US"
            params["year"] = datetime.now().year
            target_endpoint_name = "Holidays"
        else:
            return f"AbstractAPI: Type d'API '{api_type}' non support√© pour la requ√™te."

        selected_endpoint_config = None
        for config in API_CONFIG.get(self.name, []):
            if target_endpoint_name in config["endpoint_name"]:
                selected_endpoint_config = config
                break

        if not selected_endpoint_config:
            return f"AbstractAPI: Aucun endpoint sain ou disponible pour {self.name} pour le type {api_type}."

        response = await self._make_request(
            params=params,
            url=selected_endpoint_config["url"],
            method=selected_endpoint_config["method"],
            key_field=selected_endpoint_config["key_field"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            fixed_params=selected_endpoint_config.get("fixed_params", {}),
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if api_type == "PHONE_VALIDATION":
                return (
                    f"AbstractAPI (Validation T√©l): Num√©ro: {response.get('phone', 'N/A')}, "
                    f"Valide: {response.get('valid', 'N/A')}, "
                    f"Pays: {response.get('country', {}).get('name', 'N/A')}"
                )
            elif api_type == "EMAIL_VALIDATION":
                return (
                    f"AbstractAPI (Validation Email): Email: {response.get('email', 'N/A')}, "
                    f"Valide: {response.get('is_valid_format', 'N/A')}, "
                    f"Deliverable: {response.get('is_deliverable', 'N/A')}"
                )
            elif api_type == "EXCHANGE_RATES":
                return f"AbstractAPI (Taux de change): Base: {response.get('base', 'N/A')}, Taux (USD): {response.get('exchange_rates', {}).get('USD', 'N/A')}"
            elif api_type == "HOLIDAYS":
                holidays = [h.get('name', 'N/A') for h in response if h.get('name')]
                return f"AbstractAPI (Jours f√©ri√©s {params.get('country', 'US')} {params.get('year')}): {', '.join(holidays[:5])}..." if holidays else "Aucun jour f√©ri√© trouv√©."
            return f"AbstractAPI ({api_type}): R√©ponse brute: {response}"
        return f"AbstractAPI ({api_type}): Erreur: {response.get('message', 'Inconnu')}" if response else "AbstractAPI: R√©ponse vide ou erreur interne."

class GeminiAPIClient:
    def __init__(self):
        self.api_key = GEMINI_API_KEY
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models/"
        self.model_name = "gemini-1.5-flash-latest"
        self.headers = {
            "Content-Type": "application/json",
        }
        from config import GEMINI_TEMPERATURE, GEMINI_TOP_P, GEMINI_TOP_K, GEMINI_MAX_OUTPUT_TOKENS, GEMINI_SAFETY_SETTINGS
        self.generation_config = {
            "temperature": GEMINI_TEMPERATURE,
            "top_p": GEMINI_TOP_P,
            "top_k": GEMINI_TOP_K,
            "max_output_tokens": GEMINI_MAX_OUTPUT_TOKENS,
        }
        self.safety_settings = GEMINI_SAFETY_SETTINGS
        log_message(f"GeminiApiClient initialis√© avec le mod√®le par d√©faut: {self.model_name}")

    async def generate_content(self, prompt: str, chat_history: List[Dict], image_data: Optional[str] = None, model: Optional[str] = None) -> Dict:
        """
        G√©n√®re du contenu textuel ou multimodal en utilisant l'API Gemini.
        `chat_history` est une liste de dictionnaires au format Gemini (role, parts).
        `image_data` est une cha√Æne base64 de l'image avec son pr√©fixe mimeType (ex: "data:image/png;base64,...").
        `model` permet de sp√©cifier un mod√®le diff√©rent si n√©cessaire.
        """
        model_to_use = model if model else self.model_name
        url = f"{self.base_url}{model_to_use}:generateContent?key={self.api_key}"

        contents = []
        for msg in chat_history:
            role = "user" if msg["role"] == "user" else "model"
            contents.append({"role": role, "parts": [{"text": msg["content"]}]})

        user_parts = [{"text": prompt}]
        if image_data:
            if "," in image_data:
                mime_type_part, base64_data = image_data.split(",", 1)
                mime_type = mime_type_part.split(":", 1)[1].split(";", 1)[0]
            else:
                mime_type = "image/jpeg"
                base64_data = image_data

            user_parts.append({
                "inlineData": {
                    "mimeType": mime_type,
                    "data": base64_data
                }
            })
            log_message(f"Image ajout√©e au prompt Gemini (mimeType: {mime_type}).")

        contents.append({"role": "user", "parts": user_parts})

        payload = {
            "contents": contents,
            "generationConfig": self.generation_config,
            "safetySettings": self.safety_settings
        }

        log_message(f"Appel √† Gemini API pour le mod√®le {model_to_use}...")
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(url, headers=self.headers, json=payload)
                response.raise_for_status()
                result = response.json()
                log_message(f"R√©ponse Gemini re√ßue: {json.dumps(result, indent=2)}")
                return result
        except httpx.HTTPStatusError as e:
            log_message(f"Erreur HTTP Gemini API: {e.response.status_code} - {e.response.text}", level="error")
            return {"error": f"Erreur HTTP Gemini: {e.response.status_code} - {e.response.text}"}
        except httpx.RequestError as e:
            log_message(f"Erreur de requ√™te Gemini API: {e}", level="error")
            return {"error": f"Erreur de requ√™te Gemini: {e}"}
        except json.JSONDecodeError as e:
            log_message(f"Erreur de d√©codage JSON Gemini API: {e} - R√©ponse brute: {response.text}", level="error")
            return {"error": f"Erreur de d√©codage JSON Gemini: {e}"}
        except Exception as e:
            log_message(f"Erreur inattendue Gemini API: {e}\n{traceback.format_exc()}", level="error")
            return {"error": f"Erreur inattendue Gemini: {e}"}

class GoogleCustomSearchClient(APIClient):
    def __init__(self):
        super().__init__("GOOGLE_CUSTOM_SEARCH", endpoint_health_manager)

    async def query(self, query_text: str) -> str:
        """Effectue une recherche personnalis√©e Google via l'API Custom Search."""
        params = {"q": query_text}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            items = response.get("items", [])
            if items:
                output = "Google Custom Search:\n"
                for item in items[:3]:
                    output += f"- {item.get('title', 'N/A')}: {item.get('snippet', 'N/A')} {neutralize_urls(item.get('link', ''))}\n"
                return output
            return "Google Custom Search: Aucun r√©sultat trouv√©."
        return f"Google Custom Search: Erreur: {response.get('message', 'Inconnu')}" if response else "Google Custom Search: R√©ponse vide ou erreur interne."

class RandommerClient(APIClient):
    def __init__(self):
        super().__init__("RANDOMMER", endpoint_health_manager)

    async def query(self, country_code: str = "US", quantity: int = 1) -> str:
        """G√©n√®re des num√©ros de t√©l√©phone al√©atoires via Randommer.io."""
        params = {"CountryCode": country_code, "Quantity": quantity}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            if isinstance(response, list) and response:
                return f"Randommer (Num√©ros de t√©l√©phone): {', '.join(response)}"
            return f"Randommer: {response}"
        return f"Randommer: Erreur: {response.get('message', 'Inconnu')}" if response else "Randommer: R√©ponse vide ou erreur interne."

class TomorrowIOClient(APIClient):
    def __init__(self):
        super().__init__("TOMORROW.IO", endpoint_health_manager)

    async def query(self, location: str, fields: Optional[List[str]] = None) -> str:
        """R√©cup√®re les pr√©visions m√©t√©orologiques via Tomorrow.io."""
        if fields is None:
            fields = ["temperature", "humidity", "windSpeed"]
        payload = {"location": location, "fields": fields, "units": "metric", "timesteps": ["1h"]}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            data = response.get("data", {}).get("timelines", [{}])[0].get("intervals", [{}])[0].get("values", {})
            if data:
                output = f"M√©t√©o (Tomorrow.io) √† {location}:\n"
                for field in fields:
                    output += f"- {field.capitalize()}: {data.get(field, 'N/A')}\n"
                return output
            return "Tomorrow.io: Donn√©es m√©t√©o non trouv√©es."
        return f"Tomorrow.io: Erreur: {response.get('message', 'Inconnu')}" if response else "Tomorrow.io: R√©ponse vide ou erreur interne."

class OpenWeatherMapClient(APIClient):
    def __init__(self):
        super().__init__("OPENWEATHERMAP", endpoint_health_manager)

    async def query(self, location: str) -> str:
        """R√©cup√®re les conditions m√©t√©orologiques actuelles via OpenWeatherMap."""
        params = {"q": location}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            main_data = response.get("main", {})
            weather_desc = response.get("weather", [{}])[0].get("description", "N/A")
            if main_data:
                temp_kelvin = main_data.get('temp', 'N/A')
                feels_like_kelvin = main_data.get('feels_like', 'N/A')

                temp_celsius = f"{temp_kelvin - 273.15:.2f}" if isinstance(temp_kelvin, (int, float)) else "N/A"
                feels_like_celsius = f"{feels_like_kelvin - 273.15:.2f}" if isinstance(feels_like_kelvin, (int, float)) else "N/A"

                return (
                    f"M√©t√©o (OpenWeatherMap) √† {location}:\n"
                    f"Temp√©rature: {temp_celsius}¬∞C, "
                    f"Ressenti: {feels_like_celsius}¬∞C, "
                    f"Humidit√©: {main_data.get('humidity', 'N/A')}%, "
                    f"Conditions: {weather_desc}"
                )
            return "OpenWeatherMap: Donn√©es m√©t√©o non trouv√©es."
        return f"OpenWeatherMap: Erreur: {response.get('message', 'Inconnu')}" if response else "OpenWeatherMap: R√©ponse vide ou erreur interne."

class MockarooClient(APIClient):
    def __init__(self):
        super().__init__("MOCKAROO", endpoint_health_manager)

    async def query(self, count: int = 1, fields_json: Optional[str] = None) -> str:
        """G√©n√®re des donn√©es de test via Mockaroo."""
        params = {"count": count}
        if fields_json:
            params["fields"] = fields_json
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            return f"Mockaroo (G√©n√©ration de donn√©es):\n{json.dumps(response, indent=2)}"
        return f"Mockaroo: Erreur: {response.get('message', 'Inconnu')}" if response else "Mockaroo: R√©ponse vide ou erreur interne."

class OpenPageRankClient(APIClient):
    def __init__(self):
        super().__init__("OPENPAGERANK", endpoint_health_manager)

    async def query(self, domains: List[str]) -> str:
        """R√©cup√®re le PageRank de domaines via OpenPageRank."""
        params = {"domains[]": domains}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            results = response.get("response", [])
            if results:
                output = "OpenPageRank (Classement de domaine):\n"
                for res in results:
                    output += f"- Domaine: {res.get('domain', 'N/A')}, PageRank: {res.get('page_rank', 'N/A')}\n"
                return output
            return "OpenPageRank: Aucun r√©sultat trouv√©."
        return f"OpenPageRank: Erreur: {response.get('message', 'Inconnu')}" if response else "OpenPageRank: R√©ponse vide ou erreur interne."

class RapidAPIClient(APIClient):
    def __init__(self):
        super().__init__("RAPIDAPI", endpoint_health_manager)

    async def query(self, api_name: str, **kwargs) -> str:
        """
        Interroge diverses APIs disponibles via RapidAPI (blagues, taux de change, faits al√©atoires).
        `api_name` sp√©cifie l'API RapidAPI √† utiliser.
        """
        selected_endpoint_config = None
        for config in API_CONFIG.get(self.name, []):
            if api_name.lower() in config["endpoint_name"].lower():
                selected_endpoint_config = config
                break

        if not selected_endpoint_config:
            return f"RapidAPI: Endpoint pour '{api_name}' non trouv√© ou non configur√©."

        url = selected_endpoint_config["url"]
        method = selected_endpoint_config["method"]

        request_params = selected_endpoint_config.get("fixed_params", {}).copy()
        request_headers = selected_endpoint_config.get("fixed_headers", {}).copy()
        request_json_data = selected_endpoint_config.get("fixed_json", {}).copy()

        if method == "GET":
            request_params.update(kwargs)
        elif method == "POST":
            request_json_data.update(kwargs)

        headers = {
            selected_endpoint_config["key_field"]: selected_endpoint_config["key"],
            "X-RapidAPI-Host": selected_endpoint_config["fixed_headers"].get("X-RapidAPI-Host")
        }

        response = await self._make_request(
            params=request_params,
            headers=headers,
            json_data=request_json_data,
            url=url,
            method=method,
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if api_name.lower() == "programming joke":
                return f"RapidAPI (Blague de Programmation): {response.get('setup', '')} - {response.get('punchline', '')}"
            elif api_name.lower() == "currency list quotes":
                return f"RapidAPI (Devises): {json.dumps(response, indent=2)}"
            elif api_name.lower() == "random fact":
                return f"RapidAPI (Fait Al√©atoire): {response.get('text', 'N/A')}"
            return f"RapidAPI ({api_name}): {json.dumps(response, indent=2)}"
        return f"RapidAPI ({api_name}): Erreur: {response.get('message', 'Inconnu')}" if response else "RapidAPI: R√©ponse vide ou erreur interne."

class OCRApiClient:
    def __init__(self):
        from config import OCR_API_KEY
        self.api_key = OCR_API_KEY
        self.base_url = "https://api.ocr.space/parse/image"
        log_message("OCRApiClient initialis√©.")

    async def query(self, image_base64: str) -> str:
        """
        Effectue une requ√™te OCR √† l'API OCR.space.
        `image_base64` doit √™tre la cha√Æne base64 de l'image, incluant le pr√©fixe mimeType
        (ex: "data:image/png;base64,...").
        """
        payload = {
            "base64Image": image_base64,
            "language": "fre",
            "isOverlayRequired": False,
            "OCREngine": 2
        }
        headers = {
            "apikey": self.api_key,
            "Content-Type": "application/json"
        }

        log_message("Appel √† OCR.space API...")
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(self.base_url, headers=headers, json=payload)
                response.raise_for_status()
                result = response.json()

                if result.get("IsErroredOnProcessing"):
                    error_message = result.get("ErrorMessage", ["Erreur inconnue lors du traitement OCR."])
                    log_message(f"Erreur OCR.space: {error_message}", level="error")
                    return f"‚ùå Erreur OCR: {', '.join(error_message)}"

                parsed_text = ""
                if "ParsedResults" in result and result["ParsedResults"]:
                    for parsed_result in result["ParsedResults"]:
                        parsed_text += parsed_result.get("ParsedText", "") + "\n"

                if parsed_text.strip():
                    log_message("OCR.space: Texte extrait avec succ√®s.")
                    return parsed_text.strip()
                else:
                    log_message("OCR.space: Aucun texte extrait.", level="warning")
                    return "Aucun texte n'a pu √™tre extrait de l'image."

        except httpx.HTTPStatusError as e:
            log_message(f"Erreur HTTP OCR.space API: {e.response.status_code} - {e.response.text}", level="error")
            return f"‚ùå Erreur HTTP OCR: {e.response.status_code} - {e.response.text}"
        except httpx.RequestError as e:
            log_message(f"Erreur de requ√™te OCR.space API: {e}", level="error")
            return f"‚ùå Erreur de requ√™te OCR: {e}"
        except json.JSONDecodeError as e:
            log_message(f"Erreur de d√©codage JSON OCR.space API: {e} - R√©ponse brute: {response.text}", level="error")
            return {"error": f"Erreur de d√©codage JSON OCR: {e}"}
        except Exception as e:
            log_message(f"Erreur inattendue OCR.space API: {e}\n{traceback.format_exc()}", level="error")
            return f"‚ùå Erreur inattendue OCR: {e}"


# Instancier tous les clients API en leur passant le gestionnaire de sant√©
# Note: GeminiApiClient et OCRApiClient sont instanci√©s s√©par√©ment car ils g√®rent leurs cl√©s directement.
ALL_API_CLIENTS = [
    DeepSeekClient(), SerperClient(), WolframAlphaClient(), TavilyClient(),
    ApiFlashClient(), CrawlbaseClient(), DetectLanguageClient(), GuardianClient(),
    IP2LocationClient(), ShodanClient(), WeatherAPIClient(),
    CloudmersiveClient(), GreyNoiseClient(), PulsediveClient(), StormGlassClient(),
    LoginRadiusClient(), JsonbinClient(),
    HuggingFaceClient(), TwilioClient(), AbstractAPIClient(),
    GoogleCustomSearchClient(), RandommerClient(), TomorrowIOClient(),
    OpenWeatherMapClient(),
    MockarooClient(), OpenPageRankClient(), RapidAPIClient()
]

# memory_and_quotas.py
import json
import asyncio
import random
import traceback
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, Any, Optional, List, Union

# Imports des constantes depuis config.py
from config import (
    API_QUOTAS, API_COOLDOWN_DURATION_SECONDS,
    USER_CHAT_HISTORY_FILE, USER_LONG_MEMORY_FILE,
    IA_STATUS_FILE, QUOTAS_FILE, GROUP_CHAT_HISTORY_FILE, PRIVATE_GROUP_ID,
    BURN_QUOTA_THRESHOLD_RATIO, BURN_QUOTA_BEFORE_RESET_HOURS,
    API_ROTATION_INTERVAL_MINUTES # Ajout√© pour la r√©cup√©ration de diversification
)

# Imports depuis utils.py
from utils import (
    load_json, save_json, get_current_time, format_datetime, log_message,
    neutralize_urls, extract_keywords, tag_conversation, unique_preserve_order,
    similar, get_user_dir
)

class MemoryManager:
    """
    G√®re la m√©moire √† court et long terme du bot, ainsi que l'historique des conversations
    pour les utilisateurs individuels et les groupes.
    C'est un singleton pour s'assurer qu'il n'y a qu'une seule instance de gestionnaire de m√©moire.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Impl√©mente le patron de conception Singleton."""
        if cls._instance is None:
            cls._instance = super(cls, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """Initialise les structures de donn√©es pour la m√©moire."""
        if self._initialized:
            return
        self.chat_history: Dict[Union[int, str], List[Dict]] = {} # {user_id: [messages]}
        self.long_term_memory: Dict[Union[int, str], List[str]] = {} # {user_id: [facts]}
        self.ia_status: Dict[str, Dict[str, Any]] = {} # Statut global des IA
        self.group_chat_history: Dict[int, List[Dict]] = {} # {group_id: [messages]}
        # `_initialized` est g√©r√© par `init_manager` pour les op√©rations asynchrones

    async def init_manager(self):
        """
        Initialise le gestionnaire de m√©moire de mani√®re asynchrone.
        Charge les statuts persistants et les historiques de groupe.
        """
        if not self._initialized:
            # Charge le statut global des IA
            self.ia_status = await load_json(IA_STATUS_FILE, {})
            self._initialize_ia_status()

            # Charge l'historique du groupe priv√© sp√©cifique
            group_dir = get_user_dir(PRIVATE_GROUP_ID)
            self.group_chat_history[PRIVATE_GROUP_ID] = await load_json(group_dir / GROUP_CHAT_HISTORY_FILE, [])

            self._initialized = True
            log_message("Gestionnaire de m√©moire initialis√©.")

    def _initialize_ia_status(self):
        """
        Initialise ou met √† jour le statut des IA si elles ne sont pas d√©j√† pr√©sentes
        ou si leur statut est obsol√®te. S'assure que toutes les cl√©s n√©cessaires sont l√†.
        """
        updated = False
        now = get_current_time()

        default_ia_status_keys = {
            "last_used": None, "last_error": None, "error_count": 0,
            "cooldown_until": None, "success_count": 0, "current_score": 1.0,
            "last_rotation_check": format_datetime(now), "diversification_score": 1.0
        }

        for client_name in API_QUOTAS.keys(): # It√®re sur toutes les APIs d√©finies dans les quotas
            if client_name not in self.ia_status:
                self.ia_status[client_name] = default_ia_status_keys.copy()
                updated = True
            else:
                for key, default_value in default_ia_status_keys.items():
                    if key not in self.ia_status[client_name]:
                        self.ia_status[client_name][key] = default_value
                        updated = True

                # Met √† jour `last_rotation_check` si trop ancien pour permettre la r√©cup√©ration du score de diversification
                last_check_str = self.ia_status[client_name].get("last_rotation_check")
                if last_check_str:
                    try:
                        last_check_dt = datetime.strptime(last_check_str, "%Y-%m-%d %H:%M:%S UTC")
                        if (now - last_check_dt).total_seconds() > API_ROTATION_INTERVAL_MINUTES * 60 * 2:
                            self.ia_status[client_name]["last_rotation_check"] = format_datetime(now)
                            updated = True
                    except ValueError:
                        self.ia_status[client_name]["last_rotation_check"] = format_datetime(now)
                        updated = True
                        log_message(f"last_rotation_check malform√© pour {client_name}, r√©initialisation.", level="warning")

        # Supprime les noms d'IA qui ne sont plus d√©finis dans `API_QUOTAS`
        current_api_names = set(API_QUOTAS.keys())
        ia_names_to_remove = [name for name in self.ia_status if name not in current_api_names]
        for name in ia_names_to_remove:
            del self.ia_status[name]
            updated = True
            log_message(f"IA '{name}' trouv√©e dans ia_status.json mais non d√©finie dans API_QUOTAS. Supprim√©e.", level="warning")

        if updated:
            asyncio.create_task(save_json(IA_STATUS_FILE, self.ia_status))
            log_message("Statut des IA initialis√©/mis √† jour.")

    async def _update_and_save_history(self, user_id: Union[int, str], file_path: Path, history_dict: Dict, max_entries: int, entry: Dict):
        """Helper to update history and save."""
        hist = history_dict.get(user_id, await load_json(file_path, []))
        hist.append(entry)
        hist = hist[-max_entries:]
        history_dict[user_id] = hist
        asyncio.create_task(save_json(file_path, hist))

    async def add_message_to_history(self, user_id: Union[int, str], role: str, content: str, max_log_entries: int = 100):
        """
        Ajoute un message √† l'historique de la conversation d'un utilisateur.
        G√®re √©galement le log g√©n√©ral de l'utilisateur et le taggage des messages.
        """
        user_dir = get_user_dir(user_id)
        chat_history_path = user_dir / USER_CHAT_HISTORY_FILE
        log_path = user_dir / "log.json"

        neutralized_content = neutralize_urls(content)
        timestamp = format_datetime(get_current_time())

        # Add to user chat history
        await self._update_and_save_history(
            user_id, chat_history_path, self.chat_history, max_log_entries,
            {"role": role, "content": neutralized_content, "timestamp": timestamp}
        )

        # Add to general user log
        log_entry_content = neutralized_content[:500]
        log_entry = {"time": timestamp, "role": role, "text": log_entry_content}
        if role == "user":
            log_entry["tags"] = tag_conversation(content)
        
        user_log = await load_json(log_path, [])
        await self._update_and_save_history(
            user_id, log_path, {user_id: user_log}, max_log_entries, log_entry
        ) # Pass user_log as a dict for consistency with _update_and_save_history

        log_message(f"Message ajout√© √† l'historique de {user_id} par {role}.")

    async def get_chat_history(self, user_id: Union[int, str], limit: int = 10) -> List[Dict]:
        """
        Retourne les N derniers messages de l'historique de conversation d'un utilisateur.
        Charge l'historique depuis le fichier si non d√©j√† en m√©moire.
        """
        user_dir = get_user_dir(user_id)
        chat_history_path = user_dir / USER_CHAT_HISTORY_FILE
        if user_id not in self.chat_history:
            self.chat_history[user_id] = await load_json(chat_history_path, [])
        return self.chat_history[user_id][-limit:]

    async def save_group_memory(self, group_id: int, role: str, text: str, max_items: int = 1000):
        """
        Sauvegarde l'historique de chat pour un groupe sp√©cifique.
        Utilise l'ID du groupe comme un ID utilisateur pour la structure de r√©pertoire.
        """
        group_dir = get_user_dir(group_id)
        group_history_path = group_dir / GROUP_CHAT_HISTORY_FILE

        hist = self.group_chat_history.get(group_id, await load_json(group_history_path, []))
        hist.append({"time": format_datetime(get_current_time()), "role": role, "text": neutralize_urls(text)})
        hist = hist[-max_items:]
        self.group_chat_history[group_id] = hist
        asyncio.create_task(save_json(group_history_path, hist))
        log_message(f"Message ajout√© √† la m√©moire de groupe {group_id} par {role}.")

    async def get_group_memory(self, group_id: int, limit: int = 20) -> str:
        """
        R√©cup√®re les N derniers messages de la m√©moire de groupe.
        Retourne une cha√Æne format√©e pour √™tre utilis√©e comme contexte.
        """
        group_dir = get_user_dir(group_id)
        group_history_path = group_dir / GROUP_CHAT_HISTORY_FILE
        if group_id not in self.group_chat_history:
            self.group_chat_history[group_id] = await load_json(group_history_path, [])

        if not isinstance(self.group_chat_history[group_id], list):
            self.group_chat_history[group_id] = []

        recent_messages = [f"{l['role']} : {l['text']}" for l in self.group_chat_history[group_id][-limit:] if l.get("role") != "bot"]
        return "\n".join(recent_messages)

    async def add_to_long_term_memory(self, user_id: Union[int, str], text: str, max_entries: int = 100):
        """
        Ajoute une information √† la m√©moire √† long terme d'un utilisateur.
        D√©doublonne et tronque la m√©moire.
        """
        user_dir = get_user_dir(user_id)
        long_memory_path = user_dir / USER_LONG_MEMORY_FILE

        long_mem = self.long_term_memory.get(user_id, await load_json(long_memory_path, []))
        if not isinstance(long_mem, list):
            long_mem = []

        long_mem.append(text.strip())
        long_mem = unique_preserve_order(long_mem)[-max_entries:]
        self.long_term_memory[user_id] = long_mem
        asyncio.create_task(save_json(long_memory_path, long_mem))
        log_message(f"Information ajout√©e √† la m√©moire √† long terme de {user_id}.")

    async def get_long_term_memory(self, user_id: Union[int, str], limit: int = 20) -> str:
        """
        R√©cup√®re les N derni√®res entr√©es de la m√©moire √† long terme d'un utilisateur.
        Retourne une cha√Æne format√©e.
        """
        user_dir = get_user_dir(user_id)
        long_memory_path = user_dir / USER_LONG_MEMORY_FILE
        if user_id not in self.long_term_memory:
            self.long_term_memory[user_id] = await load_json(long_memory_path, [])

        if not isinstance(self.long_term_memory[user_id], list):
            self.long_term_memory[user_id] = []

        return "\n".join(self.long_term_memory[user_id][-limit:])

    async def check_for_similar_prompt(self, user_id: Union[int, str], prompt: str) -> Optional[str]:
        """
        V√©rifie si un prompt similaire a d√©j√† √©t√© pos√© r√©cemment et retourne la r√©ponse si trouv√©e.
        Utilise `MAX_CACHE_SIZE` pour la fen√™tre de recherche.
        """
        recent_chat_history = await self.get_chat_history(user_id, limit=MAX_CACHE_SIZE)
        for i in range(len(recent_chat_history) - 1, 0, -1): # Iterate backwards from second to last
            entry = recent_chat_history[i]
            if entry.get("role") == "user" and "content" in entry:
                if similar(prompt, entry["content"]) > 0.92:
                    # Check the next entry for a bot response
                    if i + 1 < len(recent_chat_history) and recent_chat_history[i+1].get("role") == "bot":
                        log_message(f"Prompt similaire d√©tect√© pour {user_id}. R√©ponse en cache utilis√©e.")
                        return recent_chat_history[i+1]["content"]
        return None

    def update_ia_status(self, ia_name: str, success: bool, error_message: Optional[str] = None):
        """
        Met √† jour le statut et le score d'une IA apr√®s une utilisation.
        Ajuste le score de performance et le score de diversification.
        """
        status = self.ia_status.get(ia_name)
        if not status:
            log_message(f"Tentative de mise √† jour d'un statut d'IA inconnu: {ia_name}", level="warning")
            return

        now = get_current_time()
        status["last_used"] = format_datetime(now)

        if success:
            status["success_count"] += 1
            status["error_count"] = 0
            status["cooldown_until"] = None
            status["last_error"] = None
            status["current_score"] = min(1.0, status["current_score"] + 0.1)
            status["diversification_score"] = max(0.1, status["diversification_score"] - 0.1)
            log_message(f"IA {ia_name} : Succ√®s enregistr√©. Nouveau score: {status['current_score']:.2f}, Diversification: {status['diversification_score']:.2f}")
        else:
            status["error_count"] += 1
            status["last_error"] = error_message
            if status["error_count"] >= 3:
                status["cooldown_until"] = format_datetime(now + timedelta(seconds=API_COOLDOWN_DURATION_SECONDS))
                status["current_score"] = max(0.1, status["current_score"] - 0.2)
                log_message(f"IA {ia_name} : Trop d'erreurs ({status['error_count']}). Cooldown jusqu'√† {status['cooldown_until']}. Nouveau score: {status['current_score']:.2f}", level="warning")
            else:
                 status["current_score"] = max(0.1, status["current_score"] - 0.05)
                 log_message(f"IA {ia_name} : Erreur enregistr√©e. Nouveau score: {status['current_score']:.2f}", level="warning")

        asyncio.create_task(save_json(IA_STATUS_FILE, self.ia_status))

    def recover_diversification_scores(self):
        """
        Augmente le score de diversification pour les IA qui n'ont pas √©t√© utilis√©es r√©cemment.
        Ceci encourage la rotation des APIs.
        """
        now = get_current_time()
        updated = False
        for ia_name, status in self.ia_status.items():
            last_used_str = status.get("last_used")
            if last_used_str:
                try:
                    last_used_dt = datetime.strptime(last_used_str, "%Y-%m-%d %H:%M:%S UTC")
                    if (now - last_used_dt).total_seconds() > API_ROTATION_INTERVAL_MINUTES * 60 * 2:
                        if status["diversification_score"] < 1.0:
                            status["diversification_score"] = min(1.0, status["diversification_score"] + 0.05)
                            updated = True
                            log_message(f"IA {ia_name}: Score de diversification r√©cup√©r√© √† {status['diversification_score']:.2f}")
                except ValueError:
                    status["last_used"] = format_datetime(now)
                    status["diversification_score"] = 1.0
                    updated = True
                    log_message(f"last_used malform√© pour {ia_name}, r√©initialisation du score de diversification.", level="warning")
            else:
                if status["diversification_score"] < 1.0:
                    status["diversification_score"] = 1.0
                    updated = True
        if updated:
            asyncio.create_task(save_json(IA_STATUS_FILE, self.ia_status))

    def get_ia_status(self, ia_name: str) -> Optional[Dict]:
        """R√©cup√®re le statut d'une IA sp√©cifique."""
        return self.ia_status.get(ia_name)

    def get_available_ias(self) -> List[str]:
        """
        Retourne les noms des IA actuellement non en cooldown.
        """
        available = []
        now = get_current_time()
        for name, status in self.ia_status.items():
            cooldown_until_str = status.get("cooldown_until")
            if cooldown_until_str:
                try:
                    cooldown_until = datetime.strptime(cooldown_until_str, "%Y-%m-%d %H:%M:%S UTC")
                    if now < cooldown_until:
                        continue
                except ValueError:
                    log_message(f"cooldown_until malform√© pour {name}, consid√©r√© comme non en cooldown.", level="warning")
            available.append(name)
        return available

class QuotaManager:
    """
    G√®re les quotas d'utilisation pour toutes les APIs.
    Suit l'utilisation mensuelle, journali√®re et horaire, et peut alerter en cas de d√©passement.
    Prend √©galement en charge le mode "br√ªlage" de quota.
    C'est un singleton.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Impl√©mente le patron de conception Singleton."""
        if cls._instance is None:
            cls._instance = super(cls, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """Initialise les structures de donn√©es pour les quotas."""
        if self._initialized:
            return
        self.quotas = {}
        # `_initialized` est g√©r√© par `init_manager` pour les op√©rations asynchrones
        self.bot_instance = None # Sera inject√© par main.py pour l'envoi d'alertes

    async def init_manager(self):
        """
        Initialise le gestionnaire de quotas de mani√®re asynchrone.
        Charge les donn√©es de quotas persistantes et s'assure qu'elles sont √† jour.
        """
        if not self._initialized:
            self.quotas = await load_json(QUOTAS_FILE, {})
            self._initialize_quotas()
            self._initialized = True
            log_message("Gestionnaire de quotas initialis√©.")

    def set_bot_instance(self, bot_instance: Any):
        """
        Permet d'injecter l'instance du bot pour envoyer des alertes de quota au groupe priv√©.
        """
        self.bot_instance = bot_instance

    def _initialize_quotas(self):
        """
        Initialise les quotas pour toutes les APIs bas√©es sur `config.API_QUOTAS`.
        Nettoie et met √† jour les entr√©es existantes si n√©cessaire.
        """
        updated = False
        now = get_current_time()

        default_quota_structure = {
            "monthly_usage": 0, "daily_usage": 0, "hourly_usage": 0,
            "hourly_timestamps": [], "last_reset_month": now.month,
            "last_reset_day": now.day, "last_usage": None,
            "total_calls": 0, "last_hourly_reset": format_datetime(now)
        }

        for api_name, quota_info in API_QUOTAS.items():
            if api_name not in self.quotas:
                self.quotas[api_name] = default_quota_structure.copy()
                updated = True
            else:
                for key, default_value in default_quota_structure.items():
                    if key not in self.quotas[api_name]:
                        self.quotas[api_name][key] = default_value
                        updated = True

                if not isinstance(self.quotas[api_name].get("hourly_timestamps"), list):
                    self.quotas[api_name]["hourly_timestamps"] = []

                one_hour_ago = now - timedelta(hours=1)
                self.quotas[api_name]["hourly_timestamps"] = [
                    ts for ts in self.quotas[api_name]["hourly_timestamps"]
                    if datetime.strptime(ts, "%Y-%m-%d %H:%M:%S UTC").replace(tzinfo=timezone.utc) > one_hour_ago
                ]
                self.quotas[api_name]["hourly_usage"] = len(self.quotas[api_name]["hourly_timestamps"])
                self.quotas[api_name]["last_hourly_reset"] = format_datetime(now)
                updated = True

        api_names_to_remove = [name for name in self.quotas if name not in API_QUOTAS]
        for name in api_names_to_remove:
            del self.quotas[name]
            updated = True
            log_message(f"API '{name}' trouv√©e dans quotas.json mais non d√©finie dans API_QUOTAS. Supprim√©e.", level="warning")

        if updated:
            asyncio.create_task(save_json(QUOTAS_FILE, self.quotas))
            log_message("Quotas API initialis√©s/mis √† jour.")

    def _reset_quotas_if_needed(self):
        """
        R√©initialise les quotas journaliers, mensuels et horaires si n√©cessaire.
        Cette m√©thode est appel√©e avant chaque v√©rification de quota pour s'assurer de l'actualit√©.
        """
        now = get_current_time()
        for api_name, data in self.quotas.items():
            if now.month != data["last_reset_month"]:
                data["monthly_usage"] = 0
                data["last_reset_month"] = now.month
                log_message(f"Quota mensuel pour {api_name} r√©initialis√©.")
            if now.day != data["last_reset_day"]:
                data["daily_usage"] = 0
                data["last_reset_day"] = now.day
                log_message(f"Quota journalier pour {api_name} r√©initialis√©.")

            one_hour_ago = now - timedelta(hours=1)
            if not isinstance(data.get("hourly_timestamps"), list):
                data["hourly_timestamps"] = []

            data["hourly_timestamps"] = [
                ts for ts in data["hourly_timestamps"]
                if datetime.strptime(ts, "%Y-%m-%d %H:%M:%S UTC").replace(tzinfo=timezone.utc) > one_hour_ago
            ]
            data["hourly_usage"] = len(data["hourly_timestamps"])
            data["last_hourly_reset"] = format_datetime(now)

        asyncio.create_task(save_json(QUOTAS_FILE, self.quotas))

    async def check_and_update_quota(self, api_name: str, cost: int = 1) -> bool:
        """
        V√©rifie si une API a du quota disponible et le d√©cr√©mente si oui.
        Retourne `True` si l'op√©ration est autoris√©e (quota disponible), `False` sinon.
        """
        self._reset_quotas_if_needed()

        if api_name not in API_QUOTAS:
            log_message(f"Tentative de v√©rification de quota pour une API non d√©finie: {api_name}. Autorisation refus√©e.", level="error")
            return False

        if api_name not in self.quotas:
            log_message(f"API {api_name} non trouv√©e dans les quotas g√©r√©s. Re-initialisation non bloquante.", level="warning")
            self._initialize_quotas()
            if api_name not in self.quotas:
                return False

        quota_data = self.quotas[api_name]
        api_limits = API_QUOTAS.get(api_name, {})
        now = get_current_time()

        limits = {
            "monthly": api_limits.get("monthly"),
            "daily": api_limits.get("daily"),
            "hourly": api_limits.get("hourly")
        }
        usages = {
            "monthly": quota_data["monthly_usage"],
            "daily": quota_data["daily_usage"],
            "hourly": quota_data["hourly_usage"]
        }

        for limit_type, limit_value in limits.items():
            if limit_value is not None and (usages[limit_type] + cost) > limit_value:
                log_message(f"Quota {limit_type} d√©pass√© pour {api_name}", level="warning")
                await self._alert_quota_if_needed(api_name, limit_type)
                return False

        rate_limit_per_sec = api_limits.get("rate_limit_per_sec")
        if rate_limit_per_sec:
            last_usage_str = quota_data.get("last_usage")
            if last_usage_str:
                try:
                    last_usage = datetime.strptime(last_usage_str, "%Y-%m-%d %H:%M:%S UTC").replace(tzinfo=timezone.utc)
                    time_since_last_call = (now - last_usage).total_seconds()
                    if time_since_last_call < (1 / rate_limit_per_sec):
                        log_message(f"Taux de requ√™tes d√©pass√© pour {api_name}. Attendre {1/rate_limit_per_sec - time_since_last_call:.2f}s", level="warning")
                        return False
                except ValueError:
                    log_message(f"last_usage malform√© pour {api_name}, consid√©r√© comme sans utilisation r√©cente pour la limite de taux.", level="warning")

        if cost > 0:
            quota_data["monthly_usage"] += cost
            quota_data["daily_usage"] += cost
            quota_data["hourly_usage"] += cost
            quota_data["hourly_timestamps"].append(format_datetime(now))
            quota_data["total_calls"] += cost
            quota_data["last_usage"] = format_datetime(now)
            asyncio.create_task(save_json(QUOTAS_FILE, self.quotas))
            log_message(f"Quota pour {api_name} mis √† jour. Usage mensuel: {quota_data['monthly_usage']}/{limits['monthly'] if limits['monthly'] else 'Illimit√©'}, Journalier: {quota_data['daily_usage']}/{limits['daily'] if limits['daily'] else 'Illimit√©'}, Horaire: {quota_data['hourly_usage']}/{limits['hourly'] if limits['hourly'] else 'Illimit√©'}")
        else:
            log_message(f"Quota pour {api_name} v√©rifi√© (co√ªt 0). Usage mensuel: {quota_data['monthly_usage']}/{limits['monthly'] if limits['monthly'] else 'Illimit√©'}, Journalier: {quota_data['daily_usage']}/{limits['daily'] if limits['daily'] else 'Illimit√©'}, Horaire: {quota_data['hourly_usage']}/{limits['hourly'] if limits['hourly'] else 'Illimit√©'}")

        return True

    async def _alert_quota_if_needed(self, api_name: str, limit_type: str):
        """
        Envoie une alerte au groupe priv√© si un quota est atteint.
        Utilise la m√©moire √† long terme pour √©viter de spammer les alertes.
        """
        if self.bot_instance and PRIVATE_GROUP_ID:
            message = f"üö® Quota {limit_type} pour l'API '{api_name}' atteint !"
            log_message(message, level="warning")
            try:
                alert_key = f"quota_alert_{api_name}_{limit_type}_{get_current_time().strftime('%Y-%m-%d')}"
                bot_global_memory_id = "bot_global_alerts"
                
                global_alerts = await memory_manager.get_long_term_memory(bot_global_memory_id, limit=1000)
                if alert_key not in global_alerts:
                    await self.bot_instance.send_message(chat_id=PRIVATE_GROUP_ID, text=message)
                    await memory_manager.add_to_long_term_memory(bot_global_memory_id, alert_key)
            except Exception as e:
                log_message(f"Erreur lors de l'envoi de l'alerte de quota: {e}", level="error")

    def get_api_usage(self, api_name: str) -> Optional[Dict]:
        """Retourne les informations d'utilisation d'une API sp√©cifique."""
        return self.quotas.get(api_name)

    def get_all_quotas_status(self) -> Dict:
        """
        Retourne le statut de tous les quotas API.
        S'assure que les quotas sont √† jour avant de les retourner.
        """
        self._reset_quotas_if_needed()
        status = {}
        for api_name, quota_data in self.quotas.items():
            api_limits = API_QUOTAS.get(api_name, {})
            monthly_limit = api_limits.get("monthly", "Illimit√©")
            daily_limit = api_limits.get("daily", "Illimit√©")
            hourly_limit = api_limits.get("hourly", "Illimit√©")
            status[api_name] = {
                "monthly_usage": quota_data["monthly_usage"],
                "monthly_limit": monthly_limit,
                "daily_usage": quota_data["daily_usage"],
                "daily_limit": daily_limit,
                "hourly_usage": quota_data["hourly_usage"],
                "hourly_limit": hourly_limit,
                "total_calls": quota_data["total_calls"],
                "last_usage": quota_data["last_usage"],
                "last_reset_month": quota_data["last_reset_month"],
                "last_reset_day": quota_data["last_reset_day"],
                "last_hourly_reset": quota_data["last_hourly_reset"]
            }
        return status

    def get_burn_window_apis(self) -> List[str]:
        """
        Identifie les APIs dont les quotas sont sur le point d'√™tre r√©initialis√©s
        et o√π il est opportun de "br√ªler" le quota restant.
        """
        burn_apis = []
        now = get_current_time()

        for api_name, data in self.quotas.items():
            api_limits = API_QUOTAS.get(api_name, {})

            for limit_type in ["monthly", "daily"]:
                limit = api_limits.get(limit_type)
                if limit is not None and limit > 0:
                    current_usage = data[f"{limit_type}_usage"]
                    
                    if limit_type == "monthly":
                        next_month = now.month + 1
                        year_for_next_month = now.year
                        if next_month > 12:
                            next_month = 1
                            year_for_next_month += 1
                        reset_time = datetime(year_for_next_month, next_month, 1, 0, 0, 0, 0, tzinfo=timezone.utc)
                    else: # daily
                        reset_time = datetime(now.year, now.month, now.day, 0, 0, 0, 0, tzinfo=timezone.utc) + timedelta(days=1)
                    
                    if now < reset_time:
                        time_until_reset = reset_time - now
                        if time_until_reset <= timedelta(hours=BURN_QUOTA_BEFORE_RESET_HOURS):
                            if current_usage < limit:
                                remaining_quota_ratio = (limit - current_usage) / limit
                                if remaining_quota_ratio > 0 and remaining_quota_ratio <= BURN_QUOTA_THRESHOLD_RATIO:
                                    burn_apis.append(f"{api_name} ({limit_type}: {limit - current_usage} restants)")
        return burn_apis


    def should_burn_quota(self, api_name: str) -> bool:
        """
        D√©termine si le quota pour une API donn√©e devrait √™tre 'br√ªl√©' avant r√©initialisation.
        Cette m√©thode est appel√©e par `_auto_burn_quota_task` pour d√©cider si une API doit √™tre utilis√©e.
        """
        self._reset_quotas_if_needed()

        quota_data = self.quotas.get(api_name)
        api_limits = API_QUOTAS.get(api_name)

        if not quota_data or not api_limits:
            return False

        now = get_current_time()

        for limit_type in ["monthly", "daily"]:
            limit = api_limits.get(limit_type)
            if limit is not None and limit > 0:
                current_usage = quota_data[f"{limit_type}_usage"]

                if limit_type == "monthly":
                    next_month = now.month + 1
                    year_for_next_month = now.year
                    if next_month > 12:
                        next_month = 1
                        year_for_next_month += 1
                    reset_time = datetime(year_for_next_month, next_month, 1, 0, 0, 0, 0, tzinfo=timezone.utc)
                else: # daily
                    reset_time = datetime(now.year, now.month, now.day, 0, 0, 0, 0, tzinfo=timezone.utc) + timedelta(days=1)

                if now < reset_time:
                    time_until_reset = reset_time - now
                    if time_until_reset <= timedelta(hours=BURN_QUOTA_BEFORE_RESET_HOURS):
                        remaining_quota_ratio = (limit - current_usage) / limit
                        if remaining_quota_ratio > 0 and remaining_quota_ratio <= BURN_QUOTA_THRESHOLD_RATIO:
                            log_message(f"Mode Burn: {api_name} ({limit_type}) - usage {current_usage}/{limit}, ratio {remaining_quota_ratio:.2f}, reset dans {time_until_reset}", level="debug")
                            return True
        return False

# Instanciation des gestionnaires de m√©moire et de quotas (seront initialis√©s dans main.py)
memory_manager = MemoryManager()
quota_manager = QuotaManager()

# filters_and_tools.py
import json
import asyncio
import random
import ast
import subprocess
import base64
import httpx
import io
import contextlib
import traceback
import hashlib
import difflib
import re
import logging
from datetime import datetime, timedelta, timezone
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, Any, Optional, List, Union, Tuple

# Imports des constantes depuis config.py
from config import FORBIDDEN_WORDS, ARCHIVES_DIR, MAX_FILE_SIZE

# Imports depuis utils.py
from utils import (
    log_message, neutralize_urls, get_user_dir
)

# Imports sp√©cifiques pour les outils de d√©veloppement (assurez-vous que ces biblioth√®ques sont install√©es)
try:
    from pygments import highlight
    from pygments.lexers import PythonLexer
    from pygments.formatters import TerminalFormatter
except ImportError:
    highlight = None
    PythonLexer = None
    TerminalFormatter = None
    log_message("Pygments non trouv√©. La surbrillance syntaxique ne sera pas disponible.", level="warning")

try:
    from pyflakes.api import check
    from pyflakes.reporter import Reporter
except ImportError:
    check = None
    Reporter = None
    log_message("Pyflakes non trouv√©. La v√©rification de code ne sera pas disponible.", level="warning")

try:
    import black
except ImportError:
    black = None
    log_message("Black non trouv√©. Le formatage de code ne sera pas disponible.", level="warning")

# Instancier le client OCR API pour une utilisation directe dans perform_ocr_api
# Note: OCRApiClient est d√©fini dans api_clients.py, donc il doit √™tre import√©.
# Pour √©viter une d√©pendance circulaire si ce fichier est import√© avant api_clients,
# l'instanciation est faite ici, mais la classe OCRApiClient doit √™tre disponible.
# Dans le script final, toutes les classes sont dans le m√™me fichier ou l'ordre d'importation est g√©r√©.
# On va passer l'instance du client OCR via une fonction setter ou directement lors de l'initialisation du bot.
# Pour l'instant, on d√©clare une variable globale qui sera assign√©e plus tard.
ocr_api_client_instance = None # Sera assign√© par main.py

def set_ocr_api_client_instance(client_instance):
    global ocr_api_client_instance
    ocr_api_client_instance = client_instance

# Pour les ex√©cutions en sandbox, nous utiliserons un ThreadPoolExecutor pour ne pas bloquer l'event loop
executor = ThreadPoolExecutor(max_workers=1)

def filter_bad_code(code: str) -> bool:
    """
    Filtre basique pour les commandes de code potentiellement dangereuses.
    Emp√™che l'ex√©cution de certaines op√©rations syst√®me ou de r√©seau.
    """
    forbidden_patterns = [
        r'os\.system', r'subprocess\.run', r'shutil\.rmtree', r'requests\.post',
        r'open\([^,\'"]*\s*,\s*[\'"]w', r'import socket', r'import http',
        r'sys\.exit', r'while True:', r'import threading', r'import multiprocessing'
    ]
    for pattern in forbidden_patterns:
        if re.search(pattern, code):
            log_message(f"Code bloqu√©: motif dangereux d√©tect√©: {pattern}", level="warning")
            return True
    return False

def detect_and_correct_toxicity(text: str) -> str:
    """
    Remplace les propos toxiques (mots d√©finis dans FORBIDDEN_WORDS)
    par des faits scientifiques ou des encouragements.
    """
    if any(word in text.lower() for word in FORBIDDEN_WORDS):
        facts = [
            "Savais-tu que 73% des conflits viennent de malentendus ?",
            "Le cerveau humain est c√¢bl√© pour la coop√©ration, pas le conflit.",
            "En 2025, l'IA √©motionnelle sera la norme. Soyons pr√©curseurs !",
            "Chaque point de vue, m√™me divergent, contribue √† la richesse de la compr√©hension.",
            "L'apprentissage est un processus continu, fait d'exp√©rimentations et d'am√©liorations.",
            "La collaboration est la cl√© de l'innovation."
        ]
        log_message(f"Toxicit√© d√©tect√©e et corrig√©e dans le message: '{text}'", level="info")
        return random.choice(facts) + " Continuons √† construire ensemble !"
    return text

async def run_in_sandbox(code: str, language: str = "python") -> str:
    """
    Ex√©cute du code Python ou Shell dans une sandbox (environnement isol√©).
    Utilise un ThreadPoolExecutor pour ex√©cuter des op√©rations bloquantes de mani√®re asynchrone,
    √©vitant ainsi de bloquer l'event loop principal.
    """
    if filter_bad_code(code):
        return "‚ùå S√©curit√©: Le code contient des motifs potentiellement dangereux et n'a pas √©t√© ex√©cut√©."

    loop = asyncio.get_running_loop()
    if language == "python":
        return await loop.run_in_executor(executor, _run_python_sync, code)
    elif language == "shell":
        return await loop.run_in_executor(executor, _run_shell_sync, code)
    else:
        return "‚ùå Langage non support√© pour la sandbox."

def _run_python_sync(code: str) -> str:
    """
    Ex√©cute du code Python de mani√®re synchrone et capture la sortie standard et d'erreur.
    Utilise un dictionnaire `__builtins__` vide pour isoler l'ex√©cution.
    """
    old_stdout = io.StringIO()
    old_stderr = io.StringIO()
    with contextlib.redirect_stdout(old_stdout), contextlib.redirect_stderr(old_stderr):
        try:
            exec(code, {'__builtins__': {}})
            output = old_stdout.getvalue()
            error = old_stderr.getvalue()
            if error:
                log_message(f"Erreur d'ex√©cution Python en sandbox:\n{error}", level="warning")
                return f"üêç Erreur Python:\n{error}\nSortie:\n{output}"
            return f"‚úÖ Sortie Python:\n{output}"
        except Exception as e:
            log_message(f"Erreur d'ex√©cution Python inattendue en sandbox: {e}\n{traceback.format_exc()}", level="error")
            return f"‚ùå Erreur d'ex√©cution Python: {e}\nSortie standard:\n{old_stdout.getvalue()}\nErreur standard:\n{old_stderr.getvalue()}"

def _run_shell_sync(command: str) -> str:
    """
    Ex√©cute une commande shell de mani√®re synchrone et capture la sortie.
    Utilise `subprocess.run` avec un timeout pour √©viter les blocages.
    """
    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            check=True,
            timeout=10
        )
        output = result.stdout
        error = result.stderr
        if error:
            log_message(f"Erreur d'ex√©cution Shell en sandbox:\n{error}", level="warning")
            return f"üêö Erreur Shell:\n{error}\nSortie:\n{output}"
        return f"‚úÖ Sortie Shell:\n{output}"
    except subprocess.CalledProcessError as e:
        log_message(f"Erreur d'ex√©cution Shell (Code: {e.returncode}):\n{e.stderr}\nSortie:\n{e.stdout}", level="error")
        return f"‚ùå Erreur d'ex√©cution Shell (Code: {e.returncode}):\n{e.stderr}\nSortie:\n{e.stdout}"
    except subprocess.TimeoutExpired:
        log_message("Erreur Shell: La commande a d√©pass√© le temps d'ex√©cution imparti.", level="warning")
        return "‚ùå Erreur Shell: La commande a d√©pass√© le temps d'ex√©cution imparti."
    except Exception as e:
        log_message(f"Erreur inattendue lors de l'ex√©cution Shell: {e}\n{traceback.format_exc()}", level="error")
        return f"‚ùå Erreur inattendue lors de l'ex√©cution Shell: {e}"

def syntax_highlight(code: str) -> str:
    """
    Met en surbrillance la syntaxe du code Python en utilisant Pygments.
    Retourne le code format√© pour l'affichage en console ou dans un bloc `<pre>`.
    """
    if highlight and PythonLexer and TerminalFormatter:
        try:
            return highlight(code, PythonLexer(), TerminalFormatter())
        except Exception as e:
            log_message(f"Erreur de surbrillance syntaxique: {e}", level="error")
            return code
    else:
        return "‚ùå Outil de surbrillance syntaxique (Pygments) non disponible."

def check_code(code: str) -> str:
    """
    V√©rifie le code Python avec Pyflakes pour d√©tecter les erreurs de syntaxe et les probl√®mes de style.
    """
    if check and Reporter:
        out = io.StringIO()
        reporter = Reporter(out, out)
        check(code, filename="<string>", reporter=reporter)
        result = out.getvalue()
        return result if result else "‚úÖ Pyflakes: Aucun probl√®me d√©tect√©."
    else:
        return "‚ùå Outil de v√©rification de code (Pyflakes) non disponible."

def format_code(code: str) -> str:
    """
    Formate le code Python avec Black pour assurer une coh√©rence stylistique.
    """
    if black:
        try:
            mode = black.Mode()
            return black.format_str(code, mode=mode)
        except black.InvalidInput as e:
            log_message(f"Erreur de formatage (Black): Code Python invalide. {e}", level="error")
            return f"‚ùå Erreur de formatage (Black): Code Python invalide. {e}"
        except Exception as e:
            log_message(f"Erreur de formatage (Black): {e}", level="error")
            return f"‚ùå Erreur de formatage (Black): {e}"
    else:
        return "‚ùå Outil de formatage de code (Black) non disponible."

def extract_functions(code: str) -> Union[List[str], str]:
    """
    Extrait les noms des fonctions d√©finies dans un code Python en utilisant l'AST.
    """
    try:
        tree = ast.parse(code)
        functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        return functions if functions else "Aucune fonction d√©tect√©e."
    except SyntaxError as e:
        log_message(f"Erreur de syntaxe Python lors de l'extraction des fonctions: {e}", level="error")
        return f"‚ùå Erreur de syntaxe Python: {e}"
    except Exception as e:
        log_message(f"Erreur lors de l'extraction des fonctions: {e}", level="error")
        return f"‚ùå Erreur lors de l'extraction des fonctions: {e}"

def analyze_code_structure(code: str) -> str:
    """
    Analyse la structure AST (Arbre Syntaxique Abstrait) d'un code Python.
    Utile pour comprendre la composition du code.
    """
    try:
        tree = ast.parse(code)
        return ast.dump(tree, indent=2)
    except SyntaxError as e:
        log_message(f"Erreur de syntaxe Python lors de l'analyse AST: {e}", level="error")
        return f"‚ùå Erreur de syntaxe Python: {e}"
    except Exception as e:
        log_message(f"Erreur lors de l'analyse de la structure AST: {e}", level="error")
        return f"‚ùå Erreur lors de l'analyse de la structure AST: {e}"

async def perform_ocr_api(image_url: str) -> str:
    """
    Effectue l'OCR sur une URL d'image en utilisant l'API OCR.space via `OCRApiClient`.
    T√©l√©charge l'image, l'encode en base64, puis envoie √† l'API.
    """
    if ocr_api_client_instance is None:
        return "‚ùå L'instance du client OCR n'est pas initialis√©e."

    try:
        async with httpx.AsyncClient(timeout=10) as client:
            img_response = await client.get(image_url)
            img_response.raise_for_status()

        base64_image_data = base64.b64encode(img_response.content).decode('utf-8')
        content_type = img_response.headers.get("Content-Type", "image/jpeg")
        image_base64_with_prefix = f"data:{content_type};base64,{base64_image_data}"

        result = await ocr_api_client_instance.query(image_base64_with_prefix)
        return result

    except httpx.HTTPStatusError as e:
        log_message(f"Erreur HTTP/r√©seau lors de l'OCR: {e.response.status_code} - {e.response.text}", level="error")
        return f"‚ùå Erreur lors de l'OCR (r√©seau/API): {e}"
    except httpx.RequestError as e:
        log_message(f"Erreur de requ√™te lors de l'OCR: {e}", level="error")
        return f"‚ùå Erreur lors de l'OCR (requ√™te): {e}"
    except Exception as e:
        log_message(f"Erreur inattendue lors de l'OCR: {e}\n{traceback.format_exc()}", level="error")
        return f"‚ùå Erreur inattendue lors de l'OCR: {e}"

async def fetch_and_archive_pages(links: List[str], user_id: Union[int, str], bot_instance: Any):
    """
    T√©l√©charge toutes les pages des liens fournis, les archive localement,
    puis les envoie au groupe priv√© Telegram.
    """
    user_archive_dir = get_user_dir(user_id) / ARCHIVES_DIR
    user_archive_dir.mkdir(exist_ok=True, parents=True)

    for idx, url in enumerate(links):
        try:
            async with httpx.AsyncClient(timeout=20) as client:
                r = await client.get(url)
                r.raise_for_status()

                if len(r.content) < MAX_FILE_SIZE:
                    ext = ".html" if "<html" in r.text.lower() else ".txt"
                    url_hash = hashlib.sha256(url.encode('utf-8')).hexdigest()[:10]
                    fname = f"page_{datetime.now().strftime('%Y%m%d%H%M%S')}_{url_hash}_{idx}{ext}"
                    fpath = user_archive_dir / fname
                    fpath.write_text(r.text, encoding="utf-8", errors="ignore")

                    if bot_instance and hasattr(bot_instance, 'send_document'):
                        try:
                            with fpath.open("rb") as f:
                                await bot_instance.send_document(chat_id=PRIVATE_GROUP_ID, document=f, filename=fname, caption=f"Page archiv√©e de {neutralize_urls(url)}")
                        except Exception as send_e:
                            log_message(f"Erreur lors de l'envoi du document archiv√© au groupe priv√©: {send_e}", level="error")

                    log_message(f"Page archiv√©e: {url} pour user {user_id}")
                else:
                    log_message(f"Page trop grande pour √™tre archiv√©e: {url} ({len(r.content)} bytes)", level="warning")
        except httpx.HTTPStatusError as e:
            log_message(f"[fetch_and_archive_pages] Erreur HTTP pour {url}: {e.response.status_code} - {e.response.text}", level="error")
        except httpx.RequestError as e:
            log_message(f"[fetch_and_archive_pages] Erreur de requ√™te pour {url}: {e}", level="error")
        except Exception as e:
            log_message(f"[fetch_and_archive_pages] Erreur inattendue pour {url}: {e}\n{traceback.format_exc()}", level="error")

# main.py
import asyncio
import telebot
import traceback
import json
import re
from telebot.async_telebot import AsyncTeleBot
from telebot.types import Message
from typing import Dict, Any, List, Optional, Union

# Importation des constantes et configurations
from config import (
        BOT_TOKEN, PRIVATE_GROUP_ID, API_QUOTAS, API_COOLDOWN_DURATION_SECONDS, # BOT_TOKEN au lieu de TELEGRAM_BOT_TOKEN
        API_ROTATION_INTERVAL_MINUTES, BURN_QUOTA_BEFORE_RESET_HOURS,
        USER_CHAT_HISTORY_FILE, USER_LONG_MEMORY_FILE, IA_STATUS_FILE,
        QUOTAS_FILE, GROUP_CHAT_HISTORY_FILE, MAX_CACHE_SIZE, FORBIDDEN_WORDS,
        GEMINI_API_KEY, OCR_API_KEY, API_CONFIG
    )

# Importation des modules locaux
from utils import (
    load_json, save_json, get_user_dir,
    get_current_time, format_datetime, log_message,
    neutralize_urls, similar, set_file_lock
)
from api_clients import (
    EndpointHealthManager, APIClient, DeepSeekClient, SerperClient, WolframAlphaClient,
    TavilyClient, ApiFlashClient, CrawlbaseClient, DetectLanguageClient, GuardianClient,
    IP2LocationClient, ShodanClient, WeatherAPIClient, CloudmersiveClient,
    GreyNoiseClient, PulsediveClient, StormGlassClient, LoginRadiusClient,
    JsonbinClient, HuggingFaceClient, TwilioClient, AbstractAPIClient,
    GeminiAPIClient, GoogleCustomSearchClient, RandommerClient, TomorrowIOClient,
    OpenWeatherMapClient, MockarooClient, OpenPageRankClient, RapidAPIClient,
    set_endpoint_health_manager_global, ALL_API_CLIENTS # Import ALL_API_CLIENTS list
)
from memory_and_quotas import MemoryManager, QuotaManager
from filters_and_tools import (
    filter_bad_code, detect_and_correct_toxicity, run_in_sandbox,
    perform_ocr_api, fetch_and_archive_pages, set_ocr_api_client_instance
)

# --- Initialisation du bot et des gestionnaires ---
bot = AsyncTeleBot(TELEGRAM_BOT_TOKEN)

# Instanciation des gestionnaires (ils sont des singletons, donc une seule instance)
endpoint_health_manager = EndpointHealthManager()
memory_manager = MemoryManager()
quota_manager = QuotaManager()

# Injection des instances n√©cessaires
set_endpoint_health_manager_global(endpoint_health_manager) # Pour api_clients.py
quota_manager.set_bot_instance(bot) # Pour que le quota_manager puisse envoyer des alertes
set_file_lock(asyncio.Lock()) # Injecte le verrou de fichier global dans utils.py

# Instanciation des clients API
# Note: GeminiAPIClient et OCRApiClient sont g√©r√©s s√©par√©ment car ils n'utilisent pas le syst√®me de s√©lection d'endpoint dynamique
gemini_client = GeminiAPIClient()
ocr_client = OCRApiClient() # Instancie ici
set_ocr_api_client_instance(ocr_client) # Injecte l'instance dans filters_and_tools.py

# Dictionnaire des clients API pour un acc√®s facile par nom
api_clients: Dict[str, APIClient] = {
    client.name: client for client in ALL_API_CLIENTS
}

# --- Fonctions utilitaires du bot ---

async def send_message_to_private_group(text: str):
    """Envoie un message au groupe priv√© configur√©."""
    if PRIVATE_GROUP_ID:
        try:
            await bot.send_message(PRIVATE_GROUP_ID, text)
        except Exception as e:
            log_message(f"Erreur lors de l'envoi au groupe priv√©: {e}", level="error")

async def get_user_id_from_message(message: Message) -> Union[int, str]:
    """
    R√©cup√®re l'ID de l'utilisateur ou du groupe √† partir d'un message.
    Utilise l'ID du chat pour les groupes et l'ID de l'exp√©diteur pour les messages priv√©s.
    """
    return message.chat.id if message.chat.type != "private" else message.from_user.id

def get_sender_info(message: Message) -> str:
    """Retourne une cha√Æne d'informations sur l'exp√©diteur du message."""
    if message.chat.type == "private":
        return f"Utilisateur: {message.from_user.first_name} (@{message.from_user.username} - {message.from_user.id})"
    else:
        return f"Groupe: {message.chat.title} (ID: {message.chat.id}), Exp√©diteur: {message.from_user.first_name} (@{message.from_user.username} - {message.from_user.id})"

# --- Boucles de fond pour la maintenance ---

async def _health_check_loop():
    """Boucle de fond pour ex√©cuter des checks de sant√© r√©guliers sur les endpoints API."""
    while True:
        log_message("Lancement des health checks pour tous les services API...")
        for service_name in API_CONFIG.keys():
            await endpoint_health_manager.run_health_check_for_service(service_name)
        await asyncio.sleep(API_ROTATION_INTERVAL_MINUTES * 60) # Ex√©cute toutes les X minutes

async def _quota_burn_loop():
    """
    Boucle de fond pour "br√ªler" les quotas API avant leur r√©initialisation.
    Tente d'utiliser les APIs qui sont dans leur fen√™tre de "br√ªlage".
    """
    while True:
        burn_apis_info = quota_manager.get_burn_window_apis()
        if burn_apis_info:
            log_message(f"APIs en mode 'burn' d√©tect√©es: {', '.join(burn_apis_info)}")
            for api_info_str in burn_apis_info:
                api_name = api_info_str.split(" ")[0] # Extrait le nom de l'API
                if quota_manager.should_burn_quota(api_name):
                    log_message(f"Tentative de 'br√ªlage' de quota pour {api_name}...", level="info")
                    client = api_clients.get(api_name)
                    if client:
                        try:
                            # Utilise des prompts al√©atoires pour Gemini ou des appels g√©n√©riques pour les autres
                            if api_name == "GEMINI":
                                from config import AUTO_BURN_PROMPTS
                                prompt = random.choice(AUTO_BURN_PROMPTS.get("GEMINI", ["G√©n√®re un texte technique."]))
                                await gemini_client.generate_content(prompt=prompt, chat_history=[])
                            elif api_name == "OCR_API":
                                from config import AUTO_BURN_PROMPTS
                                prompt = random.choice(AUTO_BURN_PROMPTS.get("OCR_API", ["D√©cris un document."]))
                                # Pour OCR, simuler un appel avec une URL bidon, l'API r√©elle ne sera pas appel√©e
                                # car perform_ocr_api n√©cessite une image valide.
                                # L'objectif ici est de d√©clencher la logique de quota.
                                # Une meilleure approche serait d'avoir une m√©thode de "burn" dans le client OCR lui-m√™me.
                                # Pour l'instant, on se contente de la v√©rification de quota.
                                await quota_manager.check_and_update_quota(api_name, cost=1)
                            else:
                                # Appels g√©n√©riques pour d'autres APIs
                                if api_name == "DEEPSEEK":
                                    await client.query(prompt="test")
                                elif api_name == "SERPER":
                                    await client.query(query_text="test")
                                elif api_name == "WOLFRAMALPHA":
                                    await client.query(input_text="1+1")
                                elif api_name == "TAVILY":
                                    await client.query(query_text="test")
                                elif api_name == "APIFLASH":
                                    pass # Requires a valid URL, hard to burn
                                elif api_name == "CRAWLBASE":
                                    pass # Requires a valid URL, hard to burn
                                elif api_name == "DETECTLANGUAGE":
                                    await client.query(text="hello")
                                elif api_name == "GUARDIAN":
                                    await client.query(query_text="news")
                                elif api_name == "IP2LOCATION":
                                    await client.query(ip_address="8.8.8.8")
                                elif api_name == "SHODAN":
                                    await client.query(query_text="test")
                                elif api_name == "WEATHERAPI":
                                    await client.query(location="London")
                                elif api_name == "CLOUDMERSIVE":
                                    await client.query(domain="example.com")
                                elif api_name == "GREYNOISE":
                                    await client.query(ip_address="8.8.8.8")
                                elif api_name == "PULSEDIVE":
                                    await client.query(indicator="8.8.8.8")
                                elif api_name == "STORMGLASS":
                                    await client.query(lat=0.0, lng=0.0)
                                elif api_name == "LOGINRADIUS":
                                    await client.query()
                                elif api_name == "JSONBIN":
                                    await client.query(data={"burn": True})
                                elif api_name == "HUGGINGFACE":
                                    await client.query(input_text="test")
                                elif api_name == "TWILIO":
                                    await client.query()
                                elif api_name == "ABSTRACTAPI":
                                    await client.query(input_value="test@example.com", api_type="EMAIL_VALIDATION")
                                elif api_name == "GOOGLE_CUSTOM_SEARCH":
                                    await client.query(query_text="test")
                                elif api_name == "RANDOMMER":
                                    await client.query(quantity=1)
                                elif api_name == "TOMORROW.IO":
                                    await client.query(location="London")
                                elif api_name == "OPENWEATHERMAP":
                                    await client.query(location="London")
                                elif api_name == "MOCKAROO":
                                    await client.query(count=1)
                                elif api_name == "OPENPAGERANK":
                                    await client.query(domains=["example.com"])
                                elif api_name == "RAPIDAPI":
                                    await client.query(api_name="random fact")

                            log_message(f"Quota pour {api_name} 'br√ªl√©' avec succ√®s.")
                        except Exception as e:
                            log_message(f"Erreur lors du 'br√ªlage' de quota pour {api_name}: {e}", level="error")
                    else:
                        log_message(f"Client API {api_name} non trouv√© pour le 'br√ªlage' de quota.", level="warning")
        await asyncio.sleep(BURN_QUOTA_BEFORE_RESET_HOURS * 3600 / 4) # V√©rifie 4 fois par fen√™tre de br√ªlage

async def _diversification_recovery_loop():
    """Boucle de fond pour r√©cup√©rer les scores de diversification des IA."""
    while True:
        memory_manager.recover_diversification_scores()
        await asyncio.sleep(API_ROTATION_INTERVAL_MINUTES * 60) # Ex√©cute toutes les X minutes

# --- Gestionnaires de messages Telegram ---

@bot.message_handler(commands=['start', 'help'])
async def send_welcome(message: Message):
    """Envoie un message de bienvenue et d'aide."""
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    log_message(f"Commande /start ou /help re√ßue de {sender_info}")

    welcome_text = (
        "Bonjour ! Je suis votre assistant IA. Je peux vous aider avec des questions, "
        "ex√©cuter du code, faire de l'OCR, et bien plus encore.\n\n"
        "Voici quelques commandes que vous pouvez utiliser :\n"
        "/status - Affiche le statut des APIs et les quotas.\n"
        "/run_code [langage] [code] - Ex√©cute du code (ex: /run_code python print('Hello')).\n"
        "/ocr [URL_image] - Extrait le texte d'une image via son URL.\n"
        "/archive_page [URL] - Archive une page web et l'envoie au groupe priv√©.\n"
        "/burn_quota - Tente de 'br√ªler' les quotas API avant leur r√©initialisation.\n"
        "Posez-moi simplement une question ou donnez-moi une t√¢che !"
    )
    await bot.reply_to(message, welcome_text)
    await memory_manager.add_message_to_history(user_id, "bot", welcome_text)

@bot.message_handler(commands=['status'])
async def get_status(message: Message):
    """Affiche le statut des APIs et les quotas d'utilisation."""
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    log_message(f"Commande /status re√ßue de {sender_info}")

    ia_status = memory_manager.ia_status
    quotas_status = quota_manager.get_all_quotas_status()

    status_text = "üìä *Statut des APIs et Quotas:*\n\n"

    for api_name, status in ia_status.items():
        quota_data = quotas_status.get(api_name, {})

        status_text += f"*{api_name}:*\n"
        status_text += f"  - Score de performance: `{status['current_score']:.2f}`\n"
        status_text += f"  - Score de diversification: `{status['diversification_score']:.2f}`\n"
        status_text += f"  - Erreurs cons√©cutives: `{status['error_count']}`\n"
        status_text += f"  - En cooldown jusqu'√†: `{status['cooldown_until'] or 'N/A'}`\n"

        if quota_data:
            status_text += f"  - Quota Mensuel: `{quota_data['monthly_usage']}/{quota_data['monthly_limit']}`\n"
            status_text += f"  - Quota Journalier: `{quota_data['daily_usage']}/{quota_data['daily_limit']}`\n"
            status_text += f"  - Quota Horaire: `{quota_data['hourly_usage']}/{quota_data['hourly_limit']}`\n"
            status_text += f"  - Dernier usage: `{quota_data['last_usage'] or 'N/A'}`\n"
        else:
            status_text += "  - _Donn√©es de quota non disponibles._\n"
        status_text += "\n"

    burn_apis = quota_manager.get_burn_window_apis()
    if burn_apis:
        status_text += "üî• *APIs en mode 'Br√ªlage' de quota:*\n"
        for api_info in burn_apis:
            status_text += f"- {api_info}\n"
    else:
        status_text += "üî• _Aucune API en mode 'Br√ªlage' de quota actuellement._\n"

    await bot.reply_to(message, status_text, parse_mode="Markdown")
    await memory_manager.add_message_to_history(user_id, "bot", status_text)

@bot.message_handler(commands=['burn_quota'])
async def trigger_burn_quota(message: Message):
    """D√©clenche manuellement la tentative de 'br√ªlage' de quota."""
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    log_message(f"Commande /burn_quota re√ßue de {sender_info}")

    burn_apis = quota_manager.get_burn_window_apis()
    if not burn_apis:
        response_text = "Aucune API n'est actuellement dans sa fen√™tre de 'br√ªlage' de quota."
        await bot.reply_to(message, response_text)
        await memory_manager.add_message_to_history(user_id, "bot", response_text)
        return

    response_text = "Tentative de 'br√ªlage' de quota pour les APIs suivantes:\n" + "\n".join(burn_apis)
    await bot.reply_to(message, response_text)
    await memory_manager.add_message_to_history(user_id, "bot", response_text)

    asyncio.create_task(_quota_burn_loop())

@bot.message_handler(commands=['run_code'])
async def handle_run_code(message: Message):
    """Ex√©cute le code fourni dans une sandbox."""
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    log_message(f"Commande /run_code re√ßue de {sender_info}")

    args = message.text.split(maxsplit=2)
    if len(args) < 3:
        await bot.reply_to(message, "Usage: `/run_code <langage> <votre_code>` (ex: `/run_code python print('Hello')`)", parse_mode="Markdown")
        await memory_manager.add_message_to_history(user_id, "bot", "Usage: `/run_code <langage> <votre_code>`")
        return

    language = args[1].lower()
    code_to_run = args[2]

    response_text = await run_in_sandbox(code_to_run, language)
    await bot.reply_to(message, f"```\n{response_text}\n```", parse_mode="Markdown")
    await memory_manager.add_message_to_history(user_id, "bot", response_text)

@bot.message_handler(commands=['ocr'])
async def handle_ocr_command(message: Message):
    """Effectue l'OCR sur une image √† partir d'une URL."""
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    log_message(f"Commande /ocr re√ßue de {sender_info}")

    args = message.text.split(maxsplit=1)
    if len(args) < 2:
        await bot.reply_to(message, "Usage: `/ocr <URL_de_l_image>`", parse_mode="Markdown")
        await memory_manager.add_message_to_history(user_id, "bot", "Usage: `/ocr <URL_de_l_image>`")
        return

    image_url = args[1].strip()
    if not re.match(r"https?://.*\.(png|jpg|jpeg|gif|bmp|tiff|webp)$", image_url, re.IGNORECASE):
        await bot.reply_to(message, "L'URL de l'image semble invalide ou le format n'est pas support√© (doit √™tre png, jpg, jpeg, gif, bmp, tiff, webp).")
        await memory_manager.add_message_to_history(user_id, "bot", "URL d'image invalide.")
        return

    try:
        await bot.reply_to(message, "Traitement de l'image par OCR, veuillez patienter...")
        ocr_result = await perform_ocr_api(image_url)
        await bot.reply_to(message, f"Texte extrait:\n```\n{ocr_result}\n```", parse_mode="Markdown")
        await memory_manager.add_message_to_history(user_id, "bot", f"OCR de {image_url}: {ocr_result}")
    except Exception as e:
        log_message(f"Erreur lors de l'OCR de l'image {image_url}: {e}\n{traceback.format_exc()}", level="error")
        await bot.reply_to(message, f"Une erreur est survenue lors de l'extraction du texte de l'image: {e}")
        await memory_manager.add_message_to_history(user_id, "bot", f"Erreur OCR: {e}")

@bot.message_handler(commands=['archive_page'])
async def handle_archive_page(message: Message):
    """Archive une page web et l'envoie au groupe priv√©."""
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    log_message(f"Commande /archive_page re√ßue de {sender_info}")

    args = message.text.split(maxsplit=1)
    if len(args) < 2:
        await bot.reply_to(message, "Usage: `/archive_page <URL>`", parse_mode="Markdown")
        await memory_manager.add_message_to_history(user_id, "bot", "Usage: `/archive_page <URL>`")
        return

    url_to_archive = args[1].strip()
    if not url_to_archive.startswith(("http://", "https://")):
        await bot.reply_to(message, "Veuillez fournir une URL valide (commen√ßant par http:// ou https://).")
        await memory_manager.add_message_to_history(user_id, "bot", "URL invalide pour l'archivage.")
        return

    await bot.reply_to(message, f"Archivage de la page {url_to_archive}, veuillez patienter...")
    try:
        await fetch_and_archive_pages([url_to_archive], user_id, bot)
        response_text = f"Page archiv√©e et envoy√©e au groupe priv√©: {neutralize_urls(url_to_archive)}"
        await bot.reply_to(message, response_text)
        await memory_manager.add_message_to_history(user_id, "bot", response_text)
    except Exception as e:
        log_message(f"Erreur lors de l'archivage de la page {url_to_archive}: {e}\n{traceback.format_exc()}", level="error")
        await bot.reply_to(message, f"Une erreur est survenue lors de l'archivage de la page: {e}")
        await memory_manager.add_message_to_history(user_id, "bot", f"Erreur archivage: {e}")

@bot.message_handler(func=lambda message: True)
async def handle_all_messages(message: Message):
    """
    Gestionnaire principal pour tous les messages textuels.
    Traite la requ√™te, choisit la meilleure IA, g√®re la m√©moire et les outils.
    """
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    raw_text = message.text
    log_message(f"Message re√ßu de {sender_info}: {raw_text}")

    await memory_manager.add_message_to_history(user_id, "user", raw_text)

    if message.chat.type != "private":
        await memory_manager.save_group_memory(user_id, "user", raw_text)

    cached_response = await memory_manager.check_for_similar_prompt(user_id, raw_text)
    if cached_response:
        log_message(f"R√©ponse en cache trouv√©e pour {user_id}.")
        await bot.reply_to(message, cached_response)
        await memory_manager.add_message_to_history(user_id, "bot", cached_response)
        return

    cleaned_text = detect_and_correct_toxicity(raw_text)
    if cleaned_text != raw_text:
        await bot.reply_to(message, f"Votre message a √©t√© mod√©r√©: {cleaned_text}")
        await memory_manager.add_message_to_history(user_id, "bot", cleaned_text)
        return

    chat_history_for_gemini = await memory_manager.get_chat_history(user_id, limit=10)
    long_term_memory_for_gemini = await memory_manager.get_long_term_memory(user_id, limit=5)
    group_memory_for_gemini = ""
    if message.chat.type != "private":
        group_memory_for_gemini = await memory_manager.get_group_memory(user_id, limit=5)

    context_parts = []
    if long_term_memory_for_gemini:
        context_parts.append(f"M√©moire √† long terme de l'utilisateur:\n{long_term_memory_for_gemini}")
    if group_memory_for_gemini:
        context_parts.append(f"Contexte du groupe:\n{group_memory_for_gemini}")

    full_context = "\n\n".join(context_parts) if context_parts else ""

    available_apis = memory_manager.get_available_ias()
    api_info_for_prompt = "APIs disponibles et leur statut:\n"
    for api_name in available_apis:
        status = memory_manager.get_ia_status(api_name)
        if status:
            api_info_for_prompt += f"- {api_name}: Score {status['current_score']:.2f}, Diversification {status['diversification_score']:.2f}\n"

    system_instruction = (
        "Vous √™tes un assistant IA avanc√©. Votre objectif est de r√©pondre aux requ√™tes de l'utilisateur "
        "de mani√®re utile, pr√©cise et concise. Vous avez acc√®s √† plusieurs outils et APIs pour vous aider.\n"
        "Lorsque vous utilisez un outil, indiquez clairement quel outil vous utilisez et pourquoi.\n"
        "Si une requ√™te n√©cessite une recherche web, utilisez les outils de recherche disponibles.\n"
        "Si une requ√™te est complexe ou n√©cessite plusieurs √©tapes, d√©composez-la.\n"
        "N'inventez pas d'informations. Si vous ne savez pas, dites-le.\n"
        "Contexte suppl√©mentaire de l'utilisateur et du groupe:\n"
        f"{full_context}\n\n"
        f"{api_info_for_prompt}\n"
        "Vous pouvez aussi ex√©cuter du code Python ou Shell en utilisant la fonction `run_in_sandbox(code, language='python')`.\n"
        "Pour l'OCR, utilisez `perform_ocr_api(image_url)`.\n"
        "Pour archiver des pages web, utilisez `fetch_and_archive_pages(links, user_id, bot_instance)`.\n"
        "Pour les recherches web, utilisez `api_clients['SERPER'].query(query_text)` ou `api_clients['TAVILY'].query(query_text)`.\n"
        "Pour les calculs ou faits, utilisez `api_clients['WOLFRAMALPHA'].query(input_text)`.\n"
        "Si l'utilisateur demande une t√¢che impliquant un outil, proposez d'utiliser l'outil et montrez comment.\n"
        "Si l'utilisateur demande des informations sur les quotas ou le statut des APIs, utilisez les commandes /status.\n"
        "Si l'utilisateur pose une question g√©n√©rale, utilisez DeepSeek."
    )

    selected_api_name = None
    best_combined_score = -float('inf')

    for api_name in available_apis:
        status = memory_manager.get_ia_status(api_name)
        if status:
            combined_score = (status["current_score"] * 0.7) + (status["diversification_score"] * 0.3)
            if combined_score > best_combined_score:
                best_combined_score = combined_score
                selected_api_name = api_name

    if not selected_api_name:
        await bot.reply_to(message, "D√©sol√©, aucune IA n'est actuellement disponible pour traiter votre requ√™te.")
        await memory_manager.add_message_to_history(user_id, "bot", "Aucune IA disponible.")
        return

    log_message(f"IA s√©lectionn√©e pour {user_id}: {selected_api_name} (Score combin√©: {best_combined_score:.2f})")
    await bot.send_chat_action(message.chat.id, 'typing')

    try:
        gemini_chat_history_formatted = []
        for entry in chat_history_for_gemini:
            role = "user" if entry["role"] == "user" else "model"
            gemini_chat_history_formatted.append({"role": role, "parts": [{"text": entry["content"]}]})

        final_gemini_prompt = f"{system_instruction}\n\nRequ√™te de l'utilisateur: {raw_text}"

        gemini_response_raw = await gemini_client.generate_content(
            prompt=final_gemini_prompt,
            chat_history=gemini_chat_history_formatted,
            model="gemini-1.5-flash-latest"
        )

        gemini_text_response = ""
        if gemini_response_raw and not gemini_response_raw.get("error"):
            candidates = gemini_response_raw.get("candidates", [])
            if candidates:
                first_candidate = candidates[0]
                if "content" in first_candidate and "parts" in first_candidate["content"]:
                    for part in first_candidate["content"]["parts"]:
                        if "text" in part:
                            gemini_text_response += part["text"]
            else:
                gemini_text_response = "Gemini n'a pas pu g√©n√©rer de r√©ponse."
        else:
            gemini_text_response = f"Erreur lors de l'appel √† Gemini: {gemini_response_raw.get('error', 'Inconnu')}"

        final_bot_response = gemini_text_response
        tool_executed = False

        # Simplified tool execution logic based on text pattern matching
        if "run_in_sandbox(" in gemini_text_response:
            match_python = re.search(r"run_in_sandbox\(['\"](.*?)['\"],\s*language=['\"]python['\"]\)", gemini_text_response, re.DOTALL)
            match_shell = re.search(r"run_in_sandbox\(['\"](.*?)['\"],\s*language=['\"]shell['\"]\)", gemini_text_response, re.DOTALL)

            if match_python:
                code_to_execute = match_python.group(1).strip()
                log_message(f"Gemini a sugg√©r√© l'ex√©cution de code Python: {code_to_execute[:100]}...")
                tool_output = await run_in_sandbox(code_to_execute, "python")
                final_bot_response = f"J'ai ex√©cut√© le code Python:\n```\n{code_to_execute}\n```\nR√©sultat:\n```\n{tool_output}\n```"
                tool_executed = True
            elif match_shell:
                code_to_execute = match_shell.group(1).strip()
                log_message(f"Gemini a sugg√©r√© l'ex√©cution de code Shell: {code_to_execute[:100]}...")
                tool_output = await run_in_sandbox(code_to_execute, "shell")
                final_bot_response = f"J'ai ex√©cut√© la commande Shell:\n```\n{code_to_execute}\n```\nR√©sultat:\n```\n{tool_output}\n```"
                tool_executed = True

        elif "perform_ocr_api(" in gemini_text_response:
            match = re.search(r"perform_ocr_api\(['\"](.*?)['\"]\)", gemini_text_response)
            if match:
                image_url = match.group(1)
                log_message(f"Gemini a sugg√©r√© l'ex√©cution de l'OCR sur: {image_url}")
                tool_output = await perform_ocr_api(image_url)
                final_bot_response = f"J'ai effectu√© l'OCR sur l'image:\n{tool_output}"
                tool_executed = True

        elif "fetch_and_archive_pages(" in gemini_text_response:
            match = re.search(r"fetch_and_archive_pages\(\[(.*?)\],\s*user_id,\s*bot_instance\)", gemini_text_response)
            if match:
                links_str = match.group(1)
                links = [link.strip().strip("'\"") for link in links_str.split(',')]
                log_message(f"Gemini a sugg√©r√© l'archivage des pages: {links}")
                await fetch_and_archive_pages(links, user_id, bot)
                final_bot_response = f"J'ai archiv√© les pages demand√©es et les ai envoy√©es au groupe priv√©."
                tool_executed = True

        elif "api_clients['SERPER'].query(" in gemini_text_response:
            match = re.search(r"api_clients\['SERPER'\]\.query\(['\"](.*?)['\"]\)", gemini_text_response)
            if match:
                query_text = match.group(1)
                log_message(f"Gemini a sugg√©r√© une recherche Serper pour: {query_text}")
                serper_client_instance = api_clients.get("SERPER")
                if serper_client_instance:
                    tool_output = await serper_client_instance.query(query_text)
                    final_bot_response = f"R√©sultat de la recherche web (Serper):\n{tool_output}"
                    tool_executed = True
                else:
                    final_bot_response = "L'outil de recherche Serper n'est pas disponible."

        elif "api_clients['WOLFRAMALPHA'].query(" in gemini_text_response:
            match = re.search(r"api_clients\['WOLFRAMALPHA'\]\.query\(['\"](.*?)['\"]\)", gemini_text_response)
            if match:
                input_text = match.group(1)
                log_message(f"Gemini a sugg√©r√© un calcul WolframAlpha pour: {input_text}")
                wolfram_client_instance = api_clients.get("WOLFRAMALPHA")
                if wolfram_client_instance:
                    tool_output = await wolfram_client_instance.query(input_text)
                    final_bot_response = f"R√©sultat WolframAlpha:\n{tool_output}"
                    tool_executed = True
                else:
                    final_bot_response = "L'outil WolframAlpha n'est pas disponible."

        if not tool_executed:
            final_bot_response = gemini_text_response
            log_message(f"Gemini a r√©pondu directement: {final_bot_response[:200]}...")

        # Update API status and quotas for Gemini (as it was the orchestrator)
        memory_manager.update_ia_status("GEMINI", success=True)
        await quota_manager.check_and_update_quota("GEMINI", cost=1)

    except Exception as e:
        log_message(f"Erreur inattendue lors du traitement du message: {e}\n{traceback.format_exc()}", level="error")
        final_bot_response = "D√©sol√©, une erreur interne est survenue. Veuillez r√©essayer plus tard."
        memory_manager.update_ia_status("GEMINI", success=False, error_message=str(e))

    await bot.reply_to(message, final_bot_response)
    await memory_manager.add_message_to_history(user_id, "bot", final_bot_response)

    if message.chat.type != "private":
        await memory_manager.save_group_memory(user_id, "bot", final_bot_response)

# --- Boucle principale d'ex√©cution ---
async def main():
    """Fonction principale pour initialiser et d√©marrer le bot."""
    log_message("D√©marrage de l'initialisation du bot...")

    await endpoint_health_manager.init_manager()
    await memory_manager.init_manager()
    await quota_manager.init_manager()

    log_message("Gestionnaires initialis√©s. D√©marrage des boucles de fond...")

    asyncio.create_task(_health_check_loop())
    asyncio.create_task(_quota_burn_loop())
    asyncio.create_task(_diversification_recovery_loop())

    log_message("Boucles de fond d√©marr√©es. D√©marrage du polling du bot...")

    await bot.infinity_polling()

if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        log_message("Bot arr√™t√© manuellement.")
    except Exception as e:
        log_message(f"Erreur fatale dans la boucle principale: {e}\n{traceback.format_exc()}", level="critical")





import os
import json
from datetime import datetime, timezone, date, timedelta
from pathlib import Path

# ----------------------------
# CONFIGURATION & CONSTANTES GLOBALES
# ----------------------------

# ==== Chemins de fichiers & Limites ====
# Assure que le temps est toujours en UTC pour une coh√©rence globale
os.environ["TZ"] = "UTC" 

# R√©pertoire de base pour toutes les sauvegardes et donn√©es
BASE_DIR = Path(__file__).resolve().parent / "sauvegardes"
# Chemin du fichier de log des erreurs critiques
ERROR_LOG_PATH = BASE_DIR / "erreurs.log"
# Chemin du fichier de log g√©n√©ral du bot (pour le suivi des op√©rations)
LOG_FILE = BASE_DIR / "bot_log.log"

# R√©pertoires sp√©cifiques pour les donn√©es utilisateur et les d√©fis de code
DAILY_CHALLENGE_PATH = Path(__file__).resolve().parent / "defis_code"
HISTORY_DIR = DAILY_CHALLENGE_PATH / "history" # Pour l'historique des d√©fis de code

# Fichiers globaux pour le statut des IA et les quotas
IA_STATUS_FILE = BASE_DIR / "ia_status.json"
QUOTAS_FILE = BASE_DIR / "quotas.json"
ENDPOINT_HEALTH_FILE = BASE_DIR / "endpoint_health.json"

# Fichiers sp√©cifiques √† l'utilisateur (stock√©s dans sauvegardes/{user_id}/)
USER_CHAT_HISTORY_FILE = "chat_history.json"
USER_LONG_MEMORY_FILE = "long_term_memory.json"
GROUP_CHAT_HISTORY_FILE = "group_chat_history.json" # Pour la m√©moire de groupe
ARCHIVES_DIR = "archives" # Sous-r√©pertoire pour l'archivage des pages web

# Taille maximale des fichiers pour la rotation/compression des logs et l'archivage
MAX_FILE_SIZE = 5 * 1024 * 1024  # 5 MB

# Param√®tres de m√©moire et de cache
MAX_CACHE_SIZE = 20       # Nombre de messages r√©cents √† garder en cache pour la similarit√©
MAX_LONG_TERM_MEMORY = 50 # Nombre d'entr√©es max dans la m√©moire √† long terme

# Assurez-vous que les r√©pertoires n√©cessaires existent
BASE_DIR.mkdir(parents=True, exist_ok=True)
DAILY_CHALLENGE_PATH.mkdir(exist_ok=True)
HISTORY_DIR.mkdir(exist_ok=True)

# ==== Telegram Bot Configuration ====
# Token de votre bot Telegram (√† remplacer par votre vrai token en production)
TELEGRAM_BOT_TOKEN = "7902342551:AAG6r1QA2GTMZcmcsWHi36Ivd_PVeMXULOs"
# ID du groupe priv√© utilis√© pour les logs, rapports et archivage
PRIVATE_GROUP_ID = -1002845235344 

# ==== Configuration du Bot ====
BOT_NAME = "Assistant IA"
BOT_DESCRIPTION = "un assistant polyvalent capable de converser, d'ex√©cuter du code, d'analyser des images et d'archiver des informations."
BOT_PERSONALITY = "toujours serviable, pr√©cis, √©thique et proactif dans l'apprentissage."
BOT_INSTRUCTIONS = "R√©ponds aux questions, ex√©cute les commandes, et utilise tes outils pour fournir les meilleures informations. Sois concis mais complet."

# ==== Cl√©s API Individuelles (centralis√©es pour la clart√©) ====
# R√©cup√©rer les cl√©s API depuis les variables d'environnement pour la production
# ou les d√©finir ici pour le d√©veloppement local (moins s√©curis√©)
APIFLASH_KEY = os.getenv("APIFLASH_KEY", "3a3cc886a18e41109e0cebc0745b12de")
DEEPSEEK_KEY_1 = os.getenv("DEEPSEEK_KEY_1", "sk-ef08317d125947b3a1ce5916592bef00")
DEEPSEEK_KEY_2 = os.getenv("DEEPSEEK_KEY_2", "sk-d73750d96142421cb1098c7056dd7f01")
CRAWLBASE_KEY_1 = os.getenv("CRAWLBASE_KEY_1", "x41P6KNU8J86yF9JV1nqSw")
CRAWLBASE_KEY_2 = os.getenv("CRAWLBASE_KEY_2", "FOg3R0v_aLxzHkYIdjPgVg")
DETECTLANGUAGE_KEY = os.getenv("DETECTLANGUAGE_KEY", "ebdc8ccc2ee75eda3ab122b08ffb1e8d")
GUARDIAN_KEY = os.getenv("GUARDIAN_KEY", "07c622c1-af05-4c24-9f37-37d219be76a0")
IP2LOCATION_KEY = os.getenv("IP2LOCATION_KEY", "11103C239EA8EA6DF2473BB445EC32F2")
SERPER_KEY = os.getenv("SERPER_KEY", "047b30db1df999aaa9c293f2048037d40c651439")
SHODAN_KEY = os.getenv("SHODAN_KEY", "umdSaWOfVq9Wt2F4wWdXiKh1zjLailzn")
TAVILY_KEY_1 = os.getenv("TAVILY_KEY_1", "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")
TAVILY_KEY_2 = os.getenv("TAVILY_KEY_2", "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")
TAVILY_KEY_3 = os.getenv("TAVILY_KEY_3", "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")
TAVILY_KEY_4 = os.getenv("TAVILY_KEY_4", "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")
WEATHERAPI_KEY = os.getenv("WEATHERAPI_KEY", "332bcdba457d4db4836175513250407")
WOLFRAM_APP_ID_1 = os.getenv("WOLFRAM_APP_ID_1", "96LX77-G8PGKJ3T7V")
WOLFRAM_APP_ID_2 = os.getenv("WOLFRAM_APP_ID_2", "96LX77-PYHRRET363")
WOLFRAM_APP_ID_3 = os.getenv("WOLFRAM_APP_ID_3", "96LX77-P9HPAYWRGL")
GREYNOISE_KEY = os.getenv("GREYNOISE_KEY", "5zNe9E6c2UNDhU09iVXbMaB04UpHAw5hNm5rHCK24fCLvI2cP33NNOpL7nhkDETG")
LOGINRADIUS_KEY = os.getenv("LOGINRADIUS_KEY", "073b2fbedf82409da2ca6f37b97e8c6a")
JSONBIN_KEY = os.getenv("JSONBIN_KEY", "$2a$10$npWSB7v1YcoqLkyPpz0PZOV5ES5vBs6JtTWVyVDXK3j3FDYYS5BPO")
HUGGINGFACE_KEY_1 = os.getenv("HUGGINGFACE_KEY_1", "hf_KzifJEYPZBXSSNcapgb3ISkPJLioDozyPC")
HUGGINGFACE_KEY_2 = os.getenv("HUGGINGFACE_KEY_2", "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")
HUGGINGFACE_KEY_3 = os.getenv("HUGGINGFACE_KEY_3", "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")
HUGGINGFACE_NEW_KEY = os.getenv("HUGGINGFACE_NEW_KEY", "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")
TWILIO_SID = os.getenv("TWILIO_SID", "SK84cc4d335650f9da168cd779f26e00e5")
TWILIO_SECRET = os.getenv("TWILIO_SECRET", "spvz5uwPE8ANYOI5Te4Mehm7YwKOZ4Lg")
ABSTRACTAPI_EMAIL_KEY_1 = os.getenv("ABSTRACTAPI_EMAIL_KEY_1", "2ffd537411ad407e9c9a7eacb7a97311")
ABSTRACTAPI_EMAIL_KEY_2 = os.getenv("ABSTRACTAPI_EMAIL_KEY_2", "5b00ade4e60e4a388bd3e749f4f66e28")
ABSTRACTAPI_EMAIL_KEY_3 = os.getenv("ABSTRACTAPI_EMAIL_KEY_3", "f4106df7b93e4db6855cb7949edc4a20")
ABSTRACTAPI_GENERIC_KEY = os.getenv("ABSTRACTAPI_GENERIC_KEY", "020a4dcd3e854ac0b19043491d79df92")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyABnzGG2YoTNY0uep-akgX1rfuvAsp049Q") # Cl√© pour GeminiApiClient
GOOGLE_API_KEYS = [
    os.getenv("GOOGLE_API_KEY_1", "AIzaSyAk6Ph25xuIY3b5o-JgdL652MvK4usp8Ms"),
    os.getenv("GOOGLE_API_KEY_2", "AIzaSyDuccmfiPSk4042NeJCYIjA8EOXPo1YKXU"),
    os.getenv("GOOGLE_API_KEY_3", "AIzaSyAQq6o9voefaDxkAEORf7W-IB3QbotIkwY"),
    os.getenv("GOOGLE_API_KEY_4", "AIzaSyDYaYrQQ7cwYFm8TBpyGM3dJweOGOYl7qw"),
]
GOOGLE_CX_LIST = [
    "3368510e864b74936",
    "e745c9ca0ffb94659"
]
PULSEDIVE_KEY = os.getenv("PULSEDIVE_KEY", "201bb09342f35d365889d7d0ca0fdf8580ebee0f1e7644ce70c99a46c1d47171")
RANDOMMER_KEY = os.getenv("RANDOMMER_KEY", "29d907df567b4226bf64b924f9e26c00")
STORMGLASS_KEY = os.getenv("STORMGLASS_KEY", "7ad5b888-5900-11f0-80b9-0242ac130006-7ad5b996-5900-11f0-80b9-0242ac130006")
TOMORROW_KEY = os.getenv("TOMORROW_KEY", "bNh6KpmddRGY0dzwvmQugVtG4Uf5Y2w1")
CLOUDMERSIVE_KEY = os.getenv("CLOUDMERSIVE_KEY", "4d407015-ce22-45d7-a2e1-b88ab6380084")
OPENWEATHER_API_KEY = os.getenv("OPENWEATHER_API_KEY", "c80075b7332716a418e47033463085ef")
MOCKAROO_KEY = os.getenv("MOCKAROO_KEY", "282b32d0")
OPENPAGERANK_KEY = os.getenv("OPENPAGERANK_KEY", "w848ws8s0848g4koosgooc0sg4ggogcggw4o4cko")
RAPIDAPI_KEY = os.getenv("RAPIDAPI_KEY", "d4d1f58d8emsh58d888c711b7400p1bcebejsn2cc04dce6efe")
OCR_API_KEY = os.getenv("OCR_API_KEY", "K82679097388957") # Cl√© pour OCRApiClient (une seule cl√© pour la classe d√©di√©e)
OCR_API_KEYS = [ # Cl√©s OCR pour les endpoints multiples si utilis√©s par APIClient g√©n√©rique
    os.getenv("OCR_API_KEY_1", "K82679097388957"),
    os.getenv("OCR_API_KEY_2", "K81079143888957"),
    os.getenv("OCR_API_KEY_3", "K84281517488957")
]

# ==== Configuration unifi√©e des APIs et Endpoints ====
# Cette configuration est utilis√©e par EndpointHealthManager et APIClient
API_CONFIG = {
    "APIFLASH": [
        {"key": APIFLASH_KEY, "endpoint_name": "URL to Image", "url": "https://api.apiflash.com/v1/urltoimage", "method": "GET", "key_field": "access_key", "key_location": "param", "health_check_params": {"url": "https://example.com"}, "timeout": 10}
    ],
    "DEEPSEEK": [
        {"key": DEEPSEEK_KEY_1, "endpoint_name": "Models List (Key 1)", "url": "https://api.deepseek.com/v1/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 5},
        {"key": DEEPSEEK_KEY_2, "endpoint_name": "Models List (Key 2)", "url": "https://api.deepseek.com/v1/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 5},
        {"key": DEEPSEEK_KEY_1, "endpoint_name": "Chat Completions", "url": "https://api.deepseek.com/v1/chat/completions", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"model": "deepseek-chat", "stream": False}, "health_check_json": {"model": "deepseek-chat", "messages": [{"role": "user", "content": "hello"}]}, "timeout": 30}
    ],
    "CRAWLBASE": [
        {"key": CRAWLBASE_KEY_1, "endpoint_name": "HTML Scraping", "url": "https://api.crawlbase.com", "method": "GET", "key_field": "token", "key_location": "param", "health_check_params": {"url": "https://example.com"}, "timeout": 15},
        {"key": CRAWLBASE_KEY_2, "endpoint_name": "JS Scraping (JavaScript Token)", "url": "https://api.crawlbase.com", "method": "GET", "key_field": "token", "key_location": "param", "fixed_params": {"javascript": "true"}, "health_check_params": {"url": "https://example.com", "javascript": "true"}, "timeout": 20}
    ],
    "DETECTLANGUAGE": [
        {"key": DETECTLANGUAGE_KEY, "endpoint_name": "Language Detection", "url": "https://ws.detectlanguage.com/0.2/detect", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "health_check_json": {"q": "hello"}, "timeout": 5}
    ],
    "GUARDIAN": [
        {"key": GUARDIAN_KEY, "endpoint_name": "News Search", "url": "https://content.guardianapis.com/search", "method": "GET", "key_field": "api-key", "key_location": "param", "fixed_params": {"show-fields": "headline,trailText"}, "health_check_params": {"q": "test"}, "timeout": 10},
        {"key": GUARDIAN_KEY, "endpoint_name": "Sections", "url": "https://content.guardianapis.com/sections", "method": "GET", "key_field": "api-key", "key_location": "param", "health_check_params": {"q": "news"}, "timeout": 5}
    ],
    "IP2LOCATION": [
        {"key": IP2LOCATION_KEY, "endpoint_name": "IP Geolocation", "url": "https://api.ip2location.io/", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"ip": "8.8.8.8"}, "timeout": 5}
    ],
    "SERPER": [
        {"key": SERPER_KEY, "endpoint_name": "Search", "url": "https://google.serper.dev/search", "method": "POST", "key_field": "X-API-KEY", "key_location": "header", "health_check_json": {"q": "test"}, "timeout": 10},
        {"key": SERPER_KEY, "endpoint_name": "Images Search", "url": "https://google.serper.dev/images", "method": "POST", "key_field": "X-API-KEY", "key_location": "header", "health_check_json": {"q": "test"}, "timeout": 10}
    ],
    "SHODAN": [
        {"key": SHODAN_KEY, "endpoint_name": "Host Info", "url": "https://api.shodan.io/shodan/host/8.8.8.8", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"ip": "8.8.8.8"}, "timeout": 10},
        {"key": SHODAN_KEY, "endpoint_name": "API Info", "url": "https://api.shodan.io/api-info", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 5}
    ],
    "TAVILY": [
        {"key": TAVILY_KEY_1, "endpoint_name": "Search (Key 1)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15},
        {"key": TAVILY_KEY_2, "endpoint_name": "Search (Key 2)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15},
        {"key": TAVILY_KEY_3, "endpoint_name": "Search (Key 3)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15},
        {"key": TAVILY_KEY_4, "endpoint_name": "Search (Key 4)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15}
    ],
    "WEATHERAPI": [
        {"key": WEATHERAPI_KEY, "endpoint_name": "Current Weather", "url": "http://api.weatherapi.com/v1/current.json", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"q": "London"}, "timeout": 5},
        {"key": WEATHERAPI_KEY, "endpoint_name": "Forecast", "url": "http://api.weatherapi.com/v1/forecast.json", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"days": 1}, "health_check_params": {"q": "London", "days": 1}, "timeout": 5}
    ],
    "WOLFRAMALPHA": [
        {"key": WOLFRAM_APP_ID_1, "endpoint_name": "Query (AppID 1)", "url": "http://api.wolframalpha.com/v2/query", "method": "GET", "key_field": "appid", "key_location": "param", "fixed_params": {"format": "plaintext", "output": "json"}, "health_check_params": {"input": "2+2"}, "timeout": 10},
        {"key": WOLFRAM_APP_ID_2, "endpoint_name": "Query (AppID 2)", "url": "http://api.wolframalpha.com/v2/query", "method": "GET", "key_field": "appid", "key_location": "param", "fixed_params": {"format": "plaintext", "output": "json"}, "health_check_params": {"input": "2+2"}, "timeout": 10},
        {"key": WOLFRAM_APP_ID_3, "endpoint_name": "Query (AppID 3)", "url": "http://api.wolframalpha.com/v2/query", "method": "GET", "key_field": "appid", "key_location": "param", "fixed_params": {"format": "plaintext", "output": "json"}, "health_check_params": {"input": "2+2"}, "timeout": 10}
    ],
    "CLOUDMERSIVE": [
        {"key": CLOUDMERSIVE_KEY, "endpoint_name": "Domain Check", "url": "https://api.cloudmersive.com/validate/domain/check", "method": "POST", "key_field": "Apikey", "key_location": "header", "health_check_json": {"domain": "example.com"}, "timeout": 10}
    ],
    "GREYNOISE": [
        {"key": GREYNOISE_KEY, "endpoint_name": "IP Analysis", "url": "https://api.greynoise.io/v3/community/", "method": "GET", "key_field": "key", "key_location": "header", "health_check_url_suffix": "1.1.1.1", "timeout": 10}
    ],
    "PULSEDIVE": [
        {"key": PULSEDIVE_KEY, "endpoint_name": "API Info", "url": "https://pulsedive.com/api/info.php", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"key": PULSEDIVE_KEY}, "timeout": 5},
        {"key": PULSEDIVE_KEY, "endpoint_name": "Analyze IP", "url": "https://pulsedive.com/api/v1/analyze", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"indicator": "8.8.8.8", "type": "ip"}, "timeout": 10},
        {"key": PULSEDIVE_KEY, "endpoint_name": "Explore", "url": "https://pulsedive.com/api/v1/explore", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"query": "type='ip'"}, "timeout": 10}
    ],
    "STORMGLASS": [
        {"key": STORMGLASS_KEY, "endpoint_name": "Weather Point", "url": "https://api.stormglass.io/v2/weather/point", "method": "GET", "key_field": "Authorization", "key_location": "header", "health_check_params": {"lat": 0, "lng": 0, "params": "airTemperature", "start": 0, "end": 0}, "timeout": 10}
    ],
    "LOGINRADIUS": [
        {"key": LOGINRADIUS_KEY, "endpoint_name": "Ping", "url": "https://api.loginradius.com/identity/v2/auth/ping", "method": "GET", "timeout": 5}
    ],
    "JSONBIN": [
        {"key": JSONBIN_KEY, "endpoint_name": "Bin Access", "url": "https://api.jsonbin.io/v3/b", "method": "GET", "key_field": "X-Master-Key", "key_location": "header", "health_check_url_suffix": "60c7b0e0f8c2a3b4c5d6e7f0", "timeout": 10},
        {"key": JSONBIN_KEY, "endpoint_name": "Bin Create", "url": "https://api.jsonbin.io/v3/b", "method": "POST", "key_field": "X-Master-Key", "key_location": "header", "health_check_json": {"record": {"test": "health"}}, "timeout": 10}
    ],
    "HUGGINGFACE": [
        {"key": HUGGINGFACE_KEY_1, "endpoint_name": "Models List (Key 1)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_KEY_1, "endpoint_name": "BERT Inference", "url": "https://api-inference.huggingface.co/models/bert-base-uncased", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "health_check_json": {"inputs": "test"}, "timeout": 30},
        {"key": HUGGINGFACE_KEY_2, "endpoint_name": "Models List (Key 2)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_KEY_3, "endpoint_name": "Models List (Key 3)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_NEW_KEY, "endpoint_name": "Models List (New Key)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_NEW_KEY, "endpoint_name": "BERT Inference (New Key)", "url": "https://api-inference.huggingface.co/models/bert-base-uncased", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "health_check_json": {"inputs": "test"}, "timeout": 30}
    ],
    "TWILIO": [
        {"key": (TWILIO_SID, TWILIO_SECRET), "endpoint_name": "Accounts", "url": "https://api.twilio.com/2010-04-01/Accounts", "method": "GET", "key_location": "auth_basic", "timeout": 10},
        {"key": (TWILIO_SID, TWILIO_SECRET), "endpoint_name": "Account Balance", "url": f"https://api.twilio.com/2010-04-01/Accounts/{TWILIO_SID}/Balance.json", "method": "GET", "key_location": "auth_basic", "timeout": 10}
    ],
    "ABSTRACTAPI": [
        {"key": ABSTRACTAPI_EMAIL_KEY_1, "endpoint_name": "Email Validation (Key 1)", "url": "https://emailvalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"email": "test@example.com"}, "timeout": 10},
        {"key": ABSTRACTAPI_EMAIL_KEY_2, "endpoint_name": "Email Validation (Key 2)", "url": "https://emailvalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"email": "test@example.com"}, "timeout": 10},
        {"key": ABSTRACTAPI_EMAIL_KEY_3, "endpoint_name": "Email Validation (Key 3)", "url": "https://emailvalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"email": "test@example.com"}, "timeout": 10},
        {"key": ABSTRACTAPI_GENERIC_KEY, "endpoint_name": "Exchange Rates", "url": "https://exchange-rates.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"base": "USD"}, "timeout": 10},
        {"key": ABSTRACTAPI_GENERIC_KEY, "endpoint_name": "Holidays", "url": "https://holidays.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "fixed_params": {"country": "US", "year": datetime.now().year}, "health_check_params": {"country": "US", "year": datetime.now().year}, "timeout": 10},
        {"key": ABSTRACTAPI_GENERIC_KEY, "endpoint_name": "Phone Validation", "url": "https://phonevalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"phone": "1234567890"}, "timeout": 10}
    ],
    "GEMINI_API": [ # Note: This is for the generic APIClient, GeminiApiClient class uses GEMINI_API_KEY directly
        {"key": GEMINI_API_KEY, "endpoint_name": "Generic Models Endpoint", "url": "https://generativelanguage.googleapis.com/v1beta/models", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": GEMINI_API_KEY, "endpoint_name": "Embed Content", "url": "https://generativelanguage.googleapis.com/v1beta/models/embedding-001:embedContent", "method": "POST", "key_field": "key", "key_location": "param", "health_check_json": {"content": {"parts": [{"text": "test"}]}}, "timeout": 30},
        {"key": GEMINI_API_KEY, "endpoint_name": "Generate Content", "url": "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent", "method": "POST", "key_field": "key", "key_location": "param", "health_check_json": {"contents": [{"parts": [{"text": "hello"}]}]}, "timeout": 60}
    ],
    "GOOGLE_CUSTOM_SEARCH": [
        {"key": GOOGLE_API_KEYS[i], "endpoint_name": f"Search (Key {i+1}, CX {j+1})", "url": "https://www.googleapis.com/customsearch/v1", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"cx": GOOGLE_CX_LIST[j]}, "health_check_params": {"q": "test"}, "timeout": 10}
        for i in range(len(GOOGLE_API_KEYS)) for j in range(len(GOOGLE_CX_LIST))
    ],
    "RANDOMMER": [
        {"key": RANDOMMER_KEY, "endpoint_name": "Generate Phone", "url": "https://randommer.io/api/Phone/Generate", "method": "GET", "key_field": "X-Api-Key", "key_location": "header", "fixed_params": {"CountryCode": "US", "Quantity": 1}, "health_check_params": {"CountryCode": "US", "Quantity": 1}, "timeout": 10}
    ],
    "TOMORROW.IO": [
        {"key": TOMORROW_KEY, "endpoint_name": "Timelines", "url": "https://api.tomorrow.io/v4/timelines", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"location": "London", "fields": ["temperature"], "units": "metric", "timesteps": ["1h"]}, "timeout": 15}
    ],
    "OPENWEATHERMAP": [
        {"key": OPENWEATHER_API_KEY, "endpoint_name": "Current Weather", "url": "https://api.openweathermap.org/data/2.5/weather", "method": "GET", "key_field": "appid", "key_location": "param", "health_check_params": {"q": "London"}, "timeout": 5}
    ],
    "MOCKAROO": [
        {"key": MOCKAROO_KEY, "endpoint_name": "Data Generation", "url": "https://api.mockaroo.com/api/generate.json", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "health_check_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Types", "url": "https://api.mockaroo.com/api/types", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Schemas", "url": "https://api.mockaroo.com/api/schemas", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Account", "url": "https://api.mockaroo.com/api/account", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Generate CSV", "url": "https://api.mockaroo.com/api/generate.csv", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "health_check_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Status", "url": "https://api.mockaroo.com/api/status", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10}
    ],
    "OPENPAGERANK": [
        {"key": OPENPAGERANK_KEY, "endpoint_name": "Domain Rank", "url": "https://openpagerank.com/api/v1.0/getPageRank", "method": "GET", "key_field": "API-OPR", "key_location": "header", "fixed_params": {"domains[]": "google.com"}, "timeout": 10}
    ],
    "RAPIDAPI": [
        {"key": RAPIDAPI_KEY, "endpoint_name": "Programming Joke", "url": "https://jokeapi-v2.p.rapidapi.com/joke/Programming", "method": "GET", "key_field": "X-RapidAPI-Key", "key_location": "header", "fixed_headers": {"X-RapidAPI-Host": "jokeapi-v2.p.rapidapi.com"}, "timeout": 10},
        {"key": RAPIDAPI_KEY, "endpoint_name": "Currency List Quotes", "url": "https://currency-exchange.p.rapidapi.com/listquotes", "method": "GET", "key_field": "X-RapidAPI-Key", "key_location": "header", "fixed_headers": {"X-RapidAPI-Host": "currency-exchange.p.rapidapi.com"}, "timeout": 10},
        {"key": RAPIDAPI_KEY, "endpoint_name": "Random Fact", "url": "https://random-facts2.p.rapidapi.com/getfact", "method": "GET", "key_field": "X-RapidAPI-Key", "key_location": "header", "fixed_headers": {"X-RapidAPI-Host": "random-facts2.p.rapidapi.com"}, "timeout": 10}
    ],
    "OCR_API": [ # Note: This is for the generic APIClient, OCRApiClient class uses OCR_API_KEY directly
        {"key": OCR_API_KEYS[0], "endpoint_name": "OCR Space (Key 1)", "url": "https://api.ocr.space/parse/image", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"base64Image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="}, "timeout": 30},
        {"key": OCR_API_KEYS[1], "endpoint_name": "OCR Space (Key 2)", "url": "https://api.ocr.space/parse/image", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"base64Image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="}, "timeout": 30},
        {"key": OCR_API_KEYS[2], "endpoint_name": "OCR Space (Key 3)", "url": "https://api.ocr.space/parse/image", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"base64Image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="}, "timeout": 30},
    ]
}

# ==== Quotas API (D√©finitions des limites pour le QuotaManager) ====
# Ces valeurs sont utilis√©es pour le suivi et la gestion des quotas d'utilisation.
# Mettre None pour indiquer une limite illimit√©e.
API_QUOTAS = {
    "gemini": {
        "monthly": 1000000, # Exemple: 1 million de tokens par mois
        "daily": 50000,    # Exemple: 50 000 tokens par jour
        "hourly": 5000,    # Exemple: 5 000 tokens par heure
        "rate_limit_per_sec": 5 # Exemple: 5 requ√™tes par seconde
    },
    "ocr_space": { # Nom interne utilis√© par OCRApiClient
        "monthly": 25000,  # Exemple: 25 000 requ√™tes par mois (free tier)
        "daily": None,
        "hourly": None,
        "rate_limit_per_sec": 1 # Exemple: 1 requ√™te par seconde
    },
    # Ajouter les quotas pour toutes les APIs list√©es dans API_CONFIG si elles ont des limites
    # Utiliser les valeurs du premier snippet si non sp√©cifi√©es ici
    "APIFLASH": {"monthly": 100, "daily": 3, "hourly": 3},
    "DEEPSEEK": {"monthly": None, "hourly": 50},
    "CRAWLBASE": {"monthly": 1000, "daily": 33, "hourly": 1},
    "DETECTLANGUAGE": {"daily": 1000, "hourly": 41},
    "GUARDIAN": {"daily": 5000, "rate_limit_per_sec": 12},
    "IP2LOCATION": {"monthly": 50, "daily": 2, "hourly": 2},
    "SERPER": {"monthly": 2500, "daily": 83, "hourly": 3},
    "SHODAN": {"monthly": 100, "daily": 3, "hourly": 3},
    "TAVILY": {"monthly": 1000, "daily": 33, "hourly": 1},
    "WEATHERAPI": {"monthly": None},
    "WOLFRAMALPHA": {"monthly": None, "hourly": 67},
    "CLOUDMERSIVE": {"monthly": 25, "daily": 1, "hourly": 1},
    "GREYNOISE": {"monthly": 100, "daily": 3, "hourly": 3},
    "PULSEDIVE": {"monthly": 50, "daily": 2, "hourly": 2},
    "STORMGLASS": {"monthly": None},
    "LOGINRADIUS": {"monthly": 25000, "daily": 833, "hourly": 34},
    "JSONBIN": {"monthly": 10000, "daily": 333, "hourly": 13},
    "HUGGINGFACE": {"hourly": 100},
    "TWILIO": {"monthly": 15},
    "ABSTRACTAPI": {"monthly": 250, "rate_limit_per_sec": 1, "daily": 8, "hourly": 1},
    "MOCKAROO": {"monthly": 200, "daily": 7, "hourly": 1},
    "OPENPAGERANK": {"monthly": 1000, "daily": 33, "hourly": 1},
    "RAPIDAPI": {"monthly": None, "hourly": 30},
    "GOOGLE_CUSTOM_SEARCH": {"daily": 100, "hourly": 4},
    "RANDOMMER": {"monthly": 1000, "daily": 100, "hourly": 4},
    "TOMORROW.IO": {"monthly": None},
    "OPENWEATHERMAP": {"monthly": 1000000, "daily": 100, "hourly": 4},
    # "OCR_API" est d√©j√† g√©r√© par "ocr_space" pour la classe d√©di√©e.
    # Si d'autres clients OCR sont ajout√©s via APIClient, ils devraient √™tre list√©s ici.
}


# Mod√®le Gemini et ses param√®tres
GEMINI_MODEL_NAME = "gemini-1.5-flash-latest" # Ou "gemini-pro"
GEMINI_TEMPERATURE = 0.7
GEMINI_TOP_P = 0.95
GEMINI_TOP_K = 40
GEMINI_MAX_OUTPUT_TOKENS = 2048
GEMINI_SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]


# ==== Bot Behavior Configuration ====
API_COOLDOWN_DURATION_SECONDS = 300 # Dur√©e du cooldown en secondes (5 minutes)
API_ROTATION_INTERVAL_MINUTES = 10 # Intervalle de rotation des APIs en minutes

# Quota Burning Configuration
# Ratio de quota restant en dessous duquel le mode "br√ªlage" s'active (ex: 0.2 signifie 20% ou moins restant)
BURN_QUOTA_THRESHOLD_RATIO = 0.2
# Fen√™tre de temps avant la r√©initialisation du quota o√π le "br√ªlage" peut s'activer (en heures)
BURN_QUOTA_BEFORE_RESET_HOURS = 6

# Prompts pour l'auto-g√©n√©ration de contenu technique afin de "br√ªler" le quota.
# Ces prompts sont choisis al√©atoirement pour les APIs en mode "burn".
AUTO_BURN_PROMPTS = {
    "gemini": [
        "G√©n√®re un script Python qui utilise une API REST pour r√©cup√©rer des donn√©es et les stocker dans une base de donn√©es NoSQL.",
        "Explique en d√©tail les principes de l'architecture microservices et comment ils se comparent aux architectures monolithiques.",
        "D√©cris les √©tapes pour d√©ployer une application web Flask sur un serveur cloud (AWS EC2 ou Google Cloud Run).",
        "√âcris un tutoriel sur l'utilisation de Docker Compose pour orchestrer plusieurs conteneurs (par exemple, une application web et une base de donn√©es).",
        "Analyse les avantages et les inconv√©nients des bases de donn√©es relationnelles vs non-relationnelles pour un projet de grande envergure.",
        "Propose un plan de test complet pour une application web critique, incluant tests unitaires, d'int√©gration, de bout en bout et de performance.",
        "G√©n√®re un exemple de code JavaScript pour une application React qui g√®re l'√©tat avec Redux ou Context API.",
        "Explique le concept de CI/CD (int√©gration et livraison continues) et son importance dans le d√©veloppement logiciel moderne.",
        "D√©cris les meilleures pratiques de s√©curit√© pour une API RESTful, incluant l'authentification, l'autorisation et la protection contre les attaques courantes.",
        "√âcris un algorithme de tri efficace (par exemple, Quicksort ou Mergesort) et explique sa complexit√© temporelle et spatiale."
    ],
    "ocr_space": [
        "D√©cris les d√©fis techniques de l'OCR sur des documents manuscrits et les approches modernes pour les surmonter.",
        "Explique comment l'OCR peut √™tre utilis√©e dans le domaine de la gestion documentaire ou de l'automatisation des processus m√©tier.",
        "Quelles sont les m√©triques d'√©valuation courantes pour les performances d'un syst√®me OCR ?",
        "Comment la pr√©-traitement d'image (bruit, binarisation, redressement) affecte-t-il la pr√©cision de l'OCR ?",
        "Compare les diff√©rentes technologies OCR disponibles sur le march√© (cloud vs on-premise, open-source vs propri√©taires)."
    ]
}

# ==== Param√®tres de S√©curit√© et Filtrage ====
FORBIDDEN_WORDS = ["fuck", "shit", "bitch", "asshole", "pute", "encul√©", "haine", "stupide", "d√©truire", "conflit", "malveillance", "idiot", "nul", "d√©bile"]

# ==== IA PROMPTS (Exemples, √† affiner selon tes besoins sp√©cifiques pour chaque IA) ====
GENERAL_IA_PROMPT = """
Tu es une IA de l'ann√©e 2025, experte en information, programmation et r√©solution de probl√®mes.
Ton objectif est de fournir des r√©ponses compl√®tes, pr√©cises et √† jour, bas√©es sur les informations que tu as acc√®s (m√©moire collective, outils API).
Tu dois TOUJOURS relire l'historique de discussion et la m√©moire collective pour √©viter les doublons et apporter des am√©liorations.
√âvite les informations obsol√®tes et concentre-toi sur une perspective de 2025.
Si tu dois ex√©cuter du code, propose-le clairement et demande si l'ex√©cution en sandbox est d√©sir√©e.
N'h√©site pas √† croiser les informations de plusieurs sources.
"""

CODING_CHALLENGE_PROMPT = """
En tant qu'IA de d√©veloppement de 2025, ton r√¥le est d'am√©liorer et de tester des morceaux de code Python/Shell.
Tu as acc√®s √† une sandbox s√©curis√©e pour ex√©cuter le code.
Tes r√©ponses doivent inclure le code corrig√© ou am√©lior√©, et les r√©sultats de l'ex√©cution en sandbox.
Apporte des am√©liorations significatives, ne te contente pas de corrections triviales si la question implique un projet plus large.
Pense √† l'efficacit√© du code et √† l'optimisation des ressources.
Chaque version doit √™tre une am√©lioration nette de la pr√©c√©dente, in√©dite.
Commence par un commentaire indiquant ce qui a √©t√© am√©lior√©.
Le code doit √™tre direct, lisible, et pr√™t √† √™tre utilis√©.
"""

# ==== Tool Reformulation Configuration ====
TOOL_RETRY_MAX_ATTEMPTS = 3

import os
import json
import gzip
import shutil
import hashlib
import difflib
import re
import logging
import io
import contextlib
import fcntl
import traceback
import asyncio
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Union, List, Dict

# Import des constantes depuis config.py
from config import BASE_DIR, MAX_FILE_SIZE, ERROR_LOG_PATH, MAX_CACHE_SIZE, FORBIDDEN_WORDS

# Configure logging pour tout le script
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Global lock for file operations to ensure atomic writes and prevent race conditions
_file_lock = None 

def set_file_lock(lock_instance: asyncio.Lock):
    """
    Permet d'injecter l'instance d'asyncio.Lock apr√®s l'initialisation de l'event loop.
    Ceci est crucial pour la gestion des acc√®s concurrents aux fichiers.
    """
    global _file_lock
    _file_lock = lock_instance

def _acquire_file_lock_sync(f):
    """
    Acquires an exclusive lock on a file using fcntl (Unix-like systems).
    This prevents other processes from writing to the file simultaneously.
    """
    try:
        if os.name == 'posix' and fcntl:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
    except Exception as e:
        logging.warning(f"Could not acquire file lock: {e}")

def _release_file_lock_sync(f):
    """
    Releases an exclusive lock on a file using fcntl (Unix-like systems).
    """
    try:
        if os.name == 'posix' and fcntl:
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
    except Exception as e:
        logging.warning(f"Could not release file lock: {e}")

def get_user_dir(uid: Union[int, str]) -> Path:
    """
    Retourne le r√©pertoire de sauvegarde sp√©cifique √† un utilisateur, le cr√©ant si n√©cessaire.
    Chaque utilisateur (ou groupe priv√©) a son propre r√©pertoire pour stocker les donn√©es.
    """
    p = BASE_DIR / str(uid)
    p.mkdir(parents=True, exist_ok=True)
    return p

def rotate_log_if_needed(path: Path):
    """
    Fait pivoter le fichier log (ou tout fichier de donn√©es) si sa taille d√©passe MAX_FILE_SIZE.
    Un nouveau fichier est cr√©√© avec un horodatage pour l'archivage, et le fichier original est r√©initialis√©.
    """
    if path.exists() and path.stat().st_size > MAX_FILE_SIZE:
        timestamp = int(datetime.now().timestamp())
        # Renomme le fichier existant pour l'archiver
        new_path = path.with_suffix(f".old_{timestamp}{path.suffix}.gz")
        try:
            # Compresse et d√©place l'ancien fichier
            with open(path, "rb") as f_in, gzip.open(new_path, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out)
            path.unlink() # Supprime l'original
            logging.info(f"Log rotated and compressed: {path} -> {new_path}")
        except Exception as e:
            log_message(f"[Rotation log] Erreur lors de la compression/rotation de {path}: {e}\n{traceback.format_exc()}", level="error")
        # Le fichier sera recr√©√© vide lors de la prochaine sauvegarde

def compress_if_large(path: Path):
    """
    Compresse le fichier s'il d√©passe 1MB apr√®s une √©criture, puis le renomme pour garder le nom original.
    Ceci est une mesure de gestion de l'espace disque pour les fichiers de donn√©es.
    """
    try:
        # V√©rifie si le fichier existe et si sa taille est sup√©rieure √† 1MB
        if path.exists() and path.stat().st_size > 1_000_000:
            gz_path = path.with_suffix(path.suffix + ".gz") # Chemin temporaire pour le fichier compress√©
            with open(path, "rb") as f_in, gzip.open(gz_path, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out) # Copie et compresse
            path.unlink() # Supprime le fichier original non compress√©
            # Renomme le fichier .gz pour qu'il ait le nom original, simulant une compression "in-place"
            gz_path.rename(path) 
            logging.info(f"File compressed: {path}")
    except Exception as e:
        log_message(f"[Compression auto] Erreur : {e}\n{traceback.format_exc()}", level="error")

async def load_json(filepath: Path, default_value: Union[Dict, List] = None) -> Union[Dict, List]:
    """
    Charge un fichier JSON de mani√®re asynchrone.
    Retourne `default_value` si le fichier n'existe pas, est vide ou est corrompu.
    Utilise un verrou global pour la s√©curit√© des acc√®s concurrents.
    """
    if default_value is None:
        default_value = {} # Default to empty dict if not specified

    if _file_lock:
        async with _file_lock:
            return _load_json_sync(filepath, default_value)
    else:
        # Fallback for synchronous loading if lock is not set (e.g., during early initialization)
        return _load_json_sync(filepath, default_value)

def _load_json_sync(filepath: Path, default_value: Union[Dict, List]) -> Union[Dict, List]:
    """
    Charge un fichier JSON de mani√®re synchrone avec verrouillage de fichier.
    """
    if not filepath.exists():
        logging.info(f"Fichier non trouv√©: {filepath}. Cr√©ation d'un fichier vide.")
        _save_json_sync(filepath, default_value) # Cr√©e un fichier vide avec la valeur par d√©faut
        return default_value
    
    # Ouvre le fichier en mode lecture/√©criture pour pouvoir le vider en cas de corruption
    with open(filepath, 'r+', encoding='utf-8') as f:
        _acquire_file_lock_sync(f) # Acquiert le verrou avant de lire
        try:
            f.seek(0) # Se positionne au d√©but du fichier
            content = f.read()
            if not content:
                logging.warning(f"Fichier vide: {filepath}. Retourne la valeur par d√©faut.")
                return default_value
            return json.loads(content)
        except json.JSONDecodeError as e:
            logging.error(f"Erreur de d√©codage JSON dans {filepath}: {e}. Le fichier sera r√©initialis√©.")
            # Si le fichier est corrompu, le r√©initialise avec la valeur par d√©faut
            f.seek(0)
            f.truncate()
            json.dump(default_value, f, indent=4, ensure_ascii=False)
            return default_value
        except Exception as e:
            logging.error(f"Erreur inattendue lors du chargement de {filepath}: {e}. Retourne la valeur par d√©faut.")
            return default_value
        finally:
            _release_file_lock_sync(f) # Rel√¢che le verrou

async def save_json(filepath: Path, data: Union[Dict, List]):
    """
    Sauvegarde les donn√©es dans un fichier JSON de mani√®re asynchrone et atomique,
    avec rotation et compression si n√©cessaire.
    Utilise un verrou global pour la s√©curit√© des acc√®s concurrents.
    """
    if _file_lock:
        async with _file_lock:
            _save_json_sync(filepath, data)
    else:
        # Fallback for synchronous saving if lock is not set
        _save_json_sync(filepath, data)

def _save_json_sync(filepath: Path, data: Union[Dict, List]):
    """
    Sauvegarde les donn√©es dans un fichier JSON de mani√®re synchrone et atomique,
    avec verrouillage de fichier.
    """
    rotate_log_if_needed(filepath) # Effectue la rotation avant la sauvegarde
    temp_filepath = filepath.with_suffix(filepath.suffix + ".tmp") # Utilise un fichier temporaire pour l'atomicit√©
    try:
        with temp_filepath.open('w', encoding='utf-8') as f:
            _acquire_file_lock_sync(f) # Acquiert le verrou avant d'√©crire
            try:
                json.dump(data, f, indent=4, ensure_ascii=False)
            finally:
                _release_file_lock_sync(f) # Rel√¢che le verrou
        os.replace(temp_filepath, filepath) # Remplace l'ancien fichier par le nouveau de mani√®re atomique
        compress_if_large(filepath) # Compresse le fichier apr√®s la sauvegarde si n√©cessaire
    except Exception as e:
        logging.error(f"Erreur lors de la sauvegarde atomique de {filepath}: {e}")
        if temp_filepath.exists():
            os.remove(temp_filepath) # Nettoie le fichier temporaire en cas d'erreur

def get_current_time():
    """
    Retourne l'heure UTC actuelle comme objet datetime.
    """
    return datetime.now(timezone.utc) # Utilise datetime.now(timezone.utc) pour √™tre explicite sur UTC

def format_datetime(dt_obj: datetime) -> str:
    """
    Formate un objet datetime en cha√Æne de caract√®res lisible (UTC).
    """
    return dt_obj.strftime("%Y-%m-%d %H:%M:%S UTC")

def is_within_time_window(target_time: datetime, start_minutes_before: int, end_minutes_after: int) -> bool:
    """
    V√©rifie si l'heure actuelle est dans une fen√™tre de temps sp√©cifi√©e autour d'une heure cible.
    """
    now = get_current_time()
    window_start = target_time - timedelta(minutes=start_minutes_before)
    window_end = target_time + timedelta(minutes=end_minutes_after)
    return window_start <= now <= window_end

def log_message(message: str, level: str = "info"):
    """
    Log un message avec un niveau sp√©cifi√©.
    Les messages d'erreur critiques sont dirig√©s vers un fichier de log d'erreurs s√©par√©.
    """
    if level == "error":
        error_logger = logging.getLogger("erreurs_api")
        if not error_logger.handlers: # Configurer le logger d'erreurs si pas d√©j√† fait
            eh = logging.FileHandler(ERROR_LOG_PATH, encoding="utf-8")
            eh.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S"))
            error_logger.addHandler(eh)
            error_logger.setLevel(logging.ERROR)
        error_logger.error(message)
    else:
        # Utilise le logger par d√©faut pour les autres niveaux
        if level == "info":
            logging.info(message)
        elif level == "warning":
            logging.warning(message)
        elif level == "debug":
            logging.debug(message)
        else:
            logging.debug(message) # Fallback pour les niveaux non reconnus

def neutralize_urls(text: str) -> str:
    """
    Remplace les URLs dans le texte par un placeholder pour pr√©venir les probl√®mes de lien direct
    et la fuite d'informations sensibles dans les logs ou la m√©moire.
    """
    # Remplace http(s):// par hxxp(s)://
    text = re.sub(r"https?://", lambda m: m.group(0).replace("t", "x", 1), text)
    # Remplace www. par wxx.
    text = re.sub(r"www\.", "wxx.", text)
    # Remplace .com, .net, .org par [.]com, [.]net, [.]org
    text = re.sub(r"\.com", "[.]com", text)
    text = re.sub(r"\.net", "[.]net", text)
    text = re.sub(r"\.org", "[.]org", text)
    return text

def clean_html_tags(text: str) -> str:
    """
    Supprime les balises HTML d'une cha√Æne de caract√®res en utilisant une expression r√©guli√®re.
    """
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

def hash_text(t: str) -> str:
    """
    Calcule le hachage SHA256 d'une cha√Æne de caract√®res.
    """
    return hashlib.sha256(t.encode('utf-8')).hexdigest()

def extract_keywords(text: str) -> List[str]:
    """
    Extrait les mots-cl√©s les plus fr√©quents d'un texte.
    Retourne une liste des 5 mots les plus fr√©quents (de 4 caract√®res ou plus).
    """
    # Trouve tous les mots de 4 caract√®res ou plus (incluant les accents)
    words = re.findall(r'\b[a-zA-Z√©√®√™√¥√†√π√ß√Æ√Ø≈ì]{4,}\b', text.lower())
    freq = {}
    for w in words:
        freq[w] = freq.get(w, 0) + 1
    # Trie les mots par fr√©quence d√©croissante
    keywords = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [w for w,_ in keywords[:5]] # Retourne seulement les mots

def tag_conversation(text: str) -> str:
    """
    G√©n√®re un tag de conversation bas√© sur les mots-cl√©s extraits du texte.
    """
    words = extract_keywords(text)
    return f"#tags : {', '.join(words)}"

def unique_preserve_order(seq: List[Any]) -> List[Any]:
    """
    √âlimine les doublons d'une s√©quence tout en pr√©servant l'ordre original des √©l√©ments.
    """
    seen = set()
    result = []
    for item in seq:
        if item not in seen:
            seen.add(item)
            result.append(item)
    return result

def similar(a: str, b: str) -> float:
    """
    Calcule la similarit√© entre deux cha√Ænes de caract√®res en utilisant le ratio de SequenceMatcher.
    Retourne un ratio de 0 √† 1, o√π 1 indique une identit√© parfaite.
    """
    return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio()

def is_code(text: str) -> bool:
    """
    D√©tecte si le texte ressemble √† du code (Python ou autre) en cherchant des motifs courants.
    """
    return bool(re.search(r"^\s*(def |class |import |print\()", text, re.MULTILINE)) or text.strip().startswith("```")

def is_python_code_block(text: str) -> bool:
    """
    D√©tecte si le texte est un bloc de code Python format√© en Markdown.
    """
    return text.strip().startswith("```python") and text.strip().endswith("```")

import time
import httpx
import json
import base64
import asyncio
import re # Importation ajout√©e pour la validation IP dans ShodanClient
from typing import Dict, Any, Optional, Union, List, Tuple

# Import des constantes et fonctions utilitaires depuis les modules locaux
from config import API_CONFIG, ENDPOINT_HEALTH_FILE, OCR_API_KEYS, GEMINI_API_KEY # Assurez-vous que GEMINI_API_KEY est import√©
from utils import load_json, save_json, get_current_time, format_datetime, log_message, neutralize_urls

class EndpointHealthManager:
    """
    G√®re la sant√© des endpoints API et s√©lectionne le meilleur endpoint disponible
    en fonction de crit√®res comme la latence, le taux de succ√®s et le nombre d'erreurs.
    C'est un singleton pour s'assurer qu'il n'y a qu'une seule instance de gestionnaire de sant√©.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Impl√©mente le patron de conception Singleton."""
        if cls._instance is None:
            cls._instance = super(cls, cls).__new__(cls)
            cls._instance._initialized = False # Indicateur pour l'initialisation asynchrone
        return cls._instance

    def __init__(self):
        """Initialise le gestionnaire. L'initialisation r√©elle se fait via `init_manager`."""
        if self._initialized:
            return
        self.health_status = {}
        # `_initialized` est g√©r√© par `init_manager` pour les op√©rations asynchrones

    async def init_manager(self):
        """
        Initialise le gestionnaire de sant√© de mani√®re asynchrone.
        Charge l'√©tat de sant√© persistant et s'assure que tous les endpoints sont suivis.
        """
        if not self._initialized:
            self.health_status = await load_json(ENDPOINT_HEALTH_FILE, {})
            self._initialize_health_status()
            self._initialized = True
            log_message("Gestionnaire de sant√© des endpoints initialis√©.")

    def _initialize_health_status(self):
        """
        Initialise ou met √† jour le statut de sant√© pour tous les endpoints configur√©s dans `API_CONFIG`.
        Ajoute les nouveaux endpoints et s'assure que toutes les cl√©s n√©cessaires sont pr√©sentes.
        """
        updated = False
        for service_name, endpoints_config in API_CONFIG.items():
            if service_name not in self.health_status:
                self.health_status[service_name] = {}
                updated = True
            for endpoint_config in endpoints_config:
                # Cr√©e une cl√© unique pour chaque endpoint en combinant son nom et sa cl√© API
                endpoint_key = f"{endpoint_config['endpoint_name']}-{str(endpoint_config['key'])}"
                if endpoint_key not in self.health_status[service_name]:
                    self.health_status[service_name][endpoint_key] = {
                        "latency": 0.0,
                        "success_rate": 1.0, # Commence avec un taux de succ√®s parfait (sain)
                        "last_checked": None,
                        "error_count": 0,
                        "total_checks": 0,
                        "is_healthy": True # Pr√©sum√© sain au d√©but
                    }
                    updated = True
        if updated:
            # Sauvegarde l'√©tat mis √† jour de mani√®re asynchrone en t√¢che de fond
            asyncio.create_task(save_json(ENDPOINT_HEALTH_FILE, self.health_status))
            log_message("Statut de sant√© des endpoints initialis√©/mis √† jour.")

    async def run_health_check_for_service(self, service_name: str):
        """
        Ex√©cute des checks de sant√© pour tous les endpoints d'un service donn√©.
        Tente d'appeler l'endpoint avec des param√®tres de sant√© pr√©d√©finis.
        """
        endpoints_config = API_CONFIG.get(service_name)
        if not endpoints_config:
            log_message(f"Aucune configuration d'endpoint trouv√©e pour le service: {service_name}", level="warning")
            return

        log_message(f"Lancement du health check pour le service: {service_name}")
        for endpoint_config in endpoints_config:
            endpoint_key = f"{endpoint_config['endpoint_name']}-{str(endpoint_config['key'])}"
            start_time = time.monotonic()
            success = False
            try:
                request_method = endpoint_config.get("method", "GET")
                url = endpoint_config["url"]
                
                # Pr√©pare les param√®tres/donn√©es pour le health check
                params = endpoint_config.get("health_check_params", endpoint_config.get("fixed_params", {})).copy()
                json_data = endpoint_config.get("health_check_json", endpoint_config.get("fixed_json", {})).copy()
                headers = endpoint_config.get("fixed_headers", {}).copy()
                auth = None # Pour l'authentification de base (Basic Auth)
                
                check_timeout = endpoint_config.get("timeout", 5) # Timeout sp√©cifique pour le health check

                # Ajoute un suffixe √† l'URL si sp√©cifi√© (ex: pour les APIs bas√©es sur des chemins d'acc√®s)
                if "health_check_url_suffix" in endpoint_config:
                    url += endpoint_config["health_check_url_suffix"]

                # G√®re l'insertion de la cl√© API selon sa localisation (param√®tre, en-t√™te, Basic Auth)
                key_field = endpoint_config.get("key_field")
                key_location = endpoint_config.get("key_location")
                key_prefix = endpoint_config.get("key_prefix", "") # Pr√©fixe pour les cl√©s dans les en-t√™tes (ex: "Bearer ")
                api_key = endpoint_config["key"]

                if key_field and key_location:
                    if key_location == "param":
                        params[key_field] = api_key
                    elif key_location == "header":
                        headers[key_field] = f"{key_prefix}{api_key}"
                    elif key_location == "auth_basic":
                        if isinstance(api_key, tuple) and len(api_key) == 2:
                            auth = httpx.BasicAuth(api_key[0], api_key[1])
                        else:
                            log_message(f"Cl√© API pour auth_basic non valide pour {service_name}:{endpoint_key}", level="error")
                            success = False
                            continue # Passe √† l'endpoint suivant

                async with httpx.AsyncClient(timeout=check_timeout) as client:
                    response = await client.request(request_method, url, params=params, headers=headers, json=json_data, auth=auth)
                    response.raise_for_status() # L√®ve une exception pour les codes d'√©tat HTTP 4xx/5xx
                    success = True
            except httpx.HTTPStatusError as e:
                log_level = "warning"
                # Les codes 4xx (sauf 429 - Too Many Requests) indiquent souvent une erreur client
                # (cl√© invalide, param√®tre manquant) qui ne se r√©soudra pas avec un r√©essai
                # et n'indique pas forc√©ment un probl√®me de "sant√©" du service lui-m√™me.
                # Nous les loguons en debug pour ne pas surcharger les logs.
                if 400 <= e.response.status_code < 500 and e.response.status_code != 429:
                    log_level = "debug" 
                log_message(f"Health check pour {endpoint_key} ({service_name}) a √©chou√© (HTTP {e.response.status_code}): {e.response.text}", level=log_level)
                success = False
            except httpx.RequestError as e:
                # Erreurs r√©seau (connexion, timeout, DNS, etc.)
                log_message(f"Health check pour {endpoint_key} ({service_name}) a √©chou√© (R√©seau): {e}", level="warning")
                success = False
            except Exception as e:
                # Autres erreurs inattendues
                log_message(f"Health check pour {endpoint_key} ({service_name}) a √©chou√© (Inattendu): {e}", level="error")
                success = False
            finally:
                latency = time.monotonic() - start_time # Calcule la latence du check
                self.update_endpoint_health(service_name, endpoint_key, success, latency)
        log_message(f"Health check termin√© pour le service: {service_name}")

    def update_endpoint_health(self, service_name: str, endpoint_key: str, success: bool, latency: float):
        """
        Met √† jour le statut de sant√© d'un endpoint sp√©cifique.
        Utilise une moyenne glissante pour le taux de succ√®s et la latence.
        """
        # S'assure que la structure de donn√©es existe
        if service_name not in self.health_status:
            self.health_status[service_name] = {}
        if endpoint_key not in self.health_status[service_name]:
            self.health_status[service_name][endpoint_key] = {
                "latency": 0.0,
                "success_rate": 1.0,
                "last_checked": None,
                "error_count": 0,
                "total_checks": 0,
                "is_healthy": True
            }

        status = self.health_status[service_name][endpoint_key]
        status["total_checks"] += 1
        status["last_checked"] = format_datetime(get_current_time())

        alpha = 0.1 # Facteur de lissage pour les moyennes glissantes (0.1 signifie 10% de la nouvelle valeur, 90% de l'ancienne)
        if success:
            status["error_count"] = max(0, status["error_count"] - 1) # Diminue le compteur d'erreurs en cas de succ√®s
            status["success_rate"] = status["success_rate"] * (1 - alpha) + 1.0 * alpha # Augmente le taux de succ√®s
            status["latency"] = status["latency"] * (1 - alpha) + latency * alpha # Met √† jour la latence
        else:
            status["error_count"] += 1 # Incr√©mente le compteur d'erreurs
            status["success_rate"] = status["success_rate"] * (1 - alpha) + 0.0 * alpha # Diminue le taux de succ√®s
            # Si √©chec, p√©nalise la latence pour rendre l'endpoint moins attrayant (valeur arbitraire √©lev√©e)
            status["latency"] = status["latency"] * (1 - alpha) + 10.0 * alpha 

        # D√©termine si l'endpoint est sain bas√© sur le nombre d'erreurs cons√©cutives ou le taux de succ√®s
        if status["error_count"] >= 3 or status["success_rate"] < 0.5:
            status["is_healthy"] = False
        else:
            status["is_healthy"] = True
        
        # Sauvegarde l'√©tat mis √† jour de mani√®re asynchrone
        asyncio.create_task(save_json(ENDPOINT_HEALTH_FILE, self.health_status))
        log_message(f"Sant√© de {service_name}:{endpoint_key} mise √† jour: Succ√®s: {success}, Latence: {latency:.2f}s, Taux Succ√®s: {status['success_rate']:.2f}, Sain: {status['is_healthy']}", level="debug" if not status["is_healthy"] else "info")

    def get_best_endpoint(self, service_name: str) -> Optional[Dict]:
        """
        S√©lectionne le meilleur endpoint pour un service donn√© bas√© sur son statut de sant√©.
        Priorise les endpoints sains, puis les moins mauvais en cas d'absence d'endpoints sains.
        """
        service_health = self.health_status.get(service_name)
        if not service_health:
            log_message(f"Aucune donn√©e de sant√© pour le service {service_name}. Retourne None.", level="warning")
            return None

        best_endpoint_key = None
        best_score = -float('inf')

        # Filtre les endpoints actuellement consid√©r√©s comme sains
        healthy_endpoints = [
            (key, status) for key, status in service_health.items() if status["is_healthy"]
        ]

        if not healthy_endpoints:
            log_message(f"Aucun endpoint sain pour le service {service_name}. Tentative de s√©lection d'un endpoint non sain.", level="warning")
            all_endpoints = service_health.items()
            if not all_endpoints: 
                return None # Aucun endpoint du tout
            
            # Si aucun endpoint sain, choisit le "moins mauvais" : moins d'erreurs, meilleure latence
            sorted_endpoints = sorted(all_endpoints, key=lambda item: (item[1]["error_count"], item[1]["latency"]))
            best_endpoint_key = sorted_endpoints[0][0]
            log_message(f"Fallback: Endpoint {best_endpoint_key} s√©lectionn√© pour {service_name} (non sain).", level="warning")
        else:
            # Calcule un score pour chaque endpoint sain pour choisir le meilleur
            for endpoint_key, status in healthy_endpoints:
                # Score = (Taux de succ√®s * 100) - (Latence * 10) - (Compteur d'erreurs * 5)
                # Favorise le succ√®s, p√©nalise la latence et les erreurs
                score = (status["success_rate"] * 100) - (status["latency"] * 10) - (status["error_count"] * 5)
                if score > best_score:
                    best_score = score
                    best_endpoint_key = endpoint_key
            log_message(f"Meilleur endpoint s√©lectionn√© pour {service_name}: {best_endpoint_key} (Score: {best_score:.2f})")

        if best_endpoint_key:
            # Une fois la cl√© du meilleur endpoint trouv√©e, on r√©cup√®re sa configuration compl√®te
            # depuis `API_CONFIG` pour l'utiliser dans la requ√™te.
            for endpoint_config in API_CONFIG.get(service_name, []):
                current_endpoint_key = f"{endpoint_config['endpoint_name']}-{str(endpoint_config['key'])}"
                if current_endpoint_key == best_endpoint_key:
                    return endpoint_config
        return None # Aucun endpoint appropri√© trouv√©

class APIClient:
    """
    Classe de base pour tous les clients API.
    Elle g√®re la s√©lection dynamique d'endpoints, les r√©essais en cas d'√©chec
    et l'int√©gration avec le gestionnaire de sant√© des endpoints.
    """
    def __init__(self, name: str, endpoint_health_manager: EndpointHealthManager):
        self.name = name
        self.endpoints_config = API_CONFIG.get(name, []) # R√©cup√®re la config des endpoints pour cette API
        self.endpoint_health_manager = endpoint_health_manager
        if not self.endpoints_config:
            log_message(f"Client API {self.name} initialis√© sans configuration d'endpoint.", level="error")

    async def _make_request(self, params: Optional[Dict] = None, headers: Optional[Dict] = None, 
                            json_data: Optional[Dict] = None, timeout: Optional[int] = None, 
                            max_retries: int = 3, initial_delay: float = 1.0, 
                            url: Optional[str] = None, method: Optional[str] = None, 
                            key_field: Optional[str] = None, key_location: Optional[str] = None, 
                            api_key: Optional[Union[str, Tuple[str, str]]] = None, 
                            fixed_params: Optional[Dict] = None, fixed_headers: Optional[Dict] = None, 
                            fixed_json: Optional[Dict] = None) -> Optional[Union[Dict, str, bytes]]:
        """
        M√©thode interne pour effectuer les requ√™tes HTTP en utilisant le meilleur endpoint avec r√©essais.
        
        Args:
            params (Dict, optional): Param√®tres de requ√™te √† ajouter √† l'URL.
            headers (Dict, optional): En-t√™tes HTTP suppl√©mentaires.
            json_data (Dict, optional): Donn√©es JSON √† envoyer dans le corps de la requ√™te (pour POST/PUT).
            timeout (int, optional): Timeout pour la requ√™te en secondes.
            max_retries (int): Nombre maximal de tentatives en cas d'√©chec.
            initial_delay (float): D√©lai initial entre les r√©essais (exponentiel).
            url (str, optional): URL sp√©cifique √† utiliser (si non bas√© sur un endpoint configur√©).
            method (str, optional): M√©thode HTTP (GET, POST, etc.).
            key_field (str, optional): Nom du champ pour la cl√© API.
            key_location (str, optional): O√π placer la cl√© API ('param', 'header', 'auth_basic').
            api_key (Union[str, Tuple[str, str]], optional): Cl√© API √† utiliser.
            fixed_params (Dict, optional): Param√®tres fixes d√©finis dans la configuration de l'endpoint.
            fixed_headers (Dict, optional): En-t√™tes fixes d√©finis dans la configuration de l'endpoint.
            fixed_json (Dict, optional): Donn√©es JSON fixes d√©finies dans la configuration de l'endpoint.

        Returns:
            Union[Dict, str, bytes, None]: La r√©ponse de l'API (JSON d√©cod√©, texte brut, ou bytes pour les images),
                                          ou un dictionnaire d'erreur en cas d'√©chec.
        """
        
        selected_endpoint_config = None
        endpoint_key_for_health = "Dynamic" # Cl√© par d√©faut pour les requ√™tes dynamiques (non configur√©es)

        if url and method: # Si l'URL et la m√©thode sont fournies directement (requ√™te dynamique)
            selected_endpoint_config = {
                "url": url,
                "method": method,
                "key_field": key_field,
                "key_location": key_location,
                "key": api_key,
                "fixed_params": fixed_params if fixed_params is not None else {},
                "fixed_headers": fixed_headers if fixed_headers is not None else {},
                "fixed_json": fixed_json if fixed_json is not None else {},
                "endpoint_name": "Dynamic", # Nom g√©n√©rique pour les endpoints dynamiques
                "timeout": timeout if timeout is not None else 30
            }
            if api_key:
                # Cr√©e une cl√© de sant√© unique pour les requ√™tes dynamiques avec cl√© API
                endpoint_key_for_health = f"Dynamic-{str(api_key)}"
            log_message(f"Requ√™te dynamique pour {self.name} vers {url}")
        else: # S√©lectionne le meilleur endpoint configur√© via le gestionnaire de sant√©
            selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
            if not selected_endpoint_config:
                log_message(f"Aucun endpoint sain ou disponible pour {self.name}.", level="error")
                return {"error": True, "message": f"Aucun endpoint sain ou disponible pour {self.name}."}
            # Construit la cl√© de sant√© pour l'endpoint s√©lectionn√©
            endpoint_key_for_health = f"{selected_endpoint_config['endpoint_name']}-{str(selected_endpoint_config['key'])}"
            log_message(f"Endpoint s√©lectionn√© pour {self.name}: {selected_endpoint_config['endpoint_name']}")
            # Utilise le timeout de la config de l'endpoint si non sp√©cifi√©
            timeout = timeout if timeout is not None else selected_endpoint_config.get("timeout", 30)

        url_to_use = selected_endpoint_config["url"]
        method_to_use = selected_endpoint_config["method"]

        # Initialise les param√®tres, en-t√™tes et donn√©es JSON avec les valeurs fixes de la config
        request_params = selected_endpoint_config.get("fixed_params", {}).copy()
        request_headers = selected_endpoint_config.get("fixed_headers", {}).copy()
        request_json_data = selected_endpoint_config.get("fixed_json", {}).copy()
        auth = None # Pour l'authentification de base

        # Fusionne les param√®tres/en-t√™tes/donn√©es JSON fournis avec les valeurs fixes
        if params:
            request_params.update(params)
        if headers:
            request_headers.update(headers)
        if json_data:
            request_json_data.update(json_data)

        # G√®re l'insertion de la cl√© API pour la requ√™te
        key_field_to_use = selected_endpoint_config.get("key_field")
        key_location_to_use = selected_endpoint_config.get("key_location")
        key_prefix = selected_endpoint_config.get("key_prefix", "")
        api_key_to_use = selected_endpoint_config["key"]

        if key_field_to_use and key_location_to_use:
            if key_location_to_use == "param":
                request_params[key_field_to_use] = api_key_to_use
            elif key_location_to_use == "header":
                request_headers[key_field_to_use] = f"{key_prefix}{api_key_to_use}"
            elif key_location_to_use == "auth_basic":
                if isinstance(api_key_to_use, tuple) and len(api_key_to_use) == 2:
                    auth = httpx.BasicAuth(api_key_to_use[0], api_key_to_use[1])
                else:
                    log_message(f"Cl√© API pour auth_basic non valide pour {self.name}:{endpoint_key_for_health}", level="error")
                    # Marque l'endpoint comme non sain et retourne une erreur
                    self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, 0.0)
                    return {"error": True, "message": "Configuration d'authentification basique invalide."}

        current_delay = initial_delay # D√©lai initial pour les r√©essais
        for attempt in range(max_retries):
            start_time = time.monotonic()
            success = False
            try:
                async with httpx.AsyncClient(timeout=timeout) as client:
                    response = await client.request(method_to_use, url_to_use, params=request_params, headers=request_headers, json=request_json_data, auth=auth)
                    response.raise_for_status() # L√®ve une exception pour les codes d'√©tat HTTP 4xx/5xx
                    success = True
                    
                    content_type = response.headers.get("Content-Type", "").lower()
                    if "application/json" in content_type:
                        try:
                            return response.json() # Tente de d√©coder la r√©ponse JSON
                        except json.JSONDecodeError:
                            log_message(f"API {self.name} r√©ponse non JSON valide (tentative {attempt+1}/{max_retries}): {response.text[:200]}...", level="warning")
                            if attempt < max_retries - 1: # Si ce n'est pas la derni√®re tentative, r√©essaie
                                await asyncio.sleep(current_delay)
                                current_delay *= 2 # Augmente le d√©lai de mani√®re exponentielle
                                continue
                            return {"error": True, "message": "R√©ponse API non JSON valide.", "raw_response": response.text}
                    else:
                        log_message(f"API {self.name} a renvoy√© un Content-Type non JSON: {content_type}", level="info")
                        return response.content # Retourne le contenu brut (ex: pour les images)

            except httpx.HTTPStatusError as e:
                log_message(f"API {self.name} erreur HTTP (tentative {attempt+1}/{max_retries}): {e.response.status_code} - {e.response.text}", level="warning")
                # Ne pas r√©essayer pour les erreurs client (4xx) sauf 429 (Too Many Requests)
                if 400 <= e.response.status_code < 500 and e.response.status_code != 429:
                    log_message(f"API {self.name}: Erreur client {e.response.status_code}, pas de r√©essai.", level="error")
                    self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, e.response.elapsed.total_seconds())
                    return {"error": True, "status_code": e.response.status_code, "message": e.response.text}
                
                if attempt < max_retries - 1:
                    log_message(f"API {self.name}: R√©essai dans {current_delay:.2f}s...", level="info")
                    await asyncio.sleep(current_delay)
                    current_delay *= 2
            except httpx.RequestError as e:
                log_message(f"API {self.name} erreur de requ√™te (tentative {attempt+1}/{max_retries}): {e}", level="warning")
                if attempt < max_retries - 1:
                    log_message(f"API {self.name}: R√©essai dans {current_delay:.2f}s...", level="info")
                    await asyncio.sleep(current_delay)
                    current_delay *= 2
            except Exception as e:
                log_message(f"API {self.name} erreur inattendue (tentative {attempt+1}/{max_retries}): {e}", level="error")
                self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, time.monotonic() - start_time)
                return {"error": True, "message": str(e)}
            finally:
                # Met √† jour la sant√© de l'endpoint m√™me en cas d'√©chec pour la s√©lection future
                if not success:
                    latency = time.monotonic() - start_time
                    self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, latency)
        
        log_message(f"API {self.name}: Toutes les tentatives ont √©chou√© apr√®s {max_retries} r√©essais.", level="error")
        return {"error": True, "message": f"√âchec de la requ√™te apr√®s {max_retries} tentatives."}

    async def query(self, *args, **kwargs) -> Any:
        """
        M√©thode abstraite pour interroger l'API.
        Doit √™tre impl√©ment√©e par chaque sous-classe de client API.
        """
        raise NotImplementedError("La m√©thode query doit √™tre impl√©ment√©e par les sous-classes.")

# Instancier le gestionnaire de sant√© des endpoints (sera initialis√© dans main.py)
endpoint_health_manager = EndpointHealthManager()

def set_endpoint_health_manager_global(manager: EndpointHealthManager):
    """
    Permet d'injecter l'instance du gestionnaire de sant√© des endpoints.
    Ceci est utilis√© pour s'assurer que tous les clients API utilisent la m√™me instance.
    """
    global endpoint_health_manager
    endpoint_health_manager = manager

# --- Clients API Sp√©cifiques ---
# Chaque classe de client API h√©rite de `APIClient` et impl√©mente la m√©thode `query`
# pour interagir avec une API sp√©cifique.

class DeepSeekClient(APIClient):
    def __init__(self):
        super().__init__("DEEPSEEK", endpoint_health_manager)

    async def query(self, prompt: str, model: str = "deepseek-chat") -> str:
        """Interroge l'API DeepSeek pour des compl√©tions de chat."""
        payload = {"model": model, "messages": [{"role": "user", "content": prompt}]}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            content = response.get("choices", [{}])[0].get("message", {}).get("content")
            if content:
                return content
            return "DeepSeek: Pas de contenu de r√©ponse trouv√©."
        return f"DeepSeek: Erreur: {response.get('message', 'Inconnu')}" if response else "DeepSeek: R√©ponse vide ou erreur interne."

class SerperClient(APIClient):
    def __init__(self):
        super().__init__("SERPER", endpoint_health_manager)

    async def query(self, query_text: str) -> str:
        """Effectue une recherche web via l'API Serper."""
        payload = {"q": query_text}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            organic_results = response.get("organic", [])
            if organic_results:
                snippet = organic_results[0].get("snippet", "Pas de snippet.")
                link = organic_results[0].get("link", "")
                return f"Serper (recherche web):\n{snippet} {neutralize_urls(link)}"
            return "Serper: Aucune information trouv√©e."
        return f"Serper: Erreur: {response.get('message', 'Inconnu')}" if response else "Serper: R√©ponse vide ou erreur interne."

class WolframAlphaClient(APIClient):
    def __init__(self):
        super().__init__("WOLFRAMALPHA", endpoint_health_manager)

    async def query(self, input_text: str) -> str:
        """Interroge WolframAlpha pour des calculs ou des faits."""
        params = {"input": input_text}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            pods = response.get("queryresult", {}).get("pods", [])
            if pods:
                for pod in pods:
                    # Tente de trouver les pods les plus pertinents pour un r√©sultat direct
                    if pod.get("title") in ["Result", "Input interpretation", "Decimal approximation"]:
                        subpods = pod.get("subpods", [])
                        if subpods and subpods[0].get("plaintext"):
                            return f"WolframAlpha:\n{subpods[0]['plaintext']}"
                # Fallback: prend le premier pod si aucun pod pertinent n'est trouv√©
                if pods and pods[0].get("subpods") and pods[0]["subpods"][0].get("plaintext"):
                    return f"WolframAlpha:\n{pods[0]['subpods'][0]['plaintext']}"
            return "WolframAlpha: Pas de r√©sultat clair."
        return f"WolframAlpha: Erreur: {response.get('message', 'Inconnu')}" if response else "WolframAlpha: R√©ponse vide ou erreur interne."

class TavilyClient(APIClient):
    def __init__(self):
        super().__init__("TAVILY", endpoint_health_manager)

    async def query(self, query_text: str, max_results: int = 3) -> str:
        """Effectue une recherche web avanc√©e via l'API Tavily."""
        payload = {"query": query_text, "max_results": max_results}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            results = response.get("results", [])
            answer = response.get("answer", "Aucune r√©ponse directe trouv√©e.")

            output = f"Tavily (recherche web):\nR√©ponse directe: {answer}\n"
            if results:
                output += "Extraits pertinents:\n"
                for i, res in enumerate(results[:max_results]):
                    output += f"- {res.get('title', 'N/A')}: {res.get('content', 'N/A')} {neutralize_urls(res.get('url', ''))}\n"
            return output
        return f"Tavily: Erreur: {response.get('message', 'Inconnu')}" if response else "Tavily: R√©ponse vide ou erreur interne."

class ApiFlashClient(APIClient):
    def __init__(self):
        super().__init__("APIFLASH", endpoint_health_manager)

    async def query(self, url: str) -> str:
        """Capture une capture d'√©cran d'une URL via ApiFlash."""
        params = {"url": url, "format": "jpeg", "full_page": "true"}
        response_content = await self._make_request(params=params)

        if isinstance(response_content, bytes):
            selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
            if selected_endpoint_config:
                # ApiFlash retourne l'image directement. On peut construire une URL de pr√©visualisation
                # si l'API le permet, ou indiquer que la capture a √©t√© faite.
                # Ici, nous construisons une URL potentielle de l'image captur√©e.
                capture_url = f"{selected_endpoint_config['url']}?access_key={selected_endpoint_config['key']}&url={url}&format=jpeg&full_page=true"
                return f"ApiFlash (capture d'√©cran): {neutralize_urls(capture_url)} (V√©rifiez le lien pour l'image)"
            return "ApiFlash: Impossible de g√©n√©rer l'URL de capture."
        elif isinstance(response_content, dict) and response_content.get("error"):
            return f"ApiFlash: Erreur: {response_content.get('message', 'Inconnu')}"
        else:
            log_message(f"ApiFlash a renvoy√© un type de r√©ponse inattendu: {type(response_content)}", level="warning")
            return f"ApiFlash: R√©ponse inattendue de l'API. {response_content}"

class CrawlbaseClient(APIClient):
    def __init__(self):
        super().__init__("CRAWLBASE", endpoint_health_manager)

    async def query(self, url: str, use_js: bool = False) -> str:
        """Scrape le contenu HTML ou JavaScript d'une URL via Crawlbase."""
        params = {"url": url, "format": "json"}
        
        selected_endpoint_config = None
        if use_js:
            # Tente de trouver un endpoint sp√©cifiquement configur√© pour le scraping JS
            for config in API_CONFIG.get(self.name, []):
                if "JS Scraping" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break
        
        # Fallback si pas de config JS sp√©cifique ou si use_js est False
        if not selected_endpoint_config: 
            selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)

        if not selected_endpoint_config:
            return f"Crawlbase: Aucun endpoint sain ou disponible pour {self.name}."

        response = await self._make_request(
            params=params,
            url=selected_endpoint_config["url"],
            method=selected_endpoint_config["method"],
            key_field=selected_endpoint_config["key_field"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            fixed_params=selected_endpoint_config.get("fixed_params", {}),
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            body = response.get("body")
            if body:
                try:
                    # Le corps est encod√© en base64, il faut le d√©coder
                    decoded_body = base64.b64decode(body).decode('utf-8', errors='ignore')
                    return f"Crawlbase (contenu web):\n{decoded_body[:1000]}..." # Retourne les 1000 premiers caract√®res
                except Exception:
                    return f"Crawlbase (contenu web - brut):\n{body[:1000]}..." # En cas d'√©chec du d√©codage
            return "Crawlbase: Contenu non trouv√©."
        return f"Crawlbase: Erreur: {response.get('message', 'Inconnu')}" if response else "Crawlbase: R√©ponse vide ou erreur interne."

class DetectLanguageClient(APIClient):
    def __init__(self):
        super().__init__("DETECTLANGUAGE", endpoint_health_manager)

    async def query(self, text: str) -> str:
        """D√©tecte la langue d'un texte via DetectLanguage API."""
        payload = {"q": text}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            detections = response.get("data", {}).get("detections", [])
            if detections:
                first_detection = detections[0]
                lang = first_detection.get("language")
                confidence = first_detection.get("confidence")
                return f"Langue d√©tect√©e: {lang} (confiance: {confidence})"
            return "DetectLanguage: Aucune langue d√©tect√©e."
        return f"DetectLanguage: Erreur: {response.get('message', 'Inconnu')}" if response else "DetectLanguage: R√©ponse vide ou erreur interne."

class GuardianClient(APIClient):
    def __init__(self):
        super().__init__("GUARDIAN", endpoint_health_manager)

    async def query(self, query_text: str) -> str:
        """Recherche des articles de presse via l'API The Guardian."""
        params = {"q": query_text}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            results = response.get("response", {}).get("results", [])
            if results:
                output = "Articles The Guardian:\n"
                for res in results[:3]: # Limite √† 3 articles pour la concision
                    output += f"- {res.get('webTitle', 'N/A')}: {res.get('fields', {}).get('trailText', 'N/A')} {neutralize_urls(res.get('webUrl', ''))}\n"
                return output
            return "Guardian: Aucun article trouv√©."
        return f"Guardian: Erreur: {response.get('message', 'Inconnu')}" if response else "Guardian: R√©ponse vide ou erreur interne."

class IP2LocationClient(APIClient):
    def __init__(self):
        super().__init__("IP2LOCATION", endpoint_health_manager)

    async def query(self, ip_address: str) -> str:
        """G√©olocalise une adresse IP via IP2Location API."""
        params = {"ip": ip_address}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            if "country_name" in response:
                return f"IP2Location (G√©olocalisation IP {ip_address}): Pays: {response['country_name']}, Ville: {response.get('city_name', 'N/A')}"
            return "IP2Location: Informations non trouv√©es."
        return f"IP2Location: Erreur: {response.get('message', 'Inconnu')}" if response else "IP2Location: R√©ponse vide ou erreur interne."

class ShodanClient(APIClient):
    def __init__(self):
        super().__init__("SHODAN", endpoint_health_manager)

    async def query(self, query_text: str = "") -> str:
        """
        Interroge Shodan pour des informations sur un h√¥te IP ou des informations sur la cl√© API.
        Si `query_text` est une IP, tente de r√©cup√©rer les infos de l'h√¥te.
        Sinon, ou en cas d'√©chec, retourne les infos de la cl√© API.
        """
        if re.match(r"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$", query_text): # V√©rifie si c'est une adresse IP
            selected_endpoint_config = None
            for config in API_CONFIG.get(self.name, []):
                if "Host Info" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break
            if selected_endpoint_config:
                # Construire l'URL pour la recherche d'h√¥te
                url = f"{selected_endpoint_config['url'].rstrip('/')}/{query_text}"
                response = await self._make_request(
                    params={"key": selected_endpoint_config["key"]},
                    url=url,
                    method="GET",
                    key_field=selected_endpoint_config["key_field"],
                    key_location=selected_endpoint_config["key_location"],
                    api_key=selected_endpoint_config["key"],
                    timeout=selected_endpoint_config.get("timeout")
                )
                if response and not response.get("error"):
                    return f"Shodan (info h√¥te {query_text}): Pays: {response.get('country_name', 'N/A')}, Ports: {response.get('ports', 'N/A')}, Vuln√©rabilit√©s: {response.get('vulns', 'Aucune')}"
                return f"Shodan (info h√¥te): Erreur: {response.get('message', 'Inconnu')}" if response else "Shodan: R√©ponse vide ou erreur interne."
            else:
                return "Shodan: Endpoint 'Host Info' non configur√©."
        else:
            # Si pas d'IP ou si la recherche d'h√¥te n'est pas applicable, retourne les infos de la cl√© API
            response = await self._make_request() # Utilise le premier endpoint disponible (API Info)
            if response and not response.get("error"):
                return f"Shodan (info cl√©): Requ√™tes restantes: {response.get('usage_limits', {}).get('query_credits', 'N/A')}, Scan cr√©dits: {response.get('usage_limits', {}).get('scan_credits', 'N/A')}"
            return f"Shodan: Erreur: {response.get('message', 'Inconnu')}" if response else "Shodan: R√©ponse vide ou erreur interne."

class WeatherAPIClient(APIClient):
    def __init__(self):
        super().__init__("WEATHERAPI", endpoint_health_manager)

    async def query(self, location: str) -> str:
        """R√©cup√®re les conditions m√©t√©orologiques actuelles pour une localisation via WeatherAPI."""
        params = {"q": location}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            current = response.get("current", {})
            location_info = response.get("location", {})
            if current and location_info:
                return (
                    f"M√©t√©o √† {location_info.get('name', 'N/A')}, {location_info.get('country', 'N/A')}:\n"
                    f"Temp√©rature: {current.get('temp_c', 'N/A')}¬∞C, "
                    f"Conditions: {current.get('condition', {}).get('text', 'N/A')}, "
                    f"Vent: {current.get('wind_kph', 'N/A')} km/h"
                )
            return "WeatherAPI: Donn√©es m√©t√©o non trouv√©es."
        return f"WeatherAPI: Erreur: {response.get('message', 'Inconnu')}" if response else "WeatherAPI: R√©ponse vide ou erreur interne."

class CloudmersiveClient(APIClient):
    def __init__(self):
        super().__init__("CLOUDMERSIVE", endpoint_health_manager)

    async def query(self, domain: str) -> str:
        """V√©rifie la validit√© et le type d'un domaine via Cloudmersive API."""
        payload = {"domain": domain}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            return f"Cloudmersive (v√©rification de domaine {domain}): Valide: {response.get('ValidDomain', 'N/A')}, Type: {response.get('DomainType', 'N/A')}"
        return f"Cloudmersive: Erreur: {response.get('message', 'Inconnu')}" if response else "Cloudmersive: R√©ponse vide ou erreur interne."

class GreyNoiseClient(APIClient):
    def __init__(self):
        super().__init__("GREYNOISE", endpoint_health_manager)

    async def query(self, ip_address: str) -> str:
        """Analyse une adresse IP pour d√©tecter des activit√©s 'bruit' (malveillantes) via GreyNoise."""
        selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
        if not selected_endpoint_config:
            return f"GreyNoise: Aucun endpoint sain ou disponible pour {self.name}."

        # Construit l'URL avec l'adresse IP √† la fin
        url = f"{selected_endpoint_config['url'].rstrip('/')}/{ip_address}"
        method = selected_endpoint_config["method"]
        headers = {selected_endpoint_config["key_field"]: selected_endpoint_config["key"]}

        response = await self._make_request(
            headers=headers,
            url=url,
            method=method,
            key_field=selected_endpoint_config["key_field"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if response.get("noise"):
                return f"GreyNoise (IP {ip_address}): C'est une IP 'bruit' (malveillante). Classification: {response.get('classification', 'N/A')}, Nom d'acteur: {response.get('actor', 'N/A')}"
            return f"GreyNoise (IP {ip_address}): Pas de 'bruit' d√©tect√©. Statut: {response.get('status', 'N/A')}"
        return f"GreyNoise: Erreur: {response.get('message', 'Inconnu')}" if response else "GreyNoise: R√©ponse vide ou erreur interne."

class PulsediveClient(APIClient):
    def __init__(self):
        super().__init__("PULSEDIVE", endpoint_health_manager)

    async def query(self, indicator: str, type: str = "auto") -> str:
        """Analyse un indicateur de menace (IP, domaine, URL) via Pulsedive."""
        params = {"indicator": indicator, "type": type}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            if response.get("results"):
                result = response["results"][0] # Prend le premier r√©sultat
                return (
                    f"Pulsedive (Analyse {indicator}): Type: {result.get('type', 'N/A')}, "
                    f"Risk: {result.get('risk', 'N/A')}, "
                    f"Description: {result.get('description', 'N/A')[:200]}..." # Tronque la description
                )
            return "Pulsedive: Aucun r√©sultat d'analyse trouv√©."
        return f"Pulsedive: Erreur: {response.get('message', 'Inconnu')}" if response else "Pulsedive: R√©ponse vide ou erreur interne."

class StormGlassClient(APIClient):
    def __init__(self):
        super().__init__("STORMGLASS", endpoint_health_manager)

    async def query(self, lat: float, lng: float, params: str = "airTemperature,waveHeight") -> str:
        """R√©cup√®re les donn√©es m√©t√©orologiques maritimes pour une coordonn√©e via StormGlass."""
        now = int(time.time())
        request_params = {
            "lat": lat,
            "lng": lng,
            "params": params,
            "start": now,
            "end": now + 3600 # Pr√©visions pour la prochaine heure (3600 secondes)
        }
        response = await self._make_request(params=request_params)
        if response and not response.get("error"):
            data = response.get("hours", [])
            if data:
                first_hour = data[0]
                # Acc√®de aux valeurs sp√©cifiques des param√®tres demand√©s
                temp = first_hour.get('airTemperature', [{}])[0].get('value', 'N/A')
                wave_height = first_hour.get('waveHeight', [{}])[0].get('value', 'N/A')
                return f"StormGlass (M√©t√©o maritime √† {lat},{lng}): Temp√©rature air: {temp}¬∞C, Hauteur vagues: {wave_height}m"
            return "StormGlass: Donn√©es non trouv√©es."
        return f"StormGlass: Erreur: {response.get('message', 'Inconnu')}" if response else "StormGlass: R√©ponse vide ou erreur interne."

class LoginRadiusClient(APIClient):
    def __init__(self):
        super().__init__("LOGINRADIUS", endpoint_health_manager)

    async def query(self) -> str:
        """Effectue un simple ping √† l'API LoginRadius pour v√©rifier sa disponibilit√©."""
        response = await self._make_request()
        if response and not response.get("error"):
            return f"LoginRadius (Ping API): Statut: {response.get('Status', 'N/A')}, Message: {response.get('Message', 'N/A')}"
        return f"LoginRadius: Erreur: {response.get('message', 'Inconnu')}" if response else "LoginRadius: R√©ponse vide ou erreur interne."

class JsonbinClient(APIClient):
    def __init__(self):
        super().__init__("JSONBIN", endpoint_health_manager)

    async def query(self, data: Optional[Dict[str, Any]] = None, private: bool = True, bin_id: Optional[str] = None) -> str:
        """
        Cr√©e un nouveau 'bin' JSON ou acc√®de √† un bin existant via Jsonbin.io.
        `data` est pour la cr√©ation, `bin_id` pour l'acc√®s.
        """
        if bin_id: # Acc√®s √† un bin existant
            selected_endpoint_config = None
            for config in API_CONFIG.get(self.name, []):
                if "Bin Access" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break
            if not selected_endpoint_config:
                return f"Jsonbin: Aucun endpoint d'acc√®s de bin sain ou disponible pour {self.name}."

            url = f"{selected_endpoint_config['url'].rstrip('/')}/{bin_id}"
            method = "GET"
            headers = {selected_endpoint_config["key_field"]: selected_endpoint_config["key"]}
            
            response = await self._make_request(
                headers=headers,
                url=url,
                method=method,
                timeout=selected_endpoint_config.get("timeout")
            )
            if response and not response.get("error"):
                return f"Jsonbin (Acc√®s bin {bin_id}):\n{json.dumps(response, indent=2)}"
            return f"Jsonbin (Acc√®s bin): Erreur: {response.get('message', 'Inconnu')}" if response else "Jsonbin: R√©ponse vide ou erreur interne."
        
        else: # Cr√©ation d'un nouveau bin
            selected_endpoint_config = None
            for config in API_CONFIG.get(self.name, []):
                if "Bin Create" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break
            
            if not selected_endpoint_config:
                return f"Jsonbin: Aucun endpoint de cr√©ation de bin sain ou disponible pour {self.name}."

            url = selected_endpoint_config["url"]
            method = "POST"
            headers = {selected_endpoint_config["key_field"]: selected_endpoint_config["key"], "Content-Type": "application/json"}
            payload = {"record": data if data is not None else {}, "private": private}

            response = await self._make_request(
                json_data=payload,
                headers=headers,
                url=url,
                method=method,
                timeout=selected_endpoint_config.get("timeout")
            )

            if response and not response.get("error"):
                return f"Jsonbin (Cr√©ation de bin): ID: {response.get('metadata', {}).get('id', 'N/A')}, URL: {neutralize_urls(response.get('metadata', {}).get('url', 'N/A'))}"
            return f"Jsonbin (Cr√©ation de bin): Erreur: {response.get('message', 'Inconnu')}" if response else "Jsonbin: R√©ponse vide ou erreur interne."

class HuggingFaceClient(APIClient):
    def __init__(self):
        super().__init__("HUGGINGFACE", endpoint_health_manager)

    async def query(self, model_name: str = "distilbert-base-uncased-finetuned-sst-2-english", input_text: str = "Hello world") -> str:
        """Effectue une inf√©rence sur un mod√®le HuggingFace (ex: classification de texte, g√©n√©ration)."""
        selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
        if not selected_endpoint_config:
            return f"HuggingFace: Aucun endpoint sain ou disponible pour {self.name}."

        # Construit l'URL d'inf√©rence pour le mod√®le sp√©cifi√©
        inference_url = f"https://api-inference.huggingface.co/models/{model_name}"
        
        headers = {
            selected_endpoint_config["key_field"]: f"{selected_endpoint_config['key_prefix']}{selected_endpoint_config['key']}",
            "Content-Type": "application/json"
        }
        payload = {"inputs": input_text}

        response = await self._make_request(
            json_data=payload,
            headers=headers,
            url=inference_url,
            method="POST",
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if isinstance(response, list) and response:
                first_result = response[0]
                if isinstance(first_result, list) and first_result: # Ex: pour les mod√®les de classification de texte
                    return f"HuggingFace ({model_name} - {first_result[0].get('label')}): Score {first_result[0].get('score', 'N/A'):.2f}"
                elif isinstance(first_result, dict) and "generated_text" in first_result: # Ex: pour les mod√®les de g√©n√©ration de texte
                    return f"HuggingFace ({model_name}): {first_result.get('generated_text')}"
            return f"HuggingFace ({model_name}): R√©ponse non pars√©e. {response}"
        return f"HuggingFace: Erreur: {response.get('message', 'Inconnu')}" if response else "HuggingFace: R√©ponse vide ou erreur interne."

class TwilioClient(APIClient):
    def __init__(self):
        super().__init__("TWILIO", endpoint_health_manager)

    async def query(self) -> str:
        """R√©cup√®re le solde du compte Twilio."""
        selected_endpoint_config = None
        for config in API_CONFIG.get(self.name, []):
            if "Account Balance" in config.get("endpoint_name", ""):
                selected_endpoint_config = config
                break
        if not selected_endpoint_config:
            # Fallback si l'endpoint sp√©cifique n'est pas trouv√©, prend le premier disponible
            if self.endpoints_config:
                selected_endpoint_config = self.endpoints_config[0]
            else:
                return f"Twilio: Aucune configuration d'endpoint disponible pour {self.name}."

        response = await self._make_request(
            url=selected_endpoint_config["url"],
            method=selected_endpoint_config["method"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            timeout=selected_endpoint_config.get("timeout")
        )
        if response and not response.get("error"):
            return f"Twilio (Balance): {response.get('balance', 'N/A')} {response.get('currency', 'N/A')}"
        return f"Twilio: Erreur: {response.get('message', 'Inconnu')}" if response else "Twilio: R√©ponse vide ou erreur interne."

class AbstractAPIClient(APIClient):
    def __init__(self):
        super().__init__("ABSTRACTAPI", endpoint_health_manager)

    async def query(self, input_value: str, api_type: str) -> str:
        """
        Interroge diverses APIs d'AbstractAPI (validation email/t√©l√©phone, taux de change, jours f√©ri√©s).
        `input_value` d√©pend du `api_type`.
        """
        params = {}
        target_endpoint_name = ""

        if api_type == "PHONE_VALIDATION":
            params["phone"] = input_value
            target_endpoint_name = "Phone Validation"
        elif api_type == "EMAIL_VALIDATION":
            params["email"] = input_value
            target_endpoint_name = "Email Validation"
        elif api_type == "EXCHANGE_RATES":
            # Pour les taux de change, input_value peut √™tre la devise de base (ex: "USD")
            params["base"] = input_value if input_value else "USD" 
            target_endpoint_name = "Exchange Rates"
        elif api_type == "HOLIDAYS":
            params["country"] = input_value if input_value else "US" # Pays par d√©faut si non sp√©cifi√©
            params["year"] = datetime.now().year # Ann√©e actuelle
            target_endpoint_name = "Holidays"
        else:
            return f"AbstractAPI: Type d'API '{api_type}' non support√© pour la requ√™te."

        selected_endpoint_config = None
        for config in API_CONFIG.get(self.name, []):
            if target_endpoint_name in config["endpoint_name"]:
                selected_endpoint_config = config
                break
        
        if not selected_endpoint_config:
            return f"AbstractAPI: Aucun endpoint sain ou disponible pour {self.name} pour le type {api_type}."

        response = await self._make_request(
            params=params,
            url=selected_endpoint_config["url"],
            method=selected_endpoint_config["method"],
            key_field=selected_endpoint_config["key_field"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            fixed_params=selected_endpoint_config.get("fixed_params", {}),
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if api_type == "PHONE_VALIDATION":
                return (
                    f"AbstractAPI (Validation T√©l): Num√©ro: {response.get('phone', 'N/A')}, "
                    f"Valide: {response.get('valid', 'N/A')}, "
                    f"Pays: {response.get('country', {}).get('name', 'N/A')}"
                )
            elif api_type == "EMAIL_VALIDATION":
                return (
                    f"AbstractAPI (Validation Email): Email: {response.get('email', 'N/A')}, "
                    f"Valide: {response.get('is_valid_format', 'N/A')}, "
                    f"Deliverable: {response.get('is_deliverable', 'N/A')}"
                )
            elif api_type == "EXCHANGE_RATES":
                # Affiche le taux de change pour USD par d√©faut, ou d'autres devises si pr√©sentes
                return f"AbstractAPI (Taux de change): Base: {response.get('base', 'N/A')}, Taux (USD): {response.get('exchange_rates', {}).get('USD', 'N/A')}"
            elif api_type == "HOLIDAYS":
                holidays = [h.get('name', 'N/A') for h in response if h.get('name')]
                return f"AbstractAPI (Jours f√©ri√©s {params.get('country', 'US')} {params.get('year')}): {', '.join(holidays[:5])}..." if holidays else "Aucun jour f√©ri√© trouv√©."
            return f"AbstractAPI ({api_type}): R√©ponse brute: {response}"
        return f"AbstractAPI ({api_type}): Erreur: {response.get('message', 'Inconnu')}" if response else "AbstractAPI: R√©ponse vide ou erreur interne."

class GeminiAPIClient: # Cette classe est distincte de APIClient car elle g√®re l'API Gemini directement
    def __init__(self):
        self.api_key = GEMINI_API_KEY
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models/"
        # Le mod√®le est d√©fini dans config.py et peut √™tre pass√© √† generate_content
        self.model_name = "gemini-1.5-flash-latest" # Valeur par d√©faut, peut √™tre √©cras√©e
        self.headers = {
            "Content-Type": "application/json",
        }
        # Les configurations de g√©n√©ration et de s√©curit√© sont √©galement dans config.py
        from config import GEMINI_TEMPERATURE, GEMINI_TOP_P, GEMINI_TOP_K, GEMINI_MAX_OUTPUT_TOKENS, GEMINI_SAFETY_SETTINGS
        self.generation_config = {
            "temperature": GEMINI_TEMPERATURE,
            "top_p": GEMINI_TOP_P,
            "top_k": GEMINI_TOP_K,
            "max_output_tokens": GEMINI_MAX_OUTPUT_TOKENS,
        }
        self.safety_settings = GEMINI_SAFETY_SETTINGS
        log_message(f"GeminiApiClient initialis√© avec le mod√®le par d√©faut: {self.model_name}")

    async def generate_content(self, prompt: str, chat_history: List[Dict], image_data: Optional[str] = None, model: Optional[str] = None) -> Dict:
        """
        G√©n√®re du contenu textuel ou multimodal en utilisant l'API Gemini.
        `chat_history` est une liste de dictionnaires au format Gemini (role, parts).
        `image_data` est une cha√Æne base64 de l'image avec son pr√©fixe mimeType (ex: "data:image/png;base64,...").
        `model` permet de sp√©cifier un mod√®le diff√©rent si n√©cessaire.
        """
        model_to_use = model if model else self.model_name
        url = f"{self.base_url}{model_to_use}:generateContent?key={self.api_key}"

        # Construire les contenus de la requ√™te. Le prompt utilisateur est toujours le dernier.
        contents = []
        for msg in chat_history:
            # L'API Gemini attend les r√¥les 'user' et 'model'
            role = "user" if msg["role"] == "user" else "model"
            contents.append({"role": role, "parts": [{"text": msg["content"]}]})
        
        user_parts = [{"text": prompt}]
        if image_data:
            # L'API Gemini attend un dictionnaire inlineData avec mimeType et data
            # Assurez-vous que image_data est au format "data:image/png;base64,..."
            # Extraire le mimeType et la base64 data
            if "," in image_data:
                mime_type_part, base64_data = image_data.split(",", 1)
                mime_type = mime_type_part.split(":", 1)[1].split(";", 1)[0]
            else:
                # Fallback si le pr√©fixe est manquant, suppose un type d'image commun
                mime_type = "image/jpeg" 
                base64_data = image_data

            user_parts.append({
                "inlineData": {
                    "mimeType": mime_type,
                    "data": base64_data
                }
            })
            log_message(f"Image ajout√©e au prompt Gemini (mimeType: {mime_type}).")

        contents.append({"role": "user", "parts": user_parts})

        payload = {
            "contents": contents,
            "generationConfig": self.generation_config,
            "safetySettings": self.safety_settings
        }

        log_message(f"Appel √† Gemini API pour le mod√®le {model_to_use}...")
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(url, headers=self.headers, json=payload)
                response.raise_for_status() # L√®ve une exception pour les codes d'√©tat HTTP 4xx/5xx
                result = response.json()
                log_message(f"R√©ponse Gemini re√ßue: {json.dumps(result, indent=2)}")
                return result
        except httpx.HTTPStatusError as e:
            log_message(f"Erreur HTTP Gemini API: {e.response.status_code} - {e.response.text}", level="error")
            return {"error": f"Erreur HTTP Gemini: {e.response.status_code} - {e.response.text}"}
        except httpx.RequestError as e:
            log_message(f"Erreur de requ√™te Gemini API: {e}", level="error")
            return {"error": f"Erreur de requ√™te Gemini: {e}"}
        except json.JSONDecodeError as e:
            log_message(f"Erreur de d√©codage JSON Gemini API: {e} - R√©ponse brute: {response.text}", level="error")
            return {"error": f"Erreur de d√©codage JSON Gemini: {e}"}
        except Exception as e:
            log_message(f"Erreur inattendue Gemini API: {e}\n{traceback.format_exc()}", level="error")
            return {"error": f"Erreur inattendue Gemini: {e}"}

class GoogleCustomSearchClient(APIClient):
    def __init__(self):
        super().__init__("GOOGLE_CUSTOM_SEARCH", endpoint_health_manager)

    async def query(self, query_text: str) -> str:
        """Effectue une recherche personnalis√©e Google via l'API Custom Search."""
        params = {"q": query_text}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            items = response.get("items", [])
            if items:
                output = "Google Custom Search:\n"
                for item in items[:3]: # Limite √† 3 r√©sultats
                    output += f"- {item.get('title', 'N/A')}: {item.get('snippet', 'N/A')} {neutralize_urls(item.get('link', ''))}\n"
                return output
            return "Google Custom Search: Aucun r√©sultat trouv√©."
        return f"Google Custom Search: Erreur: {response.get('message', 'Inconnu')}" if response else "Google Custom Search: R√©ponse vide ou erreur interne."

class RandommerClient(APIClient):
    def __init__(self):
        super().__init__("RANDOMMER", endpoint_health_manager)

    async def query(self, country_code: str = "US", quantity: int = 1) -> str:
        """G√©n√®re des num√©ros de t√©l√©phone al√©atoires via Randommer.io."""
        params = {"CountryCode": country_code, "Quantity": quantity}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            if isinstance(response, list) and response:
                return f"Randommer (Num√©ros de t√©l√©phone): {', '.join(response)}"
            return f"Randommer: {response}" # Si la r√©ponse n'est pas une liste (ex: erreur format√©e)
        return f"Randommer: Erreur: {response.get('message', 'Inconnu')}" if response else "Randommer: R√©ponse vide ou erreur interne."

class TomorrowIOClient(APIClient):
    def __init__(self):
        super().__init__("TOMORROW.IO", endpoint_health_manager)

    async def query(self, location: str, fields: Optional[List[str]] = None) -> str:
        """R√©cup√®re les pr√©visions m√©t√©orologiques via Tomorrow.io."""
        if fields is None:
            fields = ["temperature", "humidity", "windSpeed"] # Champs par d√©faut
        payload = {"location": location, "fields": fields, "units": "metric", "timesteps": ["1h"]}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            data = response.get("data", {}).get("timelines", [{}])[0].get("intervals", [{}])[0].get("values", {})
            if data:
                output = f"M√©t√©o (Tomorrow.io) √† {location}:\n"
                for field in fields:
                    output += f"- {field.capitalize()}: {data.get(field, 'N/A')}\n"
                return output
            return "Tomorrow.io: Donn√©es m√©t√©o non trouv√©es."
        return f"Tomorrow.io: Erreur: {response.get('message', 'Inconnu')}" if response else "Tomorrow.io: R√©ponse vide ou erreur interne."

class OpenWeatherMapClient(APIClient):
    def __init__(self):
        super().__init__("OPENWEATHERMAP", endpoint_health_manager)

    async def query(self, location: str) -> str:
        """R√©cup√®re les conditions m√©t√©orologiques actuelles via OpenWeatherMap."""
        params = {"q": location}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            main_data = response.get("main", {})
            weather_desc = response.get("weather", [{}])[0].get("description", "N/A")
            if main_data:
                temp_kelvin = main_data.get('temp', 'N/A')
                feels_like_kelvin = main_data.get('feels_like', 'N/A')
                
                # Convertit les temp√©ratures de Kelvin en Celsius
                temp_celsius = f"{temp_kelvin - 273.15:.2f}" if isinstance(temp_kelvin, (int, float)) else "N/A"
                feels_like_celsius = f"{feels_like_kelvin - 273.15:.2f}" if isinstance(feels_like_kelvin, (int, float)) else "N/A"

                return (
                    f"M√©t√©o (OpenWeatherMap) √† {location}:\n"
                    f"Temp√©rature: {temp_celsius}¬∞C, "
                    f"Ressenti: {feels_like_celsius}¬∞C, "
                    f"Humidit√©: {main_data.get('humidity', 'N/A')}%, "
                    f"Conditions: {weather_desc}"
                )
            return "OpenWeatherMap: Donn√©es m√©t√©o non trouv√©es."
        return f"OpenWeatherMap: Erreur: {response.get('message', 'Inconnu')}" if response else "OpenWeatherMap: R√©ponse vide ou erreur interne."

class MockarooClient(APIClient):
    def __init__(self):
        super().__init__("MOCKAROO", endpoint_health_manager)

    async def query(self, count: int = 1, fields_json: Optional[str] = None) -> str:
        """G√©n√®re des donn√©es de test via Mockaroo."""
        params = {"count": count}
        if fields_json:
            params["fields"] = fields_json # `fields_json` doit √™tre une cha√Æne JSON valide
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            return f"Mockaroo (G√©n√©ration de donn√©es):\n{json.dumps(response, indent=2)}"
        return f"Mockaroo: Erreur: {response.get('message', 'Inconnu')}" if response else "Mockaroo: R√©ponse vide ou erreur interne."

class OpenPageRankClient(APIClient):
    def __init__(self):
        super().__init__("OPENPAGERANK", endpoint_health_manager)

    async def query(self, domains: List[str]) -> str:
        """R√©cup√®re le PageRank de domaines via OpenPageRank."""
        params = {"domains[]": domains} # L'API attend "domains[]" pour une liste
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            results = response.get("response", [])
            if results:
                output = "OpenPageRank (Classement de domaine):\n"
                for res in results:
                    output += f"- Domaine: {res.get('domain', 'N/A')}, PageRank: {res.get('page_rank', 'N/A')}\n"
                return output
            return "OpenPageRank: Aucun r√©sultat trouv√©."
        return f"OpenPageRank: Erreur: {response.get('message', 'Inconnu')}" if response else "OpenPageRank: R√©ponse vide ou erreur interne."

class RapidAPIClient(APIClient):
    def __init__(self):
        super().__init__("RAPIDAPI", endpoint_health_manager)

    async def query(self, api_name: str, **kwargs) -> str:
        """
        Interroge diverses APIs disponibles via RapidAPI (blagues, taux de change, faits al√©atoires).
        `api_name` sp√©cifie l'API RapidAPI √† utiliser.
        """
        selected_endpoint_config = None
        for config in API_CONFIG.get(self.name, []):
            if api_name.lower() in config["endpoint_name"].lower():
                selected_endpoint_config = config
                break
        
        if not selected_endpoint_config:
            return f"RapidAPI: Endpoint pour '{api_name}' non trouv√© ou non configur√©."

        url = selected_endpoint_config["url"]
        method = selected_endpoint_config["method"]
        
        request_params = selected_endpoint_config.get("fixed_params", {}).copy()
        request_headers = selected_endpoint_config.get("fixed_headers", {}).copy()
        request_json_data = selected_endpoint_config.get("fixed_json", {}).copy()

        # Ajoute les kwargs aux bons endroits selon la m√©thode HTTP
        if method == "GET":
            request_params.update(kwargs)
        elif method == "POST":
            request_json_data.update(kwargs)

        # Les en-t√™tes sp√©cifiques √† RapidAPI sont essentiels
        headers = {
            selected_endpoint_config["key_field"]: selected_endpoint_config["key"],
            "X-RapidAPI-Host": selected_endpoint_config["fixed_headers"].get("X-RapidAPI-Host")
        }
        
        response = await self._make_request(
            params=request_params,
            headers=headers,
            json_data=request_json_data,
            url=url,
            method=method,
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if api_name.lower() == "programming joke":
                return f"RapidAPI (Blague de Programmation): {response.get('setup', '')} - {response.get('punchline', '')}"
            elif api_name.lower() == "currency list quotes":
                return f"RapidAPI (Devises): {json.dumps(response, indent=2)}"
            elif api_name.lower() == "random fact":
                return f"RapidAPI (Fait Al√©atoire): {response.get('text', 'N/A')}"
            return f"RapidAPI ({api_name}): {json.dumps(response, indent=2)}" # Retourne la r√©ponse brute pour les autres
        return f"RapidAPI ({api_name}): Erreur: {response.get('message', 'Inconnu')}" if response else "RapidAPI: R√©ponse vide ou erreur interne."

class OCRApiClient: # Cette classe est distincte de APIClient car elle g√®re l'API OCR.space directement
    def __init__(self):
        from config import OCR_API_KEY # Importe la cl√© OCR sp√©cifique pour cette classe
        self.api_key = OCR_API_KEY
        self.base_url = "https://api.ocr.space/parse/image"
        log_message("OCRApiClient initialis√©.")

    async def query(self, image_base64: str) -> str:
        """
        Effectue une requ√™te OCR √† l'API OCR.space.
        `image_base64` doit √™tre la cha√Æne base64 de l'image, incluant le pr√©fixe mimeType
        (ex: "data:image/png;base64,...").
        """
        payload = {
            "base64Image": image_base64,
            "language": "fre", # Langue par d√©faut : Fran√ßais
            "isOverlayRequired": False, # Ne pas inclure l'overlay des r√©gions de texte
            "OCREngine": 2 # Utilise le moteur OCR 2 pour de meilleurs r√©sultats
        }
        headers = {
            "apikey": self.api_key,
            "Content-Type": "application/json"
        }

        log_message("Appel √† OCR.space API...")
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(self.base_url, headers=headers, json=payload)
                response.raise_for_status() # L√®ve une exception pour les codes d'√©tat HTTP 4xx/5xx
                result = response.json()

                if result.get("IsErroredOnProcessing"):
                    error_message = result.get("ErrorMessage", ["Erreur inconnue lors du traitement OCR."])
                    log_message(f"Erreur OCR.space: {error_message}", level="error")
                    return f"‚ùå Erreur OCR: {', '.join(error_message)}"
                
                parsed_text = ""
                if "ParsedResults" in result and result["ParsedResults"]:
                    for parsed_result in result["ParsedResults"]:
                        parsed_text += parsed_result.get("ParsedText", "") + "\n"
                
                if parsed_text.strip():
                    log_message("OCR.space: Texte extrait avec succ√®s.")
                    return parsed_text.strip()
                else:
                    log_message("OCR.space: Aucun texte extrait.", level="warning")
                    return "Aucun texte n'a pu √™tre extrait de l'image."

        except httpx.HTTPStatusError as e:
            log_message(f"Erreur HTTP OCR.space API: {e.response.status_code} - {e.response.text}", level="error")
            return f"‚ùå Erreur HTTP OCR: {e.response.status_code} - {e.response.text}"
        except httpx.RequestError as e:
            log_message(f"Erreur de requ√™te OCR.space API: {e}", level="error")
            return f"‚ùå Erreur de requ√™te OCR: {e}"
        except json.JSONDecodeError as e:
            log_message(f"Erreur de d√©codage JSON OCR.space API: {e} - R√©ponse brute: {response.text}", level="error")
            return {"error": f"Erreur de d√©codage JSON OCR: {e}"}
        except Exception as e:
            log_message(f"Erreur inattendue OCR.space API: {e}\n{traceback.format_exc()}", level="error")
            return f"‚ùå Erreur inattendue OCR: {e}"


# Instancier tous les clients API en leur passant le gestionnaire de sant√©
# Note: GeminiApiClient et OCRApiClient sont instanci√©s s√©par√©ment car ils g√®rent leurs cl√©s directement.
ALL_API_CLIENTS = [
    DeepSeekClient(), SerperClient(), WolframAlphaClient(), TavilyClient(),
    ApiFlashClient(), CrawlbaseClient(), DetectLanguageClient(), GuardianClient(),
    IP2LocationClient(), ShodanClient(), WeatherAPIClient(),
    CloudmersiveClient(), GreyNoiseClient(), PulsediveClient(), StormGlassClient(),
    LoginRadiusClient(), JsonbinClient(),
    HuggingFaceClient(), TwilioClient(), AbstractAPIClient(),
    GoogleCustomSearchClient(), RandommerClient(), TomorrowIOClient(),
    OpenWeatherMapClient(),
    MockarooClient(), OpenPageRankClient(), RapidAPIClient()
    # GeminiApiClient et OCRApiClient ne sont pas inclus ici car ils sont g√©r√©s par leurs propres classes
]

import json
import asyncio
import random
import ast
import subprocess
import base64
import httpx
import io
import contextlib
import traceback
import hashlib
import difflib
import re
import logging
from datetime import datetime, timedelta, timezone
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, Any, Optional, List, Union, Tuple

# Imports des constantes depuis config.py
from config import (
    API_QUOTAS, API_COOLDOWN_DURATION_SECONDS, API_ROTATION_INTERVAL_MINUTES,
    QUOTA_BURN_WINDOW_HOURS, USER_CHAT_HISTORY_FILE, USER_LONG_MEMORY_FILE,
    IA_STATUS_FILE, QUOTAS_FILE, GROUP_CHAT_HISTORY_FILE, PRIVATE_GROUP_ID,
    BURN_QUOTA_THRESHOLD_RATIO, BURN_QUOTA_BEFORE_RESET_HOURS,
    FORBIDDEN_WORDS, ARCHIVES_DIR, MAX_FILE_SIZE, GEMINI_API_KEY, OCR_API_KEY
)

# Imports depuis utils.py (en supposant qu'il est d√©j√† d√©fini ou sera dans le m√™me fichier)
from utils import (
    load_json, save_json, get_current_time, format_datetime, log_message,
    neutralize_urls, extract_keywords, tag_conversation, unique_preserve_order,
    similar, get_user_dir
)

# --- D√©but du module memory_and_quotas.py ---

class MemoryManager:
    """
    G√®re la m√©moire √† court et long terme du bot, ainsi que l'historique des conversations
    pour les utilisateurs individuels et les groupes.
    C'est un singleton pour s'assurer qu'il n'y a qu'une seule instance de gestionnaire de m√©moire.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Impl√©mente le patron de conception Singleton."""
        if cls._instance is None:
            cls._instance = super(cls, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """Initialise les structures de donn√©es pour la m√©moire."""
        if self._initialized:
            return
        self.chat_history: Dict[Union[int, str], List[Dict]] = {} # {user_id: [messages]}
        self.long_term_memory: Dict[Union[int, str], List[str]] = {} # {user_id: [facts]}
        self.ia_status: Dict[str, Dict[str, Any]] = {} # Statut global des IA
        self.group_chat_history: Dict[int, List[Dict]] = {} # {group_id: [messages]}
        # `_initialized` est g√©r√© par `init_manager` pour les op√©rations asynchrones

    async def init_manager(self):
        """
        Initialise le gestionnaire de m√©moire de mani√®re asynchrone.
        Charge les statuts persistants et les historiques de groupe.
        """
        if not self._initialized:
            # Charge le statut global des IA
            self.ia_status = await load_json(IA_STATUS_FILE, {})
            self._initialize_ia_status()
            
            # Charge l'historique du groupe priv√© sp√©cifique
            group_dir = get_user_dir(PRIVATE_GROUP_ID)
            self.group_chat_history[PRIVATE_GROUP_ID] = await load_json(group_dir / GROUP_CHAT_HISTORY_FILE, [])

            self._initialized = True
            log_message("Gestionnaire de m√©moire initialis√©.")

    def _initialize_ia_status(self):
        """
        Initialise ou met √† jour le statut des IA si elles ne sont pas d√©j√† pr√©sentes
        ou si leur statut est obsol√®te. S'assure que toutes les cl√©s n√©cessaires sont l√†.
        """
        updated = False
        now = get_current_time()
        
        for client_name in API_QUOTAS.keys(): # It√®re sur toutes les APIs d√©finies dans les quotas
            if client_name not in self.ia_status:
                self.ia_status[client_name] = {
                    "last_used": None,
                    "last_error": None,
                    "error_count": 0,
                    "cooldown_until": None,
                    "success_count": 0,
                    "current_score": 1.0, # Score initial de 1.0 (parfait)
                    "last_rotation_check": format_datetime(now),
                    "diversification_score": 1.0 # Score de diversification initial de 1.0 (tr√®s diversifiable)
                }
                updated = True
            else:
                # S'assure que toutes les nouvelles cl√©s sont pr√©sentes dans les statuts existants
                default_ia_status_keys = {
                    "last_used": None, "last_error": None, "error_count": 0,
                    "cooldown_until": None, "success_count": 0, "current_score": 1.0,
                    "last_rotation_check": format_datetime(now), "diversification_score": 1.0
                }
                for key, default_value in default_ia_status_keys.items():
                    if key not in self.ia_status[client_name]:
                        self.ia_status[client_name][key] = default_value
                        updated = True
                
                # Met √† jour `last_rotation_check` si trop ancien pour permettre la r√©cup√©ration du score de diversification
                last_check_str = self.ia_status[client_name].get("last_rotation_check")
                if last_check_str:
                    try:
                        last_check_dt = datetime.strptime(last_check_str, "%Y-%m-%d %H:%M:%S UTC")
                        # Si le dernier check est plus ancien que deux fois l'intervalle de rotation, on le r√©initialise
                        if (now - last_check_dt).total_seconds() > API_ROTATION_INTERVAL_MINUTES * 60 * 2:
                            self.ia_status[client_name]["last_rotation_check"] = format_datetime(now)
                            updated = True
                    except ValueError: # G√®re les cha√Ænes de date malform√©es
                        self.ia_status[client_name]["last_rotation_check"] = format_datetime(now)
                        updated = True
                        log_message(f"last_rotation_check malform√© pour {client_name}, r√©initialisation.", level="warning")

        # Supprime les noms d'IA qui ne sont plus d√©finis dans `API_QUOTAS`
        current_api_names = set(API_QUOTAS.keys())
        ia_names_to_remove = [name for name in self.ia_status if name not in current_api_names]
        for name in ia_names_to_remove:
            del self.ia_status[name]
            updated = True
            log_message(f"IA '{name}' trouv√©e dans ia_status.json mais non d√©finie dans API_QUOTAS. Supprim√©e.", level="warning")

        if updated:
            # Sauvegarde l'√©tat mis √† jour de mani√®re asynchrone
            asyncio.create_task(save_json(IA_STATUS_FILE, self.ia_status))
            log_message("Statut des IA initialis√©/mis √† jour.")

    async def add_message_to_history(self, user_id: Union[int, str], role: str, content: str, max_log_entries: int = 100):
        """
        Ajoute un message √† l'historique de la conversation d'un utilisateur.
        G√®re √©galement le log g√©n√©ral de l'utilisateur et le taggage des messages.
        """
        user_dir = get_user_dir(user_id)
        chat_history_path = user_dir / USER_CHAT_HISTORY_FILE
        log_path = user_dir / "log.json" # Fichier de log g√©n√©ral pour l'utilisateur

        # Charge les historiques existants pour l'utilisateur (ou utilise ceux en m√©moire)
        user_chat_hist = self.chat_history.get(user_id, await load_json(chat_history_path, []))
        user_log = await load_json(log_path, [])

        # Neutralise les URLs pour le stockage (s√©curit√©/confidentialit√©)
        neutralized_content = neutralize_urls(content)

        # Ajoute au chat history de l'utilisateur
        user_chat_hist.append({"role": role, "content": neutralized_content, "timestamp": format_datetime(get_current_time())})
        user_chat_hist = user_chat_hist[-max_log_entries:] # Tronque l'historique pour ne garder que les N derniers messages
        self.chat_history[user_id] = user_chat_hist
        asyncio.create_task(save_json(chat_history_path, user_chat_hist))
        
        # Ajoute au log g√©n√©ral de l'utilisateur (contenu tronqu√© pour le log)
        log_entry_content = neutralized_content[:500] # Tronque pour le fichier de log
        log_entry = {"time": format_datetime(get_current_time()), "role": role, "text": log_entry_content}
        if role == "user": # Taggue les messages de l'utilisateur avec des mots-cl√©s
            log_entry["tags"] = tag_conversation(content)
        user_log.append(log_entry)
        user_log = user_log[-max_log_entries:] # Tronque le log g√©n√©ral
        asyncio.create_task(save_json(log_path, user_log))

        log_message(f"Message ajout√© √† l'historique de {user_id} par {role}.")

    async def get_chat_history(self, user_id: Union[int, str], limit: int = 10) -> List[Dict]:
        """
        Retourne les N derniers messages de l'historique de conversation d'un utilisateur.
        Charge l'historique depuis le fichier si non d√©j√† en m√©moire.
        """
        user_dir = get_user_dir(user_id)
        chat_history_path = user_dir / USER_CHAT_HISTORY_FILE
        if user_id not in self.chat_history:
            self.chat_history[user_id] = await load_json(chat_history_path, [])
        return self.chat_history[user_id][-limit:]

    async def save_group_memory(self, group_id: int, role: str, text: str, max_items: int = 1000):
        """
        Sauvegarde l'historique de chat pour un groupe sp√©cifique.
        Utilise l'ID du groupe comme un ID utilisateur pour la structure de r√©pertoire.
        """
        group_dir = get_user_dir(group_id)
        group_history_path = group_dir / GROUP_CHAT_HISTORY_FILE
        
        hist = self.group_chat_history.get(group_id, await load_json(group_history_path, []))
        hist.append({"time": format_datetime(get_current_time()), "role": role, "text": neutralize_urls(text)})
        hist = hist[-max_items:] # Tronque l'historique du groupe
        self.group_chat_history[group_id] = hist
        asyncio.create_task(save_json(group_history_path, hist))
        log_message(f"Message ajout√© √† la m√©moire de groupe {group_id} par {role}.")

    async def get_group_memory(self, group_id: int, limit: int = 20) -> str:
        """
        R√©cup√®re les N derniers messages de la m√©moire de groupe.
        Retourne une cha√Æne format√©e pour √™tre utilis√©e comme contexte.
        """
        group_dir = get_user_dir(group_id)
        group_history_path = group_dir / GROUP_CHAT_HISTORY_FILE
        if group_id not in self.group_chat_history:
            self.group_chat_history[group_id] = await load_json(group_history_path, [])
        
        if not isinstance(self.group_chat_history[group_id], list): # S'assure que c'est une liste
            self.group_chat_history[group_id] = []

        # Filtre les messages du bot si non n√©cessaires pour le contexte du prompt
        recent_messages = [f"{l['role']} : {l['text']}" for l in self.group_chat_history[group_id][-limit:] if l.get("role") != "bot"]
        return "\n".join(recent_messages)

    async def add_to_long_term_memory(self, user_id: Union[int, str], text: str, max_entries: int = 100):
        """
        Ajoute une information √† la m√©moire √† long terme d'un utilisateur.
        D√©doublonne et tronque la m√©moire.
        """
        user_dir = get_user_dir(user_id)
        long_memory_path = user_dir / USER_LONG_MEMORY_FILE
        
        long_mem = self.long_term_memory.get(user_id, await load_json(long_memory_path, []))
        if not isinstance(long_mem, list): # S'assure que c'est une liste
            long_mem = []

        long_mem.append(text.strip())
        long_mem = unique_preserve_order(long_mem)[-max_entries:] # D√©doublonne et tronque
        self.long_term_memory[user_id] = long_mem
        asyncio.create_task(save_json(long_memory_path, long_mem))
        log_message(f"Information ajout√©e √† la m√©moire √† long terme de {user_id}.")

    async def get_long_term_memory(self, user_id: Union[int, str], limit: int = 20) -> str:
        """
        R√©cup√®re les N derni√®res entr√©es de la m√©moire √† long terme d'un utilisateur.
        Retourne une cha√Æne format√©e.
        """
        user_dir = get_user_dir(user_id)
        long_memory_path = user_dir / USER_LONG_MEMORY_FILE
        if user_id not in self.long_term_memory:
            self.long_term_memory[user_id] = await load_json(long_memory_path, [])
        
        if not isinstance(self.long_term_memory[user_id], list): # S'assure que c'est une liste
            self.long_term_memory[user_id] = []

        return "\n".join(self.long_term_memory[user_id][-limit:])

    async def check_for_similar_prompt(self, user_id: Union[int, str], prompt: str) -> Optional[str]:
        """
        V√©rifie si un prompt similaire a d√©j√† √©t√© pos√© r√©cemment et retourne la r√©ponse si trouv√©e.
        Utilise `MAX_CACHE_SIZE` pour la fen√™tre de recherche.
        """
        recent_chat_history = await self.get_chat_history(user_id, limit=MAX_CACHE_SIZE)
        for entry in reversed(recent_chat_history): # Parcours l'historique du plus r√©cent au plus ancien
            if entry.get("role") == "user" and "content" in entry:
                # Compare la similarit√© du prompt actuel avec les anciens prompts utilisateur
                if similar(prompt, entry["content"]) > 0.92: # Seuil de similarit√© (92%)
                    # Si un prompt similaire est trouv√©, cherche la r√©ponse du bot qui suit
                    for i in range(len(recent_chat_history) - 1, -1, -1):
                        if recent_chat_history[i] == entry and i + 1 < len(recent_chat_history):
                            if recent_chat_history[i+1].get("role") == "bot":
                                log_message(f"Prompt similaire d√©tect√© pour {user_id}. R√©ponse en cache utilis√©e.")
                                return recent_chat_history[i+1]["content"]
        return None

    def update_ia_status(self, ia_name: str, success: bool, error_message: Optional[str] = None):
        """
        Met √† jour le statut et le score d'une IA apr√®s une utilisation.
        Ajuste le score de performance et le score de diversification.
        """
        status = self.ia_status.get(ia_name)
        if not status:
            log_message(f"Tentative de mise √† jour d'un statut d'IA inconnu: {ia_name}", level="warning")
            return

        now = get_current_time()
        status["last_used"] = format_datetime(now)

        if success:
            status["success_count"] += 1
            status["error_count"] = 0 # R√©initialise le compteur d'erreurs cons√©cutives
            status["cooldown_until"] = None # Annule le cooldown
            status["last_error"] = None
            status["current_score"] = min(1.0, status["current_score"] + 0.1) # Augmente le score de performance
            # Diminue le score de diversification car l'IA vient d'√™tre utilis√©e
            status["diversification_score"] = max(0.1, status["diversification_score"] - 0.1) 
            log_message(f"IA {ia_name} : Succ√®s enregistr√©. Nouveau score: {status['current_score']:.2f}, Diversification: {status['diversification_score']:.2f}")
        else:
            status["error_count"] += 1
            status["last_error"] = error_message
            if status["error_count"] >= 3: # Si 3 erreurs cons√©cutives ou plus, met en cooldown
                status["cooldown_until"] = format_datetime(now + timedelta(seconds=API_COOLDOWN_DURATION_SECONDS))
                status["current_score"] = max(0.1, status["current_score"] - 0.2) # Diminue plus fortement le score
                log_message(f"IA {ia_name} : Trop d'erreurs ({status['error_count']}). Cooldown jusqu'√† {status['cooldown_until']}. Nouveau score: {status['current_score']:.2f}", level="warning")
            else:
                 status["current_score"] = max(0.1, status["current_score"] - 0.05) # Diminue l√©g√®rement
                 log_message(f"IA {ia_name} : Erreur enregistr√©e. Nouveau score: {status['current_score']:.2f}", level="warning")

        # Sauvegarde l'√©tat mis √† jour de mani√®re asynchrone
        asyncio.create_task(save_json(IA_STATUS_FILE, self.ia_status))

    def recover_diversification_scores(self):
        """
        Augmente le score de diversification pour les IA qui n'ont pas √©t√© utilis√©es r√©cemment.
        Ceci encourage la rotation des APIs.
        """
        now = get_current_time()
        updated = False
        for ia_name, status in self.ia_status.items():
            last_used_str = status.get("last_used")
            if last_used_str:
                try:
                    last_used_dt = datetime.strptime(last_used_str, "%Y-%m-%d %H:%M:%S UTC")
                    # Si pas utilis√©e depuis 2x l'intervalle de rotation, augmente son score de diversification
                    if (now - last_used_dt).total_seconds() > API_ROTATION_INTERVAL_MINUTES * 60 * 2:
                        if status["diversification_score"] < 1.0:
                            status["diversification_score"] = min(1.0, status["diversification_score"] + 0.05)
                            updated = True
                            log_message(f"IA {ia_name}: Score de diversification r√©cup√©r√© √† {status['diversification_score']:.2f}")
                except ValueError: # G√®re les cha√Ænes de date malform√©es
                    status["last_used"] = format_datetime(now) # R√©initialise `last_used`
                    status["diversification_score"] = 1.0 # R√©initialise le score de diversification
                    updated = True
                    log_message(f"last_used malform√© pour {ia_name}, r√©initialisation du score de diversification.", level="warning")
            else: # Si jamais utilis√©e, met √† 1.0 (pleine diversification)
                if status["diversification_score"] < 1.0:
                    status["diversification_score"] = 1.0
                    updated = True
        if updated:
            asyncio.create_task(save_json(IA_STATUS_FILE, self.ia_status))

    def get_ia_status(self, ia_name: str) -> Optional[Dict]:
        """R√©cup√®re le statut d'une IA sp√©cifique."""
        return self.ia_status.get(ia_name)

    def get_available_ias(self) -> List[str]:
        """
        Retourne les noms des IA actuellement non en cooldown.
        """
        available = []
        now = get_current_time()
        for name, status in self.ia_status.items():
            cooldown_until_str = status.get("cooldown_until")
            if cooldown_until_str:
                try:
                    cooldown_until = datetime.strptime(cooldown_until_str, "%Y-%m-%d %H:%M:%S UTC")
                    if now < cooldown_until: # Si toujours en cooldown, on la saute
                        continue
                except ValueError: # Date malform√©e, on la consid√®re comme non en cooldown
                    log_message(f"cooldown_until malform√© pour {name}, consid√©r√© comme non en cooldown.", level="warning")
            available.append(name)
        return available

class QuotaManager:
    """
    G√®re les quotas d'utilisation pour toutes les APIs.
    Suit l'utilisation mensuelle, journali√®re et horaire, et peut alerter en cas de d√©passement.
    Prend √©galement en charge le mode "br√ªlage" de quota.
    C'est un singleton.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Impl√©mente le patron de conception Singleton."""
        if cls._instance is None:
            cls._instance = super(cls, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """Initialise les structures de donn√©es pour les quotas."""
        if self._initialized:
            return
        self.quotas = {}
        # `_initialized` est g√©r√© par `init_manager` pour les op√©rations asynchrones
        self.bot_instance = None # Sera inject√© par main.py pour l'envoi d'alertes

    async def init_manager(self):
        """
        Initialise le gestionnaire de quotas de mani√®re asynchrone.
        Charge les donn√©es de quotas persistantes et s'assure qu'elles sont √† jour.
        """
        if not self._initialized:
            self.quotas = await load_json(QUOTAS_FILE, {})
            self._initialize_quotas()
            self._initialized = True
            log_message("Gestionnaire de quotas initialis√©.")

    def set_bot_instance(self, bot_instance: Any):
        """
        Permet d'injecter l'instance du bot pour envoyer des alertes de quota au groupe priv√©.
        """
        self.bot_instance = bot_instance

    def _initialize_quotas(self):
        """
        Initialise les quotas pour toutes les APIs bas√©es sur `config.API_QUOTAS`.
        Nettoie et met √† jour les entr√©es existantes si n√©cessaire.
        """
        updated = False
        now = get_current_time()

        for api_name, quota_info in API_QUOTAS.items():
            if api_name not in self.quotas:
                self.quotas[api_name] = {
                    "monthly_usage": 0,
                    "daily_usage": 0,
                    "hourly_usage": 0,
                    "hourly_timestamps": [], # Pour un suivi plus pr√©cis de l'heure
                    "last_reset_month": now.month,
                    "last_reset_day": now.day,
                    "last_usage": None,
                    "total_calls": 0,
                    "last_hourly_reset": format_datetime(now)
                }
                updated = True
            else:
                # S'assure que toutes les nouvelles cl√©s sont pr√©sentes dans les donn√©es de quota existantes
                default_quota_structure = {
                    "monthly_usage": 0, "daily_usage": 0, "hourly_usage": 0,
                    "hourly_timestamps": [], "last_reset_month": now.month,
                    "last_reset_day": now.day, "last_usage": None,
                    "total_calls": 0, "last_hourly_reset": format_datetime(now)
                }
                for key, default_value in default_quota_structure.items():
                    if key not in self.quotas[api_name]:
                        self.quotas[api_name][key] = default_value
                        updated = True
                
                # Nettoie `hourly_timestamps` pour les entr√©es existantes (supprime les anciens horodatages)
                if not isinstance(self.quotas[api_name].get("hourly_timestamps"), list):
                    self.quotas[api_name]["hourly_timestamps"] = []
                
                one_hour_ago = now - timedelta(hours=1)
                self.quotas[api_name]["hourly_timestamps"] = [
                    ts for ts in self.quotas[api_name]["hourly_timestamps"]
                    if datetime.strptime(ts, "%Y-%m-%d %H:%M:%S UTC").replace(tzinfo=timezone.utc) > one_hour_ago # Ajoute tzinfo
                ]
                self.quotas[api_name]["hourly_usage"] = len(self.quotas[api_name]["hourly_timestamps"])
                self.quotas[api_name]["last_hourly_reset"] = format_datetime(now)
                updated = True # Marque comme mis √† jour car les timestamps ont √©t√© nettoy√©s

        # Supprime les noms d'API qui ne sont plus d√©finis dans `API_QUOTAS`
        api_names_to_remove = [name for name in self.quotas if name not in API_QUOTAS]
        for name in api_names_to_remove:
            del self.quotas[name]
            updated = True
            log_message(f"API '{name}' trouv√©e dans quotas.json mais non d√©finie dans API_QUOTAS. Supprim√©e.", level="warning")

        if updated:
            asyncio.create_task(save_json(QUOTAS_FILE, self.quotas))
            log_message("Quotas API initialis√©s/mis √† jour.")

    def _reset_quotas_if_needed(self):
        """
        R√©initialise les quotas journaliers, mensuels et horaires si n√©cessaire.
        Cette m√©thode est appel√©e avant chaque v√©rification de quota pour s'assurer de l'actualit√©.
        """
        now = get_current_time()
        for api_name, data in self.quotas.items():
            # R√©initialisation mensuelle
            if now.month != data["last_reset_month"]:
                data["monthly_usage"] = 0
                data["last_reset_month"] = now.month
                log_message(f"Quota mensuel pour {api_name} r√©initialis√©.")
            # R√©initialisation journali√®re
            if now.day != data["last_reset_day"]:
                data["daily_usage"] = 0
                data["last_reset_day"] = now.day
                log_message(f"Quota journalier pour {api_name} r√©initialis√©.")
            
            # R√©initialisation horaire (en nettoyant les anciens horodatages)
            one_hour_ago = now - timedelta(hours=1)
            # S'assure que hourly_timestamps est une liste
            if not isinstance(data.get("hourly_timestamps"), list):
                data["hourly_timestamps"] = []
            
            data["hourly_timestamps"] = [
                ts for ts in data["hourly_timestamps"]
                if datetime.strptime(ts, "%Y-%m-%d %H:%M:%S UTC").replace(tzinfo=timezone.utc) > one_hour_ago
            ]
            data["hourly_usage"] = len(data["hourly_timestamps"])
            data["last_hourly_reset"] = format_datetime(now) # Met √† jour l'heure du dernier reset horaire

        asyncio.create_task(save_json(QUOTAS_FILE, self.quotas))

    async def check_and_update_quota(self, api_name: str, cost: int = 1) -> bool:
        """
        V√©rifie si une API a du quota disponible et le d√©cr√©mente si oui.
        Retourne `True` si l'op√©ration est autoris√©e (quota disponible), `False` sinon.
        """
        self._reset_quotas_if_needed() # S'assure que les quotas sont √† jour

        if api_name not in API_QUOTAS:
            log_message(f"Tentative de v√©rification de quota pour une API non d√©finie: {api_name}. Autorisation refus√©e.", level="error")
            return False

        if api_name not in self.quotas:
            log_message(f"API {api_name} non trouv√©e dans les quotas g√©r√©s. Re-initialisation non bloquante.", level="warning")
            # Ce cas ne devrait pas arriver si _initialize_quotas est correctement appel√© au d√©marrage
            self._initialize_quotas() # R√©-initialise tous les quotas pour s'assurer que cette API est ajout√©e
            if api_name not in self.quotas: # Si toujours pas l√†, il y a un probl√®me grave avec API_QUOTAS
                return False

        quota_data = self.quotas[api_name]
        api_limits = API_QUOTAS.get(api_name, {})
        now = get_current_time()

        # V√©rifie la limite mensuelle
        monthly_limit = api_limits.get("monthly")
        if monthly_limit is not None and (quota_data["monthly_usage"] + cost) > monthly_limit:
            log_message(f"Quota mensuel d√©pass√© pour {api_name}", level="warning")
            await self._alert_quota_if_needed(api_name, "mensuel")
            return False

        # V√©rifie la limite journali√®re
        daily_limit = api_limits.get("daily")
        if daily_limit is not None and (quota_data["daily_usage"] + cost) > daily_limit:
            log_message(f"Quota journalier d√©pass√© pour {api_name}", level="warning")
            await self._alert_quota_if_needed(api_name, "journalier")
            return False
        
        # V√©rifie la limite horaire
        hourly_limit = api_limits.get("hourly")
        if hourly_limit is not None and (quota_data["hourly_usage"] + cost) > hourly_limit:
            log_message(f"Quota horaire d√©pass√© pour {api_name}", level="warning")
            await self._alert_quota_if_needed(api_name, "horaire")
            return False

        # V√©rifie le taux de requ√™tes par seconde
        rate_limit_per_sec = api_limits.get("rate_limit_per_sec")
        if rate_limit_per_sec:
            last_usage_str = quota_data.get("last_usage")
            if last_usage_str:
                try:
                    last_usage = datetime.strptime(last_usage_str, "%Y-%m-%d %H:%M:%S UTC").replace(tzinfo=timezone.utc)
                    time_since_last_call = (now - last_usage).total_seconds()
                    if time_since_last_call < (1 / rate_limit_per_sec):
                        log_message(f"Taux de requ√™tes d√©pass√© pour {api_name}. Attendre {1/rate_limit_per_sec - time_since_last_call:.2f}s", level="warning")
                        return False
                except ValueError: # Date malform√©e, on la consid√®re comme sans utilisation r√©cente
                    log_message(f"last_usage malform√© pour {api_name}, consid√©r√© comme sans utilisation r√©cente pour la limite de taux.", level="warning")

        # Si toutes les v√©rifications passent, met √† jour l'utilisation du quota
        if cost > 0:
            quota_data["monthly_usage"] += cost
            quota_data["daily_usage"] += cost
            quota_data["hourly_usage"] += cost
            quota_data["hourly_timestamps"].append(format_datetime(now)) # Ajoute l'horodatage pour le suivi horaire
            quota_data["total_calls"] += cost
            quota_data["last_usage"] = format_datetime(now)
            asyncio.create_task(save_json(QUOTAS_FILE, self.quotas)) # Sauvegarde de mani√®re asynchrone
            log_message(f"Quota pour {api_name} mis √† jour. Usage mensuel: {quota_data['monthly_usage']}/{monthly_limit if monthly_limit else 'Illimit√©'}, Journalier: {quota_data['daily_usage']}/{daily_limit if daily_limit else 'Illimit√©'}, Horaire: {quota_data['hourly_usage']}/{hourly_limit if hourly_limit else 'Illimit√©'}")
        else:
            log_message(f"Quota pour {api_name} v√©rifi√© (co√ªt 0). Usage mensuel: {quota_data['monthly_usage']}/{monthly_limit if monthly_limit else 'Illimit√©'}, Journalier: {quota_data['daily_usage']}/{daily_limit if daily_limit else 'Illimit√©'}, Horaire: {quota_data['hourly_usage']}/{hourly_limit if hourly_limit else 'Illimit√©'}")

        return True
