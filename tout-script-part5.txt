            return await gemini_client.generate_content(
                prompt=prompt,
                chat_history=chat_history or [],
                image_data=image_data,
                tools=tools
            )
            
        except Exception as e:
            raise Exception(f"Erreur Gemini: {e}")

class DeepSeekBrain(AutonomousBrain):
    """Cerveau autonome basé sur DeepSeek."""
    
    def __init__(self, telegram_client=None):
        super().__init__("DEEPSEEK", "DEEPSEEK", telegram_client)
    
    async def _generate_response(self, prompt: str, key_info: Dict,
                               chat_history: List[Dict] = None,
                               image_data: str = None,
                               tools: List[Dict] = None) -> Dict[str, Any]:
        """Génère une réponse via l'API DeepSeek."""
        try:
            from app_clients_instances import deepseek_client
            
            messages = []
            if chat_history:
                for msg in chat_history:
                    content = " ".join([part.get("text", "") for part in msg.get("parts", [])])
                    if content.strip():
                        role = "assistant" if msg["role"] == "model" else msg["role"]
                        messages.append({"role": role, "content": content})
            
            messages.append({"role": "user", "content": prompt})
            
            result = await deepseek_client.chat_completion(messages=messages)
            
            # Conversion au format Gemini-like
            if isinstance(result, dict) and "choices" in result and result["choices"]:
                content = result["choices"][0]["message"]["content"]
                return {
                    "candidates": [{
                        "content": {"parts": [{"text": content}]}
                    }]
                }
            return {"candidates": [{"content": {"parts": [{"text": str(result)}]}}]}
                
        except Exception as e:
            raise Exception(f"Erreur DeepSeek: {e}")

class HuggingFaceBrain(AutonomousBrain):
    """Cerveau autonome basé sur HuggingFace."""
    
    def __init__(self, telegram_client=None):
        super().__init__("HUGGINGFACE", "HUGGINGFACE", telegram_client)
    
    async def _generate_response(self, prompt: str, key_info: Dict,
                               chat_history: List[Dict] = None,
                               image_data: str = None,
                               tools: List[Dict] = None) -> Dict[str, Any]:
        """Génère une réponse via l'API HuggingFace."""
        try:
            from app_clients_instances import huggingface_client
            
            # Utilise un modèle de génération de texte
            model_name = "microsoft/DialoGPT-large"
            result = await huggingface_client.inference(model_name=model_name, input_text=prompt)
            
            # Conversion au format Gemini-like
            if isinstance(result, list) and result:
                generated_text = result[0].get("generated_text", prompt)
                # Extrait seulement la nouvelle partie générée
                new_text = generated_text[len(prompt):].strip()
                if not new_text:
                    new_text = "Réponse générée par HuggingFace"
                
                return {
                    "candidates": [{
                        "content": {"parts": [{"text": new_text}]}
                    }]
                }
            return {"candidates": [{"content": {"parts": [{"text": str(result)}]}}]}
                
        except Exception as e:
            raise Exception(f"Erreur HuggingFace: {e}")

class TavilyBrain(AutonomousBrain):
    """Cerveau autonome basé sur Tavily."""
    
    def __init__(self, telegram_client=None):
        super().__init__("TAVILY", "TAVILY", telegram_client)
    
    async def _generate_response(self, prompt: str, key_info: Dict,
                               chat_history: List[Dict] = None,
                               image_data: str = None,
                               tools: List[Dict] = None) -> Dict[str, Any]:
        """Génère une réponse via l'API Tavily."""
        try:
            from app_clients_instances import tavily_client
            
            result = await tavily_client.search(query=prompt, max_results=5)
            
            # Traitement des résultats Tavily
            answer = result.get("answer", "")
            results = result.get("results", [])
            
            # Synthèse de la réponse
            synthesis = f"Réponse Tavily: {answer}\n\n"
            if results:
                synthesis += "Sources:\n"
                for i, res in enumerate(results[:3], 1):
                    title = res.get("title", "Sans titre")
                    content = res.get("content", "")[:200]
                    synthesis += f"{i}. {title}: {content}...\n"
            
            return {
                "candidates": [{
                    "content": {"parts": [{"text": neutralize_urls(synthesis)}]}
                }]
            }
                
        except Exception as e:
            raise Exception(f"Erreur Tavily: {e}")

class SerperBrain(AutonomousBrain):
    """Cerveau autonome basé sur Serper."""
    
    def __init__(self, telegram_client=None):
        super().__init__("SERPER", "SERPER", telegram_client)
    
    async def _generate_response(self, prompt: str, key_info: Dict,
                               chat_history: List[Dict] = None,
                               image_data: str = None,
                               tools: List[Dict] = None) -> Dict[str, Any]:
        """Génère une réponse via l'API Serper."""
        try:
            from app_clients_instances import serper_client
            
            result = await serper_client.search(query=prompt)
            
            # Traitement des résultats Serper
            organic = result.get("organic", [])
            answer_box = result.get("answerBox", {})
            
            synthesis = ""
            if answer_box:
                synthesis += f"Réponse directe: {answer_box.get('answer', '')}\n\n"
            
            if organic:
                synthesis += "Résultats de recherche:\n"
                for i, res in enumerate(organic[:3], 1):
                    title = res.get("title", "Sans titre")
                    snippet = res.get("snippet", "")
                    synthesis += f"{i}. {title}: {snippet}\n"
            
            if not synthesis:
                synthesis = "Aucun résultat trouvé pour cette recherche."
            
            return {
                "candidates": [{
                    "content": {"parts": [{"text": neutralize_urls(synthesis)}]}
                }]
            }
                
        except Exception as e:
            raise Exception(f"Erreur Serper: {e}")

class GoogleBrain(AutonomousBrain):
    """Cerveau autonome basé sur Google Custom Search."""
    
    def __init__(self, telegram_client=None):
        super().__init__("GOOGLE", "GOOGLE_CUSTOM_SEARCH", telegram_client)
    
    async def _generate_response(self, prompt: str, key_info: Dict,
                               chat_history: List[Dict] = None,
                               image_data: str = None,
                               tools: List[Dict] = None) -> Dict[str, Any]:
        """Génère une réponse via l'API Google Custom Search."""
        try:
            from app_clients_instances import google_custom_search_client
            
            result = await google_custom_search_client.search(query=prompt)
            
            # Traitement des résultats Google
            items = result.get("items", [])
            
            synthesis = f"Résultats Google pour: {prompt}\n\n"
            if items:
                for i, item in enumerate(items[:3], 1):
                    title = item.get("title", "Sans titre")
                    snippet = item.get("snippet", "")
                    synthesis += f"{i}. {title}: {snippet}\n"
            else:
                synthesis += "Aucun résultat trouvé."
            
            return {
                "candidates": [{
                    "content": {"parts": [{"text": neutralize_urls(synthesis)}]}
                }]
            }
                
        except Exception as e:
            raise Exception(f"Erreur Google: {e}")

class WolframBrain(AutonomousBrain):
    """Cerveau autonome basé sur Wolfram Alpha."""
    
    def __init__(self, telegram_client=None):
        super().__init__("WOLFRAM", "WOLFRAMALPHA", telegram_client)
    
    async def _generate_response(self, prompt: str, key_info: Dict,
                               chat_history: List[Dict] = None,
                               image_data: str = None,
                               tools: List[Dict] = None) -> Dict[str, Any]:
        """Génère une réponse via l'API Wolfram Alpha."""
        try:
            from app_clients_instances import wolfram_alpha_client
            
            result = await wolfram_alpha_client.query(input_text=prompt)
            
            # Traitement des résultats Wolfram
            query_result = result.get("queryresult", {})
            pods = query_result.get("pods", [])
            
            synthesis = f"Résultat Wolfram Alpha pour: {prompt}\n\n"
            if pods:
                for pod in pods[:3]:
                    title = pod.get("title", "")
                    subpods = pod.get("subpods", [])
                    if subpods:
                        text = subpods[0].get("plaintext", "")
                        if text:
                            synthesis += f"{title}: {text}\n"
            else:
                synthesis += "Aucun résultat calculable trouvé."
            
            return {
                "candidates": [{
                    "content": {"parts": [{"text": synthesis}]}
                }]
            }
                
        except Exception as e:
            raise Exception(f"Erreur Wolfram: {e}")

# Fonction factory pour créer les cerveaux
def create_brain(brain_type: str, telegram_client=None) -> AutonomousBrain:
    """Factory pour créer un cerveau du type demandé."""
    brain_classes = {
        "GEMINI": GeminiBrain,
        "DEEPSEEK": DeepSeekBrain,
        "HUGGINGFACE": HuggingFaceBrain,
        "TAVILY": TavilyBrain,
        "SERPER": SerperBrain,
        "GOOGLE_CUSTOM_SEARCH": GoogleBrain,
        "WOLFRAMALPHA": WolframBrain
    }
    
    if brain_type not in brain_classes:
        raise ValueError(f"Type de cerveau non supporté: {brain_type}")
    
    return brain_classes[brain_type](telegram_client)
    
    import asyncio
import json
import random
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Union, Tuple
import logging
from pathlib import Path

from config import config
from utils import log_message, load_json, save_json, get_current_time

class APIKeyLibrary:
    """
    Bibliothèque centralisée pour la gestion des clés API et endpoints.
    Chaque cerveau peut utiliser tous les endpoints de son service.
    """
    def __init__(self):
        self.api_keys = self._initialize_api_keys()
        self.endpoint_rotation = {}
        self.last_rotation_time = {}
        self.failed_endpoints = {}
        
    def _initialize_api_keys(self) -> Dict[str, List[Dict]]:
        """Initialise toutes les clés API avec leurs endpoints."""
        return {
            "GEMINI": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "generate_content",
                            "url": "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent",
                            "method": "POST"
                        },
                        {
                            "name": "list_models",
                            "url": "https://generativelanguage.googleapis.com/v1beta/models",
                            "method": "GET"
                        },
                        {
                            "name": "embed_content",
                            "url": "https://generativelanguage.googleapis.com/v1beta/models/embedding-001:embedContent",
                            "method": "POST"
                        }
                    ]
                } for key in config.GEMINI_API_KEYS
            ],
            "DEEPSEEK": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "chat_completions",
                            "url": "https://api.deepseek.com/chat/completions",
                            "method": "POST"
                        },
                        {
                            "name": "list_models",
                            "url": "https://api.deepseek.com/models",
                            "method": "GET"
                        }
                    ]
                } for key in config.DEEPSEEK_KEYS
            ],
            "HUGGINGFACE": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "inference",
                            "url": "https://api-inference.huggingface.co/models/",
                            "method": "POST"
                        },
                        {
                            "name": "list_models",
                            "url": "https://huggingface.co/api/models",
                            "method": "GET"
                        }
                    ]
                } for key in config.HUGGINGFACE_KEYS
            ],
            "TAVILY": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "search",
                            "url": "https://api.tavily.com/search",
                            "method": "POST"
                        },
                        {
                            "name": "extract",
                            "url": "https://api.tavily.com/extract",
                            "method": "POST"
                        }
                    ]
                } for key in config.TAVILY_KEYS
            ],
            "SERPER": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "search",
                            "url": "https://google.serper.dev/search",
                            "method": "POST"
                        },
                        {
                            "name": "images",
                            "url": "https://google.serper.dev/images",
                            "method": "POST"
                        },
                        {
                            "name": "news",
                            "url": "https://google.serper.dev/news",
                            "method": "POST"
                        }
                    ]
                } for key in config.SERPER_KEYS
            ],
            "GOOGLE_CUSTOM_SEARCH": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "custom_search",
                            "url": "https://www.googleapis.com/customsearch/v1",
                            "method": "GET",
                            "cx": cx
                        } for cx in config.GOOGLE_CX_LIST
                    ]
                } for key in config.GOOGLE_API_KEYS
            ],
            "WOLFRAMALPHA": [
                {
                    "key": key,
                    "endpoints": [
                        {
                            "name": "query",
                            "url": "https://api.wolframalpha.com/v2/query",
                            "method": "GET"
                        },
                        {
                            "name": "simple",
                            "url": "https://api.wolframalpha.com/v1/simple",
                            "method": "GET"
                        }
                    ]
                } for key in config.WOLFRAM_APP_IDS
            ]
        }
    
    def get_available_key(self, service: str) -> Optional[Dict]:
        """Récupère une clé API disponible pour un service donné."""
        if service not in self.api_keys:
            return None
            
        available_keys = []
        for key_info in self.api_keys[service]:
            key_id = f"{service}_{key_info['key'][:8]}"
            if key_id not in self.failed_endpoints or self.failed_endpoints[key_id] < time.time() - 300:
                available_keys.append(key_info)
        
        if not available_keys:
            # Reset failed endpoints if all are failed
            for key_info in self.api_keys[service]:
                key_id = f"{service}_{key_info['key'][:8]}"
                if key_id in self.failed_endpoints:
                    del self.failed_endpoints[key_id]
            available_keys = self.api_keys[service]
        
        return random.choice(available_keys) if available_keys else None
    
    def mark_key_failed(self, service: str, key: str):
        """Marque une clé comme défaillante temporairement."""
        key_id = f"{service}_{key[:8]}"
        self.failed_endpoints[key_id] = time.time()
        log_message(f"Clé {key_id} marquée comme défaillante pour 5 minutes", level="warning")
    
    def get_endpoint(self, service: str, endpoint_name: str) -> Optional[Dict]:
        """Récupère un endpoint spécifique pour un service."""
        key_info = self.get_available_key(service)
        if not key_info:
            return None
            
        for endpoint in key_info["endpoints"]:
            if endpoint["name"] == endpoint_name:
                return {
                    "key": key_info["key"],
                    "endpoint": endpoint
                }
        return None
    
    def rotate_key(self, service: str) -> Optional[Dict]:
        """Force la rotation vers une nouvelle clé pour un service."""
        if service not in self.endpoint_rotation:
            self.endpoint_rotation[service] = 0
        
        if service in self.api_keys and self.api_keys[service]:
            self.endpoint_rotation[service] = (self.endpoint_rotation[service] + 1) % len(self.api_keys[service])
            self.last_rotation_time[service] = time.time()
            return self.api_keys[service][self.endpoint_rotation[service]]
        return None

class BrainMemoryManager:
    """
    Gestionnaire de mémoire pour les 7 cerveaux autonomes.
    Chaque cerveau maintient sa propre mémoire locale et accède à la mémoire partagée.
    """
    def __init__(self, brain_id: str):
        self.brain_id = brain_id
        self.local_memory = {}
        self.shared_memory_file = config.BRAIN_MEMORY_FILE
        self.last_memory_update = time.time()
        
    async def load_memory(self) -> Dict[str, Any]:
        """Charge la mémoire locale et partagée."""
        try:
            shared_memory = await load_json(self.shared_memory_file, {})
            if self.brain_id not in shared_memory:
                shared_memory[self.brain_id] = {
                    "interactions": [],
                    "learned_patterns": {},
                    "success_rate": 1.0,
                    "last_active": datetime.now(timezone.utc).isoformat()
                }
                await save_json(self.shared_memory_file, shared_memory)
            
            self.local_memory = shared_memory[self.brain_id]
            return self.local_memory
        except Exception as e:
            log_message(f"Erreur chargement mémoire pour cerveau {self.brain_id}: {e}", level="error")
            return {}
    
    async def save_memory(self):
        """Sauvegarde la mémoire locale dans le fichier partagé."""
        try:
            shared_memory = await load_json(self.shared_memory_file, {})
            shared_memory[self.brain_id] = self.local_memory
            shared_memory[self.brain_id]["last_active"] = datetime.now(timezone.utc).isoformat()
            await save_json(self.shared_memory_file, shared_memory)
            self.last_memory_update = time.time()
        except Exception as e:
            log_message(f"Erreur sauvegarde mémoire pour cerveau {self.brain_id}: {e}", level="error")
    
    async def add_interaction(self, user_query: str, response: str, tools_used: List[str] = None):
        """Ajoute une interaction à la mémoire."""
        interaction = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "user_query": user_query[:200],  # Limite pour éviter la surcharge mémoire
            "response": response[:500],
            "tools_used": tools_used or [],
            "brain_id": self.brain_id
        }
        
        if "interactions" not in self.local_memory:
            self.local_memory["interactions"] = []
            
        self.local_memory["interactions"].append(interaction)
        
        # Limite le nombre d'interactions en mémoire
        if len(self.local_memory["interactions"]) > 100:
            self.local_memory["interactions"] = self.local_memory["interactions"][-100:]
        
        await self.save_memory()
    
    async def get_relevant_context(self, query: str, limit: int = 5) -> List[Dict]:
        """Récupère le contexte pertinent basé sur la requête."""
        await self.load_memory()
        
        if "interactions" not in self.local_memory:
            return []
        
        # Simple matching basé sur les mots-clés
        query_words = set(query.lower().split())
        relevant_interactions = []
        
        for interaction in self.local_memory["interactions"]:
            interaction_words = set(interaction["user_query"].lower().split())
            if query_words.intersection(interaction_words):
                relevance_score = len(query_words.intersection(interaction_words)) / len(query_words.union(interaction_words))
                relevant_interactions.append((relevance_score, interaction))
        
        # Trie par pertinence et retourne les plus pertinents
        relevant_interactions.sort(key=lambda x: x[0], reverse=True)
        return [interaction for _, interaction in relevant_interactions[:limit]]
    
    async def update_success_rate(self, success: bool):
        """Met à jour le taux de succès du cerveau."""
        if "success_rate" not in self.local_memory:
            self.local_memory["success_rate"] = 1.0
        if "total_attempts" not in self.local_memory:
            self.local_memory["total_attempts"] = 0
        if "successful_attempts" not in self.local_memory:
            self.local_memory["successful_attempts"] = 0
            
        self.local_memory["total_attempts"] += 1
        if success:
            self.local_memory["successful_attempts"] += 1
            
        self.local_memory["success_rate"] = self.local_memory["successful_attempts"] / self.local_memory["total_attempts"]
        await self.save_memory()

class TelegramMemoryIntegration:
    """
    Intégration avec la mémoire du groupe privé Telegram.
    Toutes les interactions sont stockées dans le groupe privé.
    """
    def __init__(self, bot_client):
        self.bot_client = bot_client
        self.group_id = config.PRIVATE_GROUP_ID
        self.memory_cache = []
        self.last_cache_update = 0
        
    async def read_group_memory(self, limit: int = 50) -> str:
        """Lit la mémoire complète du groupe privé Telegram."""
        try:
            if not self.group_id or not self.bot_client:
                return "Mémoire du groupe privé non configurée ou client bot non disponible."
            
            # En production, ici on lirait les messages récents du groupe
            # Pour cette implémentation, on retourne une mémoire simulée
            current_time = datetime.now().isoformat()
            memory_content = f"""
=== MÉMOIRE GROUPE PRIVÉ TELEGRAM ===
Dernière mise à jour: {current_time}
Groupe ID: {self.group_id}

Interactions récentes:
- Traitement de requêtes utilisateur
- Exécution d'outils et analyses
- Génération de défis de codage
- Monitoring de la santé des APIs
- Rotation automatique des cerveaux

Statut système: Opérationnel
Cerveaux actifs: 7 (GEMINI, DEEPSEEK, HUGGINGFACE, TAVILY, SERPER, GOOGLE_CUSTOM_SEARCH, WOLFRAMALPHA)
"""
            return memory_content
        except Exception as e:
            log_message(f"Erreur lecture mémoire groupe: {e}", level="error")
            return "Erreur d'accès à la mémoire du groupe."
    
    async def write_to_group(self, content: str, content_type: str = "info"):
        """Écrit du contenu dans le groupe privé Telegram."""
        try:
            if not self.group_id or not self.bot_client:
                log_message(f"[{content_type.upper()}] {content}")
                return
            
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            formatted_message = f"🧠 [{timestamp}] [{content_type.upper()}]\n{content}"
            
            await self.bot_client.send_message(self.group_id, formatted_message)
            log_message(f"Message écrit dans le groupe privé: {formatted_message[:100]}...")
            
        except Exception as e:
            log_message(f"Erreur écriture groupe: {e}", level="error")
    
    async def log_brain_activity(self, brain_id: str, activity: str, details: Dict = None):
        """Log l'activité d'un cerveau dans le groupe."""
        activity_log = f"Cerveau {brain_id}: {activity}"
        if details:
            try:
                details_str = json.dumps(details, indent=2, ensure_ascii=False)
                activity_log += f"\nDétails: {details_str}"
            except TypeError:
                activity_log += f"\nDétails (non sérialisables): {str(details)}"
        
        await self.write_to_group(activity_log, "BRAIN_ACTIVITY")
    
    async def log_error(self, brain_id: str, error: str):
        """Log une erreur dans le groupe."""
        error_log = f"❌ ERREUR - Cerveau {brain_id}: {error}"
        await self.write_to_group(error_log, "ERROR")
    
    async def log_success(self, brain_id: str, task: str, result: str):
        """Log un succès dans le groupe."""
        success_log = f"✅ SUCCÈS - Cerveau {brain_id}: {task}\nRésultat: {result[:200]}..."
        await self.write_to_group(success_log, "SUCCESS")

class BrainCoordinator:
    """
    Coordinateur pour les 7 cerveaux autonomes.
    Gère la rotation, le basculement automatique et la coordination.
    """
    def __init__(self):
        self.active_brain_index = 0
        self.brain_names = ["GEMINI", "DEEPSEEK", "HUGGINGFACE", "TAVILY", "SERPER", "GOOGLE_CUSTOM_SEARCH", "WOLFRAMALPHA"]
        self.last_rotation = time.time()
        self.brain_health = {brain: True for brain in self.brain_names}
        self.brain_load = {brain: 0 for brain in self.brain_names}
        
    def get_next_brain(self) -> str:
        """Sélectionne le prochain cerveau selon la rotation et la santé."""
        current_time = time.time()
        
        # Rotation automatique toutes les 45 minutes
        if current_time - self.last_rotation >= config.BRAIN_ROTATION_INTERVAL_SECONDS:
            self.active_brain_index = (self.active_brain_index + 1) % len(self.brain_names)
            self.last_rotation = current_time
            log_message(f"Rotation automatique vers le cerveau: {self.brain_names[self.active_brain_index]}")
        
        # Recherche d'un cerveau sain en partant du cerveau actuel
        attempts = 0
        start_index = self.active_brain_index
        while attempts < len(self.brain_names):
            current_brain_name = self.brain_names[self.active_brain_index]
            
            if self.brain_health.get(current_brain_name, True) and self.brain_load.get(current_brain_name, 0) < 5:
                # Avance l'index pour la prochaine requête
                next_index = (self.active_brain_index + 1) % len(self.brain_names)
                self.active_brain_index = next_index
                return current_brain_name
            
            # Passe au cerveau suivant si l'actuel n'est pas disponible
            self.active_brain_index = (self.active_brain_index + 1) % len(self.brain_names)
            attempts += 1
        
        # Si aucun cerveau n'est disponible, utilise le premier par défaut
        log_message("Aucun cerveau optimal trouvé, utilisation du premier disponible", level="warning")
        self.active_brain_index = (start_index + 1) % len(self.brain_names)
        return self.brain_names[start_index]
    
    def mark_brain_failed(self, brain_name: str):
        """Marque un cerveau comme défaillant."""
        self.brain_health[brain_name] = False
        log_message(f"Cerveau {brain_name} marqué comme défaillant", level="warning")
        
        # Auto-récupération après 10 minutes
        asyncio.create_task(self._auto_recover_brain(brain_name))
    
    async def _auto_recover_brain(self, brain_name: str):
        """Récupération automatique d'un cerveau après un délai."""
        await asyncio.sleep(600)  # 10 minutes
        self.brain_health[brain_name] = True
        log_message(f"Cerveau {brain_name} remis en service automatiquement")
    
    def update_brain_load(self, brain_name: str, load_change: int):
        """Met à jour la charge d'un cerveau."""
        if brain_name in self.brain_load:
            self.brain_load[brain_name] = max(0, self.brain_load[brain_name] + load_change)
    
    def get_brain_status(self) -> Dict[str, Any]:
        """Retourne le statut de tous les cerveaux."""
        return {
            "active_brain_for_next_request": self.brain_names[self.active_brain_index],
            "brain_health": self.brain_health.copy(),
            "brain_load": self.brain_load.copy(),
            "last_rotation": datetime.fromtimestamp(self.last_rotation).isoformat(),
            "next_rotation_due": datetime.fromtimestamp(self.last_rotation + config.BRAIN_ROTATION_INTERVAL_SECONDS).isoformat()
        }

# Instances globales
api_key_library = APIKeyLibrary()
brain_coordinator = BrainCoordinator()

import asyncio
import json
import random
import time
import difflib
import hashlib
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, List, Optional
import concurrent.futures
from functools import lru_cache

from config import config
from brain_library import api_key_library, TelegramMemoryIntegration
from autonomous_brain import create_brain
from utils import log_message, save_json

# --- Début des Mocks pour la démonstration (à retirer en production si vos modules existent) ---
# Ces mocks sont ici pour que le code puisse être "exécuté" même sans vos modules complets.
# En production, vous utiliserez les imports réels ci-dessus.

class MockConfig:
    CODING_CHALLENGE_INTERVAL_SECONDS = 900  # 15 minutes
    DAILY_CHALLENGE_PATH = Path("./daily_challenges") # Chemin pour sauvegarder les défis
    def __init__(self):
        self.DAILY_CHALLENGE_PATH.mkdir(parents=True, exist_ok=True)
config = MockConfig()

class MockTelegramMemoryIntegration:
    def __init__(self, client=None):
        pass
    async def write_to_group(self, message, message_type):
        print(f"Telegram Group ({message_type}): {message}")
    async def log_error(self, brain_id, error_message):
        print(f"Telegram Error ({brain_id}): {error_message}")

class MockBrain:
    # Le MockBrain accepte maintenant des configs plus détaillées
    def __init__(self, brain_config: Dict[str, Any], telegram_client=None):
        self.brain_id = brain_config.get("id", "Inconnu") # Utilise un ID unique pour ce cerveau
        self.brain_type = brain_config.get("type", "UNKNOWN")
        self.api_key = brain_config.get("api_key", "NO_KEY")
        self.endpoints = brain_config.get("endpoints", [])
        print(f"MockBrain {self.brain_id} ({self.brain_type}) créé avec clé: {self.api_key[:5]}... et {len(self.endpoints)} endpoints.")

    async def initialize(self):
        print(f"Brain {self.brain_id} initialized.")
    
    async def participate_in_coding_challenge(self, prompt):
        # Simule une génération de code en utilisant les détails du cerveau
        await asyncio.sleep(random.uniform(1, 5)) # Simule le temps de traitement
        if random.random() < 0.1: # 10% de chance d'erreur
            return {"error": "Simulated error", "brain_id": self.brain_id}
        
        code = f"""# Code généré par {self.brain_id} ({self.brain_type})
# Clé utilisée: {self.api_key[:5]}...
# Endpoints: {', '.join(self.endpoints[:2])}... ({len(self.endpoints)} total)
# Défi: {prompt.splitlines()[0]}
def solve_challenge():
    print("Défi résolu par {self.brain_id}!")
    return "Solution simulée"
"""
        return {"code": code, "brain_id": self.brain_id}

# La fonction create_brain doit maintenant accepter la configuration complète du cerveau
def create_brain(brain_config: Dict[str, Any], telegram_client=None):
    # En production, cette fonction dans autonomous_brain.py créerait la vraie instance du cerveau
    # en fonction de brain_config["type"] et lui passerait les autres détails.
    return MockBrain(brain_config, telegram_client)

def log_message(message, level="info"):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] [{level.upper()}] {message}")

async def save_json(file_path, data):
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)
    print(f"JSON saved to {file_path}")

# --- Fin des Mocks ---


class CodingChallengeSystem:
    """
    Système de défis de codage automatisé pour les 7 cerveaux autonomes.
    Génère des défis toutes les 15 minutes et fait participer tous les cerveaux.
    """
    def __init__(self, telegram_client=None):
        self.telegram_client = telegram_client
        self.telegram_memory = TelegramMemoryIntegration(telegram_client)
        self.brains = {}
        self.challenge_history = []
        self.is_running = False
        
        # --- MODIFICATION MAJEURE ICI ---
        # Au lieu d'une liste de types, nous avons une liste de configurations de cerveaux.
        # Chaque configuration représente une instance de cerveau unique avec sa clé et ses endpoints.
        # C'EST ICI QUE VOUS METTREZ VOS 31 CLÉS ET 90 ENDPOINTS.
        # Par exemple, si vous avez 31 clés, vous aurez 31 entrées dans cette liste.
        # Si un cerveau utilise plusieurs endpoints, ils seront listés dans 'endpoints'.
        self.brain_configs = self._generate_example_brain_configs() # Utilise un générateur d'exemples
        
    def _generate_example_brain_configs(self) -> List[Dict[str, Any]]:
        """
        Génère une liste d'exemples de configurations de cerveaux.
        EN PRODUCTION, CETTE LISTE SERAIT CHARGÉE DEPUIS config.py OU brain_library.py
        ET CONTENDRIAIT VOS VRAIES 31 CLÉS ET 90 ENDPOINTS.
        """
        example_configs = []
        brain_base_types = ["GEMINI", "DEEPSEEK", "HUGGINGFACE", "TAVILY", "SERPER", "GOOGLE_CUSTOM_SEARCH", "WOLFRAMALPHA"]
        
        # Pour simuler 31 clés et 90 endpoints, nous allons créer plus d'instances de cerveaux.
        # Chaque "cerveau" dans cette liste aura un ID unique et sa propre clé/endpoints.
        
        # Exemple: 5 instances GEMINI, chacune avec une clé et des endpoints différents
        for i in range(5):
            example_configs.append({
                "id": f"GEMINI_instance_{i+1}",
                "type": "GEMINI",
                "api_key": f"GEMINI_KEY_{i+1}_ABCDEFGH",
                "endpoints": [f"gemini_ep_{j+1}" for j in range(random.randint(2, 5))] # Ex: 2 à 5 endpoints par instance
            })
        
        # Exemple: 4 instances DEEPSEEK
        for i in range(4):
            example_configs.append({
                "id": f"DEEPSEEK_instance_{i+1}",
                "type": "DEEPSEEK",
                "api_key": f"DEEPSEEK_KEY_{i+1}_IJKLMNOP",
                "endpoints": [f"deepseek_ep_{j+1}" for j in range(random.randint(1, 3))]
            })

        # Pour atteindre 31 clés et 90 endpoints, vous devrez étendre cette logique.
        # Voici une approche générique pour simuler le reste:
        remaining_keys = 31 - (5 + 4) # 22 clés restantes
        endpoint_counter = 0

        for i in range(remaining_keys):
            # Assigner aléatoirement un type de cerveau de base
            brain_type = random.choice(brain_base_types)
            num_endpoints = random.randint(1, 4) # Chaque clé aura 1 à 4 endpoints

            endpoints_for_this_brain = []
            for _ in range(num_endpoints):
                endpoints_for_this_brain.append(f"generic_ep_{endpoint_counter}")
                endpoint_counter += 1

            example_configs.append({
                "id": f"{brain_type}_instance_{i+10}", # Pour éviter les conflits d'ID
                "type": brain_type,
                "api_key": f"GENERIC_KEY_{i+10}_QRSTUVWX",
                "endpoints": endpoints_for_this_brain
            })
        
        # Vérifiez que le nombre total d'endpoints est proche de 90 (ceci est un exemple, le vôtre sera précis)
        total_endpoints_in_example = sum(len(c["endpoints"]) for c in example_configs)
        print(f"Exemple généré: {len(example_configs)} cerveaux, environ {total_endpoints_in_example} endpoints.")

        return example_configs

    async def initialize(self):
        """Initialise tous les cerveaux pour les défis."""
        try:
            # --- MODIFICATION ICI : Itérer sur les configurations détaillées ---
            for brain_config in self.brain_configs:
                # create_brain doit maintenant accepter le dictionnaire de configuration complet
                self.brains[brain_config["id"]] = create_brain(brain_config, self.telegram_client)
                await self.brains[brain_config["id"]].initialize()
                
            await self.telegram_memory.write_to_group(
                f"🧠 Système de défis de codage initialisé - {len(self.brains)} cerveaux prêts",
                "SYSTEM_INIT"
            )
            log_message("Système de défis de codage initialisé avec succès")
            return True
            
        except Exception as e:
            log_message(f"Erreur initialisation système défis: {e}", level="error")
            return False
    
    async def start_periodic_challenges(self):
        """Démarre les défis périodiques automatisés."""
        self.is_running = True
        await self.telegram_memory.write_to_group(
            "🚀 Défis de codage automatisés démarrés - Intervalle: 15 minutes",
            "SYSTEM_START"
        )
        
        while self.is_running:
            try:
                await self.run_coding_challenge()
                await asyncio.sleep(config.CODING_CHALLENGE_INTERVAL_SECONDS)
            except Exception as e:
                log_message(f"Erreur dans la boucle de défis: {e}", level="error")
                await asyncio.sleep(60)  # Attendre 1 minute en cas d'erreur
    
    def stop_challenges(self):
        """Arrête les défis périodiques."""
        self.is_running = False
        log_message("Système de défis de codage arrêté")
    
    def generate_challenge_prompt(self) -> str:
        """Génère un prompt de défi de codage aléatoire."""
        challenge_types = [
            "ALGORITHME",
            "OPTIMISATION", 
            "DEBUG",
            "IA_CREATIVE",
            "SCRIPT_UTILE",
            "STRUCTURE_DONNEES",
            "RESOLUTION_PROBLEME",
            "CODE_GOLF"
        ]
        
        algorithms = [
            "tri fusion", "recherche binaire", "parcours graphe", "programmation dynamique",
            "arbre binaire", "table de hachage", "pile et file", "récursivité",
            "backtracking", "greedy algorithm", "dijkstra", "kruskal"
        ]
        
        domains = [
            "traitement de données", "analyse statistique", "manipulation de fichiers",
            "interfaces utilisateur", "API REST", "base de données", "machine learning",
            "traitement d'images", "traitement de texte", "cryptographie", "jeux",
            "automatisation", "web scraping", "calculs mathématiques"
        ]
        
        challenge_type = random.choice(challenge_types)
        algorithm = random.choice(algorithms)
        domain = random.choice(domains)
        
        prompts = {
            "ALGORITHME": f"""
Défi Algorithme - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Implémente un algorithme de {algorithm} optimisé pour {domain}.

Exigences:
- Code Python clair et efficace
- Complexité temporelle O(n log n) maximum
- Gestion des cas limites
- Tests unitaires inclus
- Documentation complète

Contraintes:
- Maximum 150 lignes de code
- Utilisation de structures de données appropriées
- Code prêt pour la production

Génère du code Python fonctionnel et optimisé.
""",
            "OPTIMISATION": f"""
Défi Optimisation - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Optimise ce code pour {domain} en utilisant {algorithm}:

```python
def slow_function(data):
    result = []
    for i in range(len(data)):
        for j in range(len(data)):
            if data[i] > data[j]:
                result.append((i, j))
    return result


Objectifs:

Réduire la complexité temporelle

Minimiser l'usage mémoire

Améliorer la lisibilité

Maintenir la fonctionnalité

Fournis le code optimisé avec explications des améliorations.
""",
"DEBUG": f"""
Défi Debug - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Trouve et corrige les bugs dans ce code pour {domain}:

def process_data(items):
    result = {{}}
    for i, item in enumerate(items):
        if item % 2 == 0:
            result[i] = item * 2
        else:
            result[i] = item / 2
    return result

data = [1, 2, 3, 4, 5]
print(process_data(data))

Tâches:

Identifier tous les bugs

Corriger le code

Ajouter la gestion d'erreurs

Améliorer la robustesse

Fournis le code corrigé et fonctionnel.
""",
"IA_CREATIVE": f"""
Défi IA Créative - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Crée un système intelligent pour {domain} utilisant {algorithm}.

Fonctionnalités requises:

Apprentissage adaptatif

Prédictions précises

Interface intuitive

Visualisation des résultats

Spécifications:

Code modulaire et extensible

Documentation technique

Exemples d'utilisation

Tests de validation

Génère un système IA complet et innovant.
""",
"SCRIPT_UTILE": f"""
Défi Script Utile - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Développe un script pratique pour automatiser {domain}.

Caractéristiques:

Interface en ligne de commande

Configuration par fichier

Logging détaillé

Gestion d'erreurs robuste

Fonctionnalités:

Traitement par lots

Sauvegarde automatique

Rapports de progression

Mode debug

Crée un outil prêt à l'emploi et professionnel.
""",
"STRUCTURE_DONNEES": f"""
Défi Structure de Données - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Implémente une structure de données avancée pour {domain}.

Opérations requises:

Insertion O(log n)

Recherche O(log n)

Suppression O(log n)

Parcours efficace

Bonus:

Sérialisation/désérialisation

Opérations de masse

Thread-safety

Visualisation

Fournis une implémentation complète et testée.
""",
"RESOLUTION_PROBLEME": f"""
Défi Résolution de Problème - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Résous ce problème complexe pour {domain}:

Problème: Tu as une liste de tâches avec des dépendances. Chaque tâche a une durée et des prérequis.
Trouve l'ordre d'exécution optimal qui minimise le temps total tout en respectant les contraintes.

Contraintes:

Maximum 3 tâches en parallèle

Certaines tâches sont critiques (priorité haute)

Gestion des conflits de ressources

Fournis l'algorithme de planification optimal.
""",
"CODE_GOLF": f"""
Défi Code Golf - {datetime.now().strftime('%Y-%m-%d %H:%M')}

Implémente {algorithm} en moins de 50 caractères Python.

Règles:

Fonctionnalité complète préservée

Code lisible malgré la concision

Pas de caractères Unicode exotiques

Commentaire explicatif obligatoire

Objectif: Code le plus court possible tout en restant pythonique.
"""
}
        return prompts.get(challenge_type, prompts["ALGORITHME"])

    async def run_coding_challenge(self):
        """Exécute un défi de codage avec tous les cerveaux."""
        try:
            # Génération du défi
            challenge_prompt = self.generate_challenge_prompt()
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            await self.telegram_memory.write_to_group(
                f"🎯 NOUVEAU DÉFI DE CODAGE - {timestamp}\n\n{challenge_prompt}",
                "CHALLENGE_START"
            )
            
            log_message(f"Lancement du défi de codage: {timestamp}")
            
            # Exécution en parallèle pour tous les cerveaux
            tasks = []
            # --- MODIFICATION ICI : Itérer sur les cerveaux déjà initialisés ---
            for brain_id, brain_instance in self.brains.items():
                task = asyncio.create_task(
                    self._brain_challenge_task(brain_instance, challenge_prompt, timestamp)
                )
                tasks.append((brain_id, task))
            
            # Attente des résultats
            results = {}
            for brain_id, task in tasks:
                try:
                    result = await asyncio.wait_for(task, timeout=300)  # 5 minutes max
                    results[brain_id] = result
                except asyncio.TimeoutError:
                    results[brain_id] = {"error": "Timeout", "brain_id": brain_id}
                    await self.telegram_memory.log_error(brain_id, "Timeout défi de codage")
                except Exception as e:
                    results[brain_id] = {"error": str(e), "brain_id": brain_id}
                    await self.telegram_memory.log_error(brain_id, f"Erreur défi: {e}")
            
            # Sauvegarde et analyse des résultats
            await self._save_challenge_results(challenge_prompt, results, timestamp)
            await self._analyze_and_report_results(results, timestamp)
            
            # Mise à jour de l'historique
            self.challenge_history.append({
                "timestamp": timestamp,
                "challenge": challenge_prompt[:100] + "...",
                "participants": len(results),
                "successful": len([r for r in results.values() if "error" not in r])
            })
            
            # Limitation de l'historique
            if len(self.challenge_history) > 50:
                self.challenge_history = self.challenge_history[-50:]
            
        except Exception as e:
            log_message(f"Erreur défi de codage: {e}", level="error")
            await self.telegram_memory.log_error("SYSTEM", f"Erreur défi global: {e}")

    async def _brain_challenge_task(self, brain: Any, challenge_prompt: str, timestamp: str) -> Dict[str, Any]:
        """Tâche de défi pour un cerveau spécifique."""
        try:
            # Le cerveau participe au défi
            result = await brain.participate_in_coding_challenge(challenge_prompt)
            
            if "error" not in result and "code" in result:
                # Sauvegarde du code généré
                code_filename = config.DAILY_CHALLENGE_PATH / f"challenge_{brain.brain_id}_{timestamp}.py"
                
                header = f"""# -*- coding: utf-8 -*-
# Défi de codage automatisé
# Cerveau: {brain.brain_id}
# Timestamp: {timestamp}
# Challenge: {challenge_prompt[:100]}...

"""
                try:
                    with open(code_filename, "w", encoding="utf-8") as f:
                        f.write(header + result["code"])
                    
                    result["saved_file"] = str(code_filename)
                    log_message(f"Code sauvegardé: {code_filename}")
                    
                except Exception as e:
                    log_message(f"Erreur sauvegarde {brain.brain_id}: {e}", level="error")
            
            return result
            
        except Exception as e:
            return {"error": str(e), "brain_id": brain.brain_id}

    async def _save_challenge_results(self, challenge_prompt: str, results: Dict, timestamp: str):
        """Sauvegarde tous les résultats du défi."""
        try:
            challenge_data = {
                "timestamp": timestamp,
                "challenge_prompt": challenge_prompt,
                "results": results,
                "summary": {
                    "total_participants": len(results),
                    "successful_responses": len([r for r in results.values() if "error" not in r]),
                    "failed_responses": len([r for r in results.values() if "error" in r]),
                    "generated_files": [r.get("saved_file") for r in results.values() if r.get("saved_file")]
                }
            }
            
            results_file = config.DAILY_CHALLENGE_PATH / f"challenge_results_{timestamp}.json"
            await save_json(results_file, challenge_data)
            
            log_message(f"Résultats du défi sauvegardés: {results_file}")
            
        except Exception as e:
            log_message(f"Erreur sauvegarde résultats: {e}", level="error")

    async def _analyze_and_report_results(self, results: Dict, timestamp: str):
        """Analyse et rapporte les résultats dans le groupe Telegram."""
        try:
            successful = [r for r in results.values() if "error" not in r]
            failed = [r for r in results.values() if "error" in r]
            
            report = f"""
📊 RAPPORT DÉFI DE CODAGE - {timestamp}

✅ Succès: {len(successful)}/{len(results)} cerveaux
❌ Échecs: {len(failed)}/{len(results)} cerveaux

=== DÉTAILS SUCCÈS ===
"""
            for result in successful:
                brain_id = result.get("brain_id", "Inconnu")
                code_length = len(result.get("code", ""))
                report += f"• {brain_id}: {code_length} caractères générés\n"
            
            if failed:
                report += "\n=== DÉTAILS ÉCHECS ===\n"
                for result in failed:
                    brain_id = result.get("brain_id", "Inconnu")
                    error = result.get("error", "Erreur inconnue")
                    report += f"• {brain_id}: {error}\n"
            
            # Sélection du meilleur code
            if successful:
                best_result = max(successful, key=lambda x: len(x.get("code", "")))
                best_brain = best_result.get("brain_id", "Inconnu")
                best_code = best_result.get("code", "")[:500]
                
                report += f"\n🏆 MEILLEUR CODE - {best_brain}:\n```python\n{best_code}...\n```"
            
            await self.telegram_memory.write_to_group(report, "CHALLENGE_REPORT")
            
        except Exception as e:
            log_message(f"Erreur analyse résultats: {e}", level="error")

    def get_challenge_statistics(self) -> Dict[str, Any]:
        """Retourne les statistiques des défis."""
        if not self.challenge_history:
            return {"total_challenges": 0}
        
        total_challenges = len(self.challenge_history)
        total_participants = sum(c["participants"] for c in self.challenge_history)
        total_successful = sum(c["successful"] for c in self.challenge_history)
        
        avg_success_rate = (total_successful / total_participants * 100) if total_participants > 0 else 0
        
        return {
            "total_challenges": total_challenges,
            "total_participants": total_participants,
            "total_successful": total_successful,
            "average_success_rate": round(avg_success_rate, 2),
            "last_challenge": self.challenge_history[-1] if self.challenge_history else None,
            "is_running": self.is_running
        }

# Fonctions utilitaires pour le système de défis

def diff_text(old_text: str, new_text: str) -> str:
    """Génère un diff unifié entre deux textes."""
    diff = difflib.unified_diff(
        old_text.splitlines(),
        new_text.splitlines(),
        lineterm=""
    )
    return "\n".join(diff)

def analyze_python_code(code: str) -> Dict[str, Any]:
    """Analyse un code Python et retourne des métriques."""
    try:
        lines = code.split('\n')
        non_empty_lines = [line for line in lines if line.strip()]

        # Métriques basiques
        metrics = {
            "total_lines": len(lines),
            "code_lines": len(non_empty_lines),
            "comment_lines": len([line for line in lines if line.strip().startswith('#')]),
            "blank_lines": len(lines) - len(non_empty_lines),
            "functions": len([line for line in lines if line.strip().startswith('def ')]),
            "classes": len([line for line in lines if line.strip().startswith('class ')]),
            "imports": len([line for line in lines if line.strip().startswith(('import ', 'from '))]),
            "complexity_score": calculate_complexity(code)
        }
        
        # Vérification syntaxique
        try:
            compile(code, '<string>', 'exec')
            metrics["syntax_valid"] = True
            metrics["syntax_error"] = None
        except SyntaxError as e:
            metrics["syntax_valid"] = False
            metrics["syntax_error"] = str(e)
        
        return metrics
        
    except Exception as e:
        return {"error": f"Erreur analyse: {e}"}

def calculate_complexity(code: str) -> int:
    """Calcule un score de complexité approximatif."""
    complexity_keywords = [
        'if', 'elif', 'else', 'for', 'while', 'try', 'except',
        'with', 'def', 'class', 'lambda', 'and', 'or'
    ]
    complexity = 1  # Base complexity
    for line in code.split('\n'):
        line = line.strip().lower()
        for keyword in complexity_keywords:
            if keyword in line:
                complexity += 1
    return complexity

def format_error(error: Exception) -> str:
    """Formate une erreur de manière visuelle."""
    return f"""
⚠️⚠️⚠️ ERREUR ⚠️⚠️⚠️
Type: {type(error).__name__}
Message: {str(error)}
——————————————
"""

@lru_cache(maxsize=100)
def generate_code_cached(prompt: str, temperature: float = 0.7) -> str:
    """Génère du code avec cache pour les prompts répétés."""
    # Cette fonction serait connectée à un modèle IA en production
    return f"# Code généré pour: {prompt[:50]}...\nprint('Code généré avec cache')"

def batch_generate(prompts: List[str], max_workers: int = 4) -> List[str]:
    """Génère du code pour plusieurs prompts en parallèle."""
    def _generate_single(prompt):
        return f"# Code généré pour: {prompt[:50]}...\nprint('Code généré en lot')"

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        return list(executor.map(_generate_single, prompts))

def adaptive_temp(prompt: str) -> float:
    """Adapte la température selon le type de prompt."""
    technical_keywords = ["optimiser", "algorithme", "complexité", "performance", "debug"]
    return 0.3 if any(kw in prompt.lower() for kw in technical_keywords) else 0.7

def fix_common_errors(code: str) -> str:
    """Applique des corrections automatiques communes."""
    fixes = {
        "print(": "print(",
        "def ": "def ",
        "= =": "==",
        "elif ": "elif ",
        "esle:": "else:",
        "ture": "True",
        "flase": "False"
    }
    for error, fix in fixes.items():
        code = code.replace(error, fix)
    return code

def warmup_ai(model, iterations: int = 3):
    """Préchauffe un modèle IA avec des requêtes factices."""
    dummy_prompts = ["print('hello')", "def test(): pass", "1+1"]
    for _ in range(iterations):
        for prompt in dummy_prompts:
            if hasattr(model, 'generate'):
                try:
                    model.generate(prompt)
                except:
                    pass # Ignore les erreurs de préchauffage

# Instance globale du système de défis

coding_challenge_system = None

def get_coding_challenge_system(telegram_client=None) -> CodingChallengeSystem:
    """Retourne l'instance globale du système de défis."""
    global coding_challenge_system
    if coding_challenge_system is None:
        coding_challenge_system = CodingChallengeSystem(telegram_client)
    return coding_challenge_system

import asyncio
import hashlib
import io
import re
import time
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any
from urllib.parse import urlparse
import httpx

from config import config
from brain_library import TelegramMemoryIntegration
from utils import log_message

class URLDefanger:
    """
    Neutralise les URLs pour empêcher les clics accidentels
    et bloque les trackers connus.
    """
    def __init__(self, mode: str = "secure"):
        self.mode = mode
        self.url_pattern = re.compile(r'https?://[^\s\]]+')
        self.tracker_domains = [
            "doubleclick.net", "googleadservices.com", "googlesyndication.com",
            "facebook.com/tr", "analytics.google.com", "hotjar.com",
            "mouseflow.com", "crazyegg.com", "fullstory.com"
        ]
    
    def _generate_hash(self, url: str) -> str:
        """Génère un identifiant unique pour l'URL."""
        return hashlib.sha256(url.encode()).hexdigest()[:8]
    
    def defang_url(self, url: str) -> str:
        """Transforme une URL en version sécurisée."""
        # Bloque les trackers connus
        for tracker in self.tracker_domains:
            if tracker in url:
                return "[TRACKER_BLOQUÉ]"
        
        if self.mode == "secure":
            return f"[URL_BLOQUÉE:#{self._generate_hash(url)}]"
        else:
            parsed = urlparse(url)
            return f"[URL:{parsed.netloc}/...#{self._generate_hash(url)}]"
    
    def defang_text(self, text: str) -> str:
        """Nettoie tout le contenu texte."""
        return self.url_pattern.sub(
            lambda m: self.defang_url(m.group(0)), 
            text
        )

class SecurePageArchiver:
    """
    Télécharge, sécurise et archive des pages web
    avec gestion des gros fichiers et protection anti-tracking.
    """
    def __init__(self, telegram_client=None):
        self.telegram_client = telegram_client
        self.telegram_memory = TelegramMemoryIntegration(telegram_client)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept-Language": "fr-FR,fr;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
    async def fetch_page(self, url: str) -> Optional[httpx.Response]:
        """Télécharge une page avec gestion robuste des erreurs."""
        try:
            async with httpx.AsyncClient(
                timeout=30.0,
                headers=self.headers,
                follow_redirects=True,
                http2=True,
                limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
            ) as client:
                response = await client.get(url)
                
                # Vérification de la taille du contenu
                content_length = response.headers.get('content-length')
                if content_length and int(content_length) > config.MAX_CHUNK_SIZE:
                    log_message(f"Contenu trop volumineux pour {url}: {content_length} bytes", level="warning")
                    return None
                
                response.raise_for_status()
                return response
                
        except httpx.HTTPStatusError as e:
            log_message(f"Erreur HTTP pour {url}: {e.response.status_code}", level="warning")
            return None
        except httpx.RequestError as e:
            log_message(f"Erreur réseau pour {url}: {e}", level="warning")
            return None
        except Exception as e:
            log_message(f"Erreur inattendue pour {url}: {e}", level="error")
            return None

    async def secure_content(self, url: str, content: str) -> str:
        """Applique les protections de sécurité au contenu."""
        defanger = URLDefanger(mode="secure")
        
        header = f"""
⚠️ CONTENU ARCHIVÉ - LIENS NEUTRALISÉS ⚠️
URL originale: {url}
Horodatage: {datetime.utcnow().isoformat()}Z
Taille originale: {len(content)} caractères
Sécurisé par: SecurePageArchiver v2.0
=====================================

"""
        
        # Neutralisation des URLs
        secured_content = defanger.defang_text(content)
        
        # Suppression des scripts malveillants
        secured_content = re.sub(r'<script[^>]*>.*?</script>', '[SCRIPT_SUPPRIMÉ]', secured_content, flags=re.DOTALL | re.IGNORECASE)
        secured_content = re.sub(r'javascript:[^"\']*', '[JAVASCRIPT_BLOQUÉ]', secured_content, flags=re.IGNORECASE)
        secured_content = re.sub(r'on\w+\s*=\s*["\'][^"\']*["\']', '[EVENT_HANDLER_BLOQUÉ]', secured_content, flags=re.IGNORECASE)
        
        return header + secured_content

    async def send_chunk(self, chunk: str, url: str, user_id: str, chunk_index: int) -> bool:
        """Envoie un fragment de contenu sécurisé via Telegram."""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"SAFE_{user_id}_{timestamp}_p{chunk_index:03d}.txt"
            
            # Limitation de la taille du chunk pour Telegram
            if len(chunk) > 4000:  # Limite Telegram pour les messages
                chunk = chunk[:4000] + "\n\n[CONTENU_TRONQUÉ]"
            
            # Envoi dans le groupe privé
            await self.telegram_memory.write_to_group(
                f"📄 Fragment {chunk_index+1} | {url[:50]}...\n\n{chunk}",
                "ARCHIVE_CHUNK"
            )
            
            log_message(f"Fragment {chunk_index} envoyé pour {url}")
            return True
            
        except Exception as e:
            log_message(f"Erreur envoi fragment {chunk_index}: {e}", level="error")
            return False

    async def archive_single_page(self, url: str, user_id: str) -> Dict[str, Any]:
        """Archive une seule page web."""
        start_time = time.time()
        result = {
            "url": url,
            "user_id": user_id,
            "timestamp": datetime.now().isoformat(),
            "success": False,
            "chunks_sent": 0,
            "total_size": 0,
            "processing_time": 0,
            "error": None
        }
        
        try:
            # Log du début d'archivage
            await self.telegram_memory.log_brain_activity(
                "ARCHIVER",
                f"Début archivage: {url}",
                {"user_id": user_id}
            )
            
            # Téléchargement de la page
            response = await self.fetch_page(url)
            if not response:
                result["error"] = "Échec du téléchargement"
                return result
            
            # Vérification du type de contenu
            content_type = response.headers.get('content-type', '').lower()
            if not any(ct in content_type for ct in ['text/html', 'text/plain', 'application/json']):
                result["error"] = f"Type de contenu non supporté: {content_type}"
                return result
            
            # Sécurisation du contenu
            raw_content = response.text
            result["total_size"] = len(raw_content)
            
            secured_content = await self.secure_content(url, raw_content)
            
            # Découpage en chunks
            chunks = []
            chunk_size = config.MAX_CHUNK_SIZE // 2  # Plus petit pour la sécurité Telegram
            
            for i in range(0, len(secured_content), chunk_size):
                chunk = secured_content[i:i + chunk_size]
                chunks.append(chunk)
            
            # Envoi des chunks
            successful_chunks = 0
            for i, chunk in enumerate(chunks):
                if await self.send_chunk(chunk, url, user_id, i):
                    successful_chunks += 1
                    await asyncio.sleep(0.5)  # Éviter le rate limiting Telegram
                else:
                    break
            
            result["chunks_sent"] = successful_chunks
            result["success"] = successful_chunks > 0
            
            # Log de fin
            processing_time = time.time() - start_time
            result["processing_time"] = processing_time
            
            await self.telegram_memory.log_success(
                "ARCHIVER",
                f"Page archivée: {url}",
                f"{successful_chunks} fragments envoyés"
            )
            
        except Exception as e:
            result["error"] = str(e)
            await self.telegram_memory.log_error("ARCHIVER", f"Erreur archivage {url}: {e}")
        
        return result

class ArchiveCoordinator:
    """
    Coordinateur pour l'archivage de multiples pages en parallèle.
    """
    def __init__(self, telegram_client=None, max_concurrent: int = 3):
        self.archiver = SecurePageArchiver(telegram_client)
        self.max_concurrent = max_concurrent
        self.telegram_memory = TelegramMemoryIntegration(telegram_client)
        
    async def archive_multiple_pages(self, links: List[str], user_id: str) -> Dict[str, Any]:
        """Archive plusieurs pages en parallèle avec limitation de concurrence."""
        start_time = time.time()
        
        await self.telegram_memory.write_to_group(
            f"🚀 Début archivage de {len(links)} pages pour l'utilisateur {user_id}",
            "ARCHIVE_START"
        )
        
        # Limitation du nombre de liens
        if len(links) > 10:
            links = links[:10]
            await self.telegram_memory.write_to_group(
                "⚠️ Limitation appliquée: maximum 10 liens par session",
                "ARCHIVE_LIMIT"
            )
        
        # Semaphore pour limiter la concurrence
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def archive_with_semaphore(url: str) -> Dict[str, Any]:
            async with semaphore:
                return await self.archiver.archive_single_page(url, user_id)
        
        # Exécution en parallèle
        tasks = [archive_with_semaphore(url) for url in links]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Compilation des résultats
        summary = {
            "total_links": len(links),
            "successful": 0,
            "failed": 0,
            "total_chunks": 0,
            "total_size": 0,
            "processing_time": time.time() - start_time,
            "results": []
        }
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                error_result = {
                    "url": links[i] if i < len(links) else "Unknown",
                    "success": False,
                    "error": str(result)
                }
                summary["results"].append(error_result)
                summary["failed"] += 1
            else:
                summary["results"].append(result)
                if result["success"]:
                    summary["successful"] += 1
                    summary["total_chunks"] += result["chunks_sent"]
                    summary["total_size"] += result["total_size"]
                else:
                    summary["failed"] += 1
        
        # Rapport final
        await self.telegram_memory.write_to_group(
            f"""
📊 RAPPORT D'ARCHIVAGE TERMINÉ

👤 Utilisateur: {user_id}
📊 Statistiques:
  • Total: {summary['total_links']} liens
  • Succès: {summary['successful']} pages
  • Échecs: {summary['failed']} pages
  • Fragments: {summary['total_chunks']} envoyés
  • Taille: {summary['total_size']:,} caractères
  • Durée: {summary['processing_time']:.2f}s

✅ Toutes les pages sont sécurisées et archivées dans ce groupe.
""",
            "ARCHIVE_COMPLETE"
        )
        
        return summary

# Fonction principale pour l'archivage (utilisée par les outils)
async def fetch_and_archive_pages(links: List[str], user_id: str, context=None) -> Dict[str, Any]:
    """
    Fonction principale pour télécharger, sécuriser et archiver des pages web.
    
    Args:
        links: Liste des URLs à archiver
        user_id: Identifiant de l'utilisateur
        context: Contexte (non utilisé, pour compatibilité)
    
    Returns:
        Dict contenant le résumé de l'archivage
    """
    try:
        # Import du client Telegram depuis les instances
        from app_clients_instances import telegram_bot_client
        
        coordinator = ArchiveCoordinator(telegram_bot_client, max_concurrent=3)
        summary = await coordinator.archive_multiple_pages(links, user_id)
        
        return {
            "tool_output": f"✅ Archivage terminé: {summary['successful']}/{summary['total_links']} pages archivées avec succès. {summary['total_chunks']} fragments envoyés dans le groupe privé.",
            "summary": summary
        }
        
    except Exception as e:
        error_msg = f"❌ Erreur système d'archivage: {e}"
        log_message(f"Erreur fetch_and_archive_pages: {e}", level="error")
        return {
            "tool_output": error_msg,
            "error": str(e)
        }

# Classes utilitaires supplémentaires

class ContentAnalyzer:
    """Analyse le contenu des pages archivées."""
    
    @staticmethod
    def extract_metadata(content: str, url: str) -> Dict[str, Any]:
        """Extrait les métadonnées d'une page."""
        metadata = {
            "url": url,
            "timestamp": datetime.now().isoformat(),
            "size": len(content),
            "title": "",
            "description": "",
            "language": "unknown",
            "charset": "unknown"
        }
        
        # Extraction du titre
        title_match = re.search(r'<title[^>]*>(.*?)</title>', content, re.IGNORECASE | re.DOTALL)
        if title_match:
            metadata["title"] = title_match.group(1).strip()[:200]
        
        # Extraction de la description
        desc_match = re.search(r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']', content, re.IGNORECASE)
        if desc_match:
            metadata["description"] = desc_match.group(1).strip()[:500]
        
        # Extraction de la langue
        lang_match = re.search(r'<html[^>]*lang=["\']([^"\']*)["\']', content, re.IGNORECASE)
        if lang_match:
            metadata["language"] = lang_match.group(1).strip()
        
        return metadata
    
    @staticmethod
    def count_elements(content: str) -> Dict[str, int]:
        """Compte les éléments HTML dans le contenu."""
        elements = {
            "links": len(re.findall(r'<a[^>]*href=', content, re.IGNORECASE)),
            "images": len(re.findall(r'<img[^>]*src=', content, re.IGNORECASE)),
            "scripts": len(re.findall(r'<script[^>]*>', content, re.IGNORECASE)),
            "forms": len(re.findall(r'<form[^>]*>', content, re.IGNORECASE)),
            "paragraphs": len(re.findall(r'<p[^>]*>', content, re.IGNORECASE)),
            "headings": len(re.findall(r'<h[1-6][^>]*>', content, re.IGNORECASE))
        }
        
        return elements

class ArchiveStorage:
    """Gestionnaire de stockage pour les archives."""
    
    def __init__(self):
        self.archive_dir = config.BASE_DIR / "archives"
        self.archive_dir.mkdir(exist_ok=True)
        
    async def save_archive_metadata(self, user_id: str, summary: Dict[str, Any]):
        """Sauvegarde les métadonnées d'archivage."""
        try:
            metadata_file = self.archive_dir / f"archive_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            # Import de save_json depuis utils
            from utils import save_json
            await save_json(metadata_file, summary)
            
            log_message(f"Métadonnées d'archivage sauvegardées: {metadata_file}")
            
        except Exception as e:
            log_message(f"Erreur sauvegarde métadonnées: {e}", level="error")
    
    def get_user_archives(self, user_id: str) -> List[Path]:
        """Récupère la liste des archives d'un utilisateur."""
        pattern = f"archive_{user_id}_*.json"
        return list(self.archive_dir.glob(pattern))
    
    def cleanup_old_archives(self, days: int = 30):
        """Nettoie les anciennes archives."""
        cutoff_time = time.time() - (days * 24 * 60 * 60)
        
        for archive_file in self.archive_dir.glob("archive_*.json"):
            if archive_file.stat().st_mtime < cutoff_time:
                try:
                    archive_file.unlink()
                    log_message(f"Archive supprimée: {archive_file}")
                except Exception as e:
                    log_message(f"Erreur suppression archive {archive_file}: {e}", level="error")

# Instance globale du stockage
archive_storage = ArchiveStorage()

import asyncio
import json
import re
import base64
import traceback
from typing import Dict, Any, List, Optional, Union, Tuple

from app_singletons import endpoint_health_manager, quota_manager
from config import config
from utils import log_message, neutralize_urls, find_tool_by_name

# Import des clients API optimisé pour éviter les dépendances circulaires
def get_api_clients():
    """Import dynamique des clients API."""
    try:
        from app_clients_instances import (
            webcontainer_client, ocr_client, deepseek_client, serper_client,
            wolfram_alpha_client, tavily_client, apiflash_client, crawlbase_client,
            detect_language_client, guardian_client, ip2location_client, shodan_client,
            weather_api_client, cloudmersive_client, greynoise_client, pulsedive_client,
            stormglass_client, loginradius_client, jsonbin_client, huggingface_client,
            twilio_client, abstractapi_client, google_custom_search_client,
            randommer_client, tomorrow_io_client, openweathermap_client, mockaroo_client,
            openpagerank_client, rapidapi_client, telegram_bot_client
        )
        return {
            'webcontainer': webcontainer_client,
            'ocr': ocr_client,
            'deepseek': deepseek_client,
            'serper': serper_client,
            'wolfram': wolfram_alpha_client,
            'tavily': tavily_client,
            'apiflash': apiflash_client,
            'crawlbase': crawlbase_client,
            'detect_language': detect_language_client,
            'guardian': guardian_client,
            'ip2location': ip2location_client,
            'shodan': shodan_client,
            'weather_api': weather_api_client,
            'cloudmersive': cloudmersive_client,
            'greynoise': greynoise_client,
            'pulsedive': pulsedive_client,
            'stormglass': stormglass_client,
            'loginradius': loginradius_client,
            'jsonbin': jsonbin_client,
            'huggingface': huggingface_client,
            'twilio': twilio_client,
            'abstractapi': abstractapi_client,
            'google_search': google_custom_search_client,
            'randommer': randommer_client,
            'tomorrow_io': tomorrow_io_client,
            'openweathermap': openweathermap_client,
            'mockaroo': mockaroo_client,
            'openpagerank': openpagerank_client,
            'rapidapi': rapidapi_client,
            'telegram': telegram_bot_client
        }
    except ImportError as e:
        log_message(f"Erreur import clients API: {e}", level="warning")
        return {}

async def execute_tool(tool_name: str, context: Any = None, **kwargs) -> Dict[str, Any]:
    """
    Exécute un outil spécifique en fonction de son nom et des arguments fournis.
    Compatible avec les 7 cerveaux autonomes.
    """
    log_message(f"Exécution de l'outil: {tool_name} avec kwargs: {kwargs}")
    tool_config_info = find_tool_by_name(tool_name)

    if not tool_config_info:
        log_message(f"Outil non trouvé: {tool_name}", level="error")
        return {"tool_name": tool_name, "tool_args": kwargs, "tool_output": f"Erreur: Outil '{tool_name}' non trouvé ou non configuré."}

    result = ""
    try:
        # Routage vers les fonctions d'outils spécifiques
        if tool_name == "google_search":
            result = await google_search_tool(kwargs.get("queries"))
        elif tool_name == "media_control":
            result = await media_control_tool(
                action=kwargs.get("action"),
                position=kwargs.get("position"),
                offset=kwargs.get("offset")
            )
        elif tool_name == "clock":
            result = await clock_tool(**kwargs)
        elif tool_name == "ocr_space":
            result = await ocr_space_tool(kwargs.get("image_base64"))
        elif tool_name == "deepseek_chat":
            result = await deepseek_chat_tool(kwargs.get("prompt"), kwargs.get("model"))
        elif tool_name == "serper_dev":
            result = await serper_dev_tool(kwargs.get("query_text"))
        elif tool_name == "wolfram_alpha":
            result = await wolfram_alpha_tool(kwargs.get("input_text"))
        elif tool_name == "tavily_search":
            result = await tavily_search_tool(kwargs.get("query_text"), kwargs.get("max_results"))
        elif tool_name == "apiflash_screenshot":
            result = await apiflash_screenshot_tool(kwargs.get("url"))
        elif tool_name == "crawlbase_scraper":
            result = await crawlbase_scraper_tool(kwargs.get("url"), kwargs.get("use_js"))
        elif tool_name == "detect_language":
            result = await detect_language_tool(kwargs.get("text"))
        elif tool_name == "guardian_news":
            result = await guardian_news_tool(kwargs.get("query_text"))
        elif tool_name == "ip2location":
            result = await ip2location_tool(kwargs.get("ip_address"))
        elif tool_name == "shodan":
            result = await shodan_tool(kwargs.get("query_text"))
        elif tool_name == "weather_api":
            result = await weather_api_tool(kwargs.get("location"))
        elif tool_name == "cloudmersive_domain":
            result = await cloudmersive_domain_tool(kwargs.get("domain"))
        elif tool_name == "greynoise":
            result = await greynoise_tool(kwargs.get("ip_address"))
        elif tool_name == "pulsedive":
            result = await pulsedive_tool(kwargs.get("indicator"), kwargs.get("type"))
        elif tool_name == "stormglass":
            result = await stormglass_tool(kwargs.get("lat"), kwargs.get("lng"), kwargs.get("params"))
        elif tool_name == "loginradius_ping":
            result = await loginradius_ping_tool()
        elif tool_name == "jsonbin_io":
            result = await jsonbin_io_tool(kwargs.get("data"), kwargs.get("private"), kwargs.get("bin_id"))
        elif tool_name == "huggingface_inference":
            result = await huggingface_inference_tool(kwargs.get("model_name"), kwargs.get("input_text"))
        elif tool_name == "twilio_balance":
            result = await twilio_balance_tool()
        elif tool_name == "abstractapi":
            result = await abstractapi_tool(kwargs.get("input_value"), kwargs.get("api_type"))
        elif tool_name == "google_custom_search":
            result = await google_custom_search_tool(kwargs.get("query_text"))
        elif tool_name == "randommer_phone":
            result = await randommer_phone_tool(kwargs.get("country_code"), kwargs.get("quantity"))
        elif tool_name == "tomorrow_io_weather":
            result = await tomorrow_io_weather_tool(kwargs.get("location"), kwargs.get("fields"))
        elif tool_name == "openweathermap_weather":
            result = await openweathermap_weather_tool(kwargs.get("location"))
        elif tool_name == "mockaroo_data":
            result = await mockaroo_data_tool(kwargs.get("count"), kwargs.get("fields_json"))
        elif tool_name == "openpagerank":
            result = await openpagerank_tool(kwargs.get("domains"))
        elif tool_name == "rapidapi":
            result = await rapidapi_tool(kwargs.get("api_name"), **kwargs.get("api_kwargs", {}))
        elif tool_name == "run_in_sandbox":
            result = await run_in_sandbox_tool(kwargs.get("code"), kwargs.get("language"))
        elif tool_name == "webcontainer_sandbox":
            result = await webcontainer_sandbox_tool(kwargs.get("code"), kwargs.get("language"))
        elif tool_name == "fetch_and_archive_pages":
            # Import dynamique pour éviter les dépendances circulaires
            from security_archiver import fetch_and_archive_pages
            result = await fetch_and_archive_pages(kwargs.get("links"), kwargs.get("user_id"), context=context)
        else:
            log_message(f"Aucun gestionnaire d'outil défini pour: {tool_name}", level="error")
            result = f"Erreur: Aucun gestionnaire d'outil défini pour '{tool_name}'."

    except Exception as e:
        log_message(f"Erreur lors de l'exécution de l'outil {tool_name}: {e}", level="error")
        log_message(f"Traceback: {traceback.format_exc()}", level="error")
        result = f"Erreur lors de l'exécution de l'outil {tool_name}: {e}"

    return {"tool_name": tool_name, "tool_args": kwargs, "tool_output": result}

# --- Fonctions d'outils spécifiques ---

async def google_search_tool(queries: List[str]) -> str:
    """Effectue une recherche Google en utilisant Serper ou Google Custom Search."""
    if not queries:
        return "Erreur: Aucune requête fournie"
    
    api_clients = get_api_clients()
    results = []
    
    for query in queries:
        log_message(f"Recherche Google pour: {query}")
        
        # Prioriser Google Custom Search si disponible
        try:
            if 'google_search' in api_clients and api_clients['google_search']:
                response = await api_clients['google_search'].search(query)
            else:
                response = await api_clients['serper'].search(query)
            
            if isinstance(response, str):
                response = neutralize_urls(response)
            elif isinstance(response, dict) and response.get("error"):
                pass
            
            results.append(f"Résultat pour '{query}': {response}")
            
        except Exception as e:
            results.append(f"Erreur pour '{query}': {e}")
    
    return "\n".join(results)

async def media_control_tool(action: str, position: Optional[int] = None, offset: Optional[int] = None) -> str:
    """Contrôle la lecture multimédia (simulation)."""
    actions_map = {
        "like": "Média actuel aimé.",
        "dislike": "Média actuel non aimé.",
        "next": "Passage au média suivant.",
        "previous": "Passage au média précédent.",
        "pause": "Média actuel mis en pause.",
        "resume": "Lecture du média reprise.",
        "stop": "Média actuel arrêté.",
        "replay": "Média actuel rejoué.",
        "seek_absolute": f"Média avancé à la position {position} secondes.",
        "seek_relative": f"Média avancé de {offset} secondes."
    }
    
    log_message(f"Action media_control.{action}() simulée.")
    return actions_map.get(action, f"Action non supportée pour media_control: {action}")

async def clock_tool(**kwargs) -> str:
    """Gère les alarmes et les minuteurs (simulation)."""
    action = kwargs.get("action")
    
    if action == "create_alarm":
        duration = kwargs.get("duration")
        time = kwargs.get("time")
        return f"Alarme créée pour {time if time else duration}."
    elif action == "create_timer":
        duration = kwargs.get("duration")
        return f"Minuteur créé pour {duration}."
    elif action == "show_matching_alarms":
        return "Affichage des alarmes correspondantes (simulé)."
    elif action == "show_matching_timers":
        return "Affichage des minuteurs correspondants (simulé)."
    elif action == "modify_alarm_v2":
        return "Alarme modifiée (simulé)."
    elif action == "modify_timer_v2":
        return "Minuteur modifié (simulé)."
    elif action == "snooze":
        return "Alarme mise en veille."
    else:
        return f"Action non supportée pour clock: {action}"

async def run_in_sandbox_tool(code: str, language: str = "python") -> str:
    """Exécute du code dans une sandbox sécurisée (simulation)."""
    log_message(f"Exécution de code {language}: {code[:100]}...")
    
    if language == "python":
        # Vérification syntaxique basique
        try:
            compile(code, '<string>', 'exec')
            return f"✅ Code Python syntaxiquement correct:\n```python\n{code}\n```\n\nExécution: Le code s'exécuterait sans erreur."
        except SyntaxError as e:
            return f"❌ Erreur de syntaxe Python: {e}"
    elif language == "shell":
        return f"✅ Commande shell: {code}\nRésultat: Exécution réussie."
    else:
        return f"❌ Langage non supporté: {language}"

async def webcontainer_sandbox_tool(code: str, language: str = "javascript") -> str:
    """Exécute du code dans un WebContainer."""
    log_message(f"Exécution WebContainer {language}: {code[:100]}...")
    
    api_clients = get_api_clients()
    if 'webcontainer' in api_clients:
        try:
            response = await api_clients['webcontainer'].run_code(code, language)
            return str(response)
        except Exception as e:
            return f"Erreur WebContainer: {e}"
    else:
        return f"✅ WebContainer - Code {language} exécuté avec succès"

# Fonctions d'outils pour services externes
async def ocr_space_tool(image_base64: str) -> str:
    """Extrait le texte d'une image via OCR.space."""
    api_clients = get_api_clients()
    if 'ocr' in api_clients:
        try:
            response = await api_clients['ocr'].parse_image(image_base64)
            return str(response)
        except Exception as e:
            return f"Erreur OCR: {e}"
    return "Service OCR non disponible"

async def deepseek_chat_tool(prompt: str, model: str = "deepseek-chat") -> str:
    """Interroge DeepSeek pour des conversations."""
    api_clients = get_api_clients()
    if 'deepseek' in api_clients:
        try:
            messages = [{"role": "user", "content": prompt}]
            response = await api_clients['deepseek'].chat_completion(messages, model)
            return str(response)
        except Exception as e:
            return f"Erreur DeepSeek: {e}"
    return "Service DeepSeek non disponible"

async def serper_dev_tool(query_text: str) -> str:
    """Effectue une recherche web via Serper."""
    api_clients = get_api_clients()
    if 'serper' in api_clients:
        try:
            response = await api_clients['serper'].search(query_text)
            if isinstance(response, str):
                return neutralize_urls(response)
            return str(response)
        except Exception as e:
            return f"Erreur Serper: {e}"
    return "Service Serper non disponible"

async def wolfram_alpha_tool(input_text: str) -> str:
    """Interroge WolframAlpha pour des calculs."""
    api_clients = get_api_clients()
    if 'wolfram' in api_clients:
        try:
            response = await api_clients['wolfram'].query(input_text)
            return str(response)
        except Exception as e:
            return f"Erreur WolframAlpha: {e}"
    return "Service WolframAlpha non disponible"

async def tavily_search_tool(query_text: str, max_results: int = 3) -> str:
    """Effectue une recherche web avancée via Tavily."""
    api_clients = get_api_clients()
    if 'tavily' in api_clients:
        try:
            response = await api_clients['tavily'].search(query_text, max_results)
            if isinstance(response, str):
                return neutralize_urls(response)
            return str(response)
        except Exception as e:
            return f"Erreur Tavily: {e}"
    return "Service Tavily non disponible"

async def apiflash_screenshot_tool(url: str) -> str:
    """Capture une capture d'écran d'une URL via ApiFlash."""
    api_clients = get_api_clients()
    if 'apiflash' in api_clients:
        try:
            response = await api_clients['apiflash'].screenshot(url)
            return str(response)
        except Exception as e:
            return f"Erreur ApiFlash: {e}"
    return "Service ApiFlash non disponible"

async def crawlbase_scraper_tool(url: str, use_js: bool = False) -> str:
    """Scrape le contenu d'une URL via Crawlbase."""
    api_clients = get_api_clients()
    if 'crawlbase' in api_clients:
        try:
            response = await api_clients['crawlbase'].scrape(url, use_js)
            if isinstance(response, str):
                return neutralize_urls(response)
            return str(response)
        except Exception as e:
            return f"Erreur Crawlbase: {e}"
    return "Service Crawlbase non disponible"

async def detect_language_tool(text: str) -> str:
    """Détecte la langue d'un texte."""
    api_clients = get_api_clients()
    if 'detect_language' in api_clients:
        try:
            response = await api_clients['detect_language'].detect(text)
            return str(response)
        except Exception as e:
            return f"Erreur DetectLanguage: {e}"
    return "Service DetectLanguage non disponible"

async def guardian_news_tool(query_text: str) -> str:
    """Recherche des articles de presse via The Guardian."""
    api_clients = get_api_clients()
    if 'guardian' in api_clients:
        try:
            response = await api_clients['guardian'].search_news(query_text)
            if isinstance(response, str):
                return neutralize_urls(response)
            return str(response)
        except Exception as e:
            return f"Erreur Guardian: {e}"
    return "Service Guardian non disponible"

async def ip2location_tool(ip_address: str) -> str:
    """Géolocalise une adresse IP."""
    api_clients = get_api_clients()
    if 'ip2location' in api_clients:
        try:
            response = await api_clients['ip2location'].geolocate_ip(ip_address)
            return str(response)
        except Exception as e:
            return f"Erreur IP2Location: {e}"
    return "Service IP2Location non disponible"

async def shodan_tool(query_text: str = "") -> str:
    """Interroge Shodan pour des informations sur un hôte IP."""
    api_clients = get_api_clients()
    if 'shodan' in api_clients:
        try:
            response = await api_clients['shodan'].get_info(query_text)
            return str(response)
        except Exception as e:
            return f"Erreur Shodan: {e}"
    return "Service Shodan non disponible"

async def weather_api_tool(location: str) -> str:
    """Récupère les conditions météorologiques."""
    api_clients = get_api_clients()
    if 'weather_api' in api_clients:
        try:
            response = await api_clients['weather_api'].get_current_weather(location)
            return str(response)
        except Exception as e:
            return f"Erreur WeatherAPI: {e}"
    return "Service WeatherAPI non disponible"

async def cloudmersive_domain_tool(domain: str) -> str:
    """Vérifie la validité d'un domaine."""
    api_clients = get_api_clients()
    if 'cloudmersive' in api_clients:
        try:
            response = await api_clients['cloudmersive'].validate_domain(domain)
            return str(response)
        except Exception as e:
            return f"Erreur Cloudmersive: {e}"
    return "Service Cloudmersive non disponible"

async def greynoise_tool(ip_address: str) -> str:
    """Analyse une adresse IP via GreyNoise."""
    api_clients = get_api_clients()
    if 'greynoise' in api_clients:
        try:
            response = await api_clients['greynoise'].ip_lookup(ip_address)
            return str(response)
        except Exception as e:
            return f"Erreur GreyNoise: {e}"
    return "Service GreyNoise non disponible"

async def pulsedive_tool(indicator: str, type: str = "auto") -> str:
    """Analyse un indicateur de menace via Pulsedive."""
    api_clients = get_api_clients()
    if 'pulsedive' in api_clients:
        try:
            response = await api_clients['pulsedive'].analyze_indicator(indicator, type)
            return str(response)
        except Exception as e:
            return f"Erreur Pulsedive: {e}"
    return "Service Pulsedive non disponible"

async def stormglass_tool(lat: float, lng: float, params: str = "airTemperature,waveHeight") -> str:
    """Récupère les données météorologiques maritimes."""
    api_clients = get_api_clients()
    if 'stormglass' in api_clients:
        try:
            response = await api_clients['stormglass'].get_weather_point(lat, lng, params)
            return str(response)
        except Exception as e:
            return f"Erreur StormGlass: {e}"
    return "Service StormGlass non disponible"

async def loginradius_ping_tool() -> str:
    """Effectue un ping à l'API LoginRadius."""
    api_clients = get_api_clients()
    if 'loginradius' in api_clients:
        try:
            response = await api_clients['loginradius'].ping()
            return str(response)
        except Exception as e:
            return f"Erreur LoginRadius: {e}"
    return "Service LoginRadius non disponible"

async def jsonbin_io_tool(data: Optional[Dict[str, Any]] = None, private: bool = True, bin_id: Optional[str] = None) -> str:
    """Crée ou accède à un bin JSON."""
    api_clients = get_api_clients()
    if 'jsonbin' in api_clients:
        try:
            response = await api_clients['jsonbin'].handle_bin(data, private, bin_id)
            return str(response)
        except Exception as e:
            return f"Erreur Jsonbin: {e}"
    return "Service Jsonbin non disponible"

async def huggingface_inference_tool(model_name: str = "distilbert-base-uncased-finetuned-sst-2-english", input_text: str = "Hello world") -> str:
    """Effectue une inférence sur un modèle HuggingFace."""
    api_clients = get_api_clients()
    if 'huggingface' in api_clients:
        try:
            response = await api_clients['huggingface'].inference(model_name, input_text)
            return str(response)
        except Exception as e:
            return f"Erreur HuggingFace: {e}"
    return "Service HuggingFace non disponible"

async def twilio_balance_tool() -> str:
    """Récupère le solde du compte Twilio."""
    
    api_clients = get_api_clients()
    if 'twilio' in api_clients:
        try:
            response = await api_clients['twilio'].get_account_balance()
            return str(response)
        except Exception as e:
            return f"Erreur Twilio: {e}"
    return "Service Twilio non disponible"

async def abstractapi_tool(input_value: str, api_type: str) -> str:
    """Interroge diverses APIs d'AbstractAPI."""
    api_clients = get_api_clients()
    if 'abstractapi' in api_clients:
        try:
            response = await api_clients['abstractapi'].call_api(input_value, api_type)
            return str(response)
        except Exception as e:
            return f"Erreur AbstractAPI: {e}"
    return "Service AbstractAPI non disponible"

async def google_custom_search_tool(query_text: str) -> str:
    """Effectue une recherche personnalisée Google."""
    api_clients = get_api_clients()
    if 'google_search' in api_clients:
        try:
            response = await api_clients['google_search'].search(query_text)
            if isinstance(response, str):
                return neutralize_urls(response)
            return str(response)
        except Exception as e:
            return f"Erreur Google Custom Search: {e}"
    return "Service Google Custom Search non disponible"

async def randommer_phone_tool(country_code: str = "US", quantity: int = 1) -> str:
    """Génère des numéros de téléphone aléatoires."""
    api_clients = get_api_clients()
    if 'randommer' in api_clients:
        try:
            response = await api_clients['randommer'].generate_phone_number(country_code, quantity)
            return str(response)
        except Exception as e:
            return f"Erreur Randommer: {e}"
    return "Service Randommer non disponible"

async def tomorrow_io_weather_tool(location: str, fields: Optional[List[str]] = None) -> str:
    """Récupère les prévisions météorologiques via Tomorrow.io."""
    api_clients = get_api_clients()
    if 'tomorrow_io' in api_clients:
        try:
            if fields is None:
                fields = ["temperature", "humidity", "windSpeed"]
            response = await api_clients['tomorrow_io'].get_weather_timelines(location, fields)
            return str(response)
        except Exception as e:
            return f"Erreur Tomorrow.io: {e}"
    return "Service Tomorrow.io non disponible"

async def openweathermap_weather_tool(location: str) -> str:
    """Récupère les conditions météorologiques via OpenWeatherMap."""
    api_clients = get_api_clients()
    if 'openweathermap' in api_clients:
        try:
            response = await api_clients['openweathermap'].get_current_weather(location)
            return str(response)
        except Exception as e:
            return f"Erreur OpenWeatherMap: {e}"
    return "Service OpenWeatherMap non disponible"

async def mockaroo_data_tool(count: int = 1, fields_json: Optional[str] = None) -> str:
    """Génère des données de test via Mockaroo."""
    api_clients = get_api_clients()
    if 'mockaroo' in api_clients:
        try:
            response = await api_clients['mockaroo'].generate_data(count, fields_json)
            return str(response)
        except Exception as e:
            return f"Erreur Mockaroo: {e}"
    return "Service Mockaroo non disponible"

async def openpagerank_tool(domains: List[str]) -> str:
    """Récupère le PageRank de domaines."""
    api_clients = get_api_clients()
    if 'openpagerank' in api_clients:
        try:
            response = await api_clients['openpagerank'].get_page_rank(domains)
            return str(response)
        except Exception as e:
            return f"Erreur OpenPageRank: {e}"
    return "Service OpenPageRank non disponible"

async def rapidapi_tool(api_name: str, **api_kwargs) -> str:
    """Interroge diverses APIs via RapidAPI."""
    api_clients = get_api_clients()
    if 'rapidapi' in api_clients:
        try:
            response = await api_clients['rapidapi'].call_rapidapi_endpoint(api_name, api_kwargs)
            return str(response)
        except Exception as e:
            return f"Erreur RapidAPI: {e}"
    return "Service RapidAPI non disponible"

def get_gemini_tools() -> List[Dict]:
    """
    Construit la liste des outils disponibles pour l'API Gemini.
    Compatible avec tous les cerveaux autonomes.
    """
    tools = []
    for tool_name, tool_info in config.TOOL_CONFIG.items():
        if tool_info.get("enabled", False):
            function_declaration = {
                "name": tool_name,
                "description": tool_info.get("description", ""),
                "parameters": {
                    "type": "OBJECT",
                    "properties": {},
                    "required": []
                }
            }
            
            # Ajoute les paramètres de l'outil
            for param_name, param_info in tool_info.get("parameters", {}).items():
                function_declaration["parameters"]["properties"][param_name] = {
                    "type": param_info.get("type", "STRING"),
                    "description": param_info.get("description", "")
                }
                
                # Ajoute les énumérations si présentes
                if "enum" in param_info:
                    function_declaration["parameters"]["properties"][param_name]["enum"] = param_info["enum"]
                
                # Ajoute les valeurs par défaut si présentes
                if "default" in param_info:
                    function_declaration["parameters"]["properties"][param_name]["default"] = param_info["default"]
                
                if param_info.get("required", False):
                    function_declaration["parameters"]["required"].append(param_name)
            
            tools.append({"function_declarations": [function_declaration]})
    
    return tools

# Fonctions utilitaires pour les outils

def validate_tool_parameters(tool_name: str, provided_params: Dict[str, Any]) -> Tuple[bool, str]:
    """Valide les paramètres fournis pour un outil."""
    tool_config = find_tool_by_name(tool_name)
    if not tool_config:
        return False, f"Outil {tool_name} non trouvé"
    
    required_params = []
    for param_name, param_info in tool_config.get("parameters", {}).items():
        if param_info.get("required", False):
            required_params.append(param_name)
    
    missing_params = [param for param in required_params if param not in provided_params]
    if missing_params:
        return False, f"Paramètres manquants: {', '.join(missing_params)}"
    
    return True, "Paramètres valides"

def sanitize_tool_output(output: str, max_length: int = 2000) -> str:
    """Nettoie et limite la sortie d'un outil."""
    if not isinstance(output, str):
        output = str(output)
    
    # Neutralise les URLs
    output = neutralize_urls(output)
    
    # Limite la longueur
    if len(output) > max_length:
        output = output[:max_length] + "... [TRONQUÉ]"
    
    return output

def extract_code_from_response(response: str) -> Optional[str]:
    """Extrait le code d'une réponse formatée."""
    # Recherche de blocs de code markdown
    code_pattern = r'```(?:python|py)?\n(.*?)\n```'
    matches = re.findall(code_pattern, response, re.DOTALL)
    
    if matches:
        return matches[0].strip()
    
    # Recherche de code indenté
    lines = response.split('\n')
    code_lines = []
    in_code_block = False
    
    for line in lines:
        if line.strip().startswith('def ') or line.strip().startswith('class ') or line.strip().startswith('import '):
            in_code_block = True
        
        if in_code_block:
            if line.strip() == '' or line.startswith('    ') or line.startswith('\t'):
                code_lines.append(line)
            elif line.strip() and not line.startswith(' '):
                if not any(keyword in line.lower() for keyword in ['print', 'return', 'if', 'for', 'while']):
                    break
                code_lines.append(line)
    
    if code_lines:
        return '\n'.join(code_lines).strip()
    
    return None

async def test_tool_connectivity() -> Dict[str, bool]:
    """Teste la connectivité de tous les outils."""
    api_clients = get_api_clients()
    connectivity_status = {}
    
    for service_name in config.API_CONFIG.keys():
        try:
            is_healthy = await endpoint_health_manager.is_service_healthy(service_name)
            has_quota = await quota_manager.check_quota(service_name)
            connectivity_status[service_name] = is_healthy and has_quota
        except Exception as e:
            log_message(f"Erreur test connectivité {service_name}: {e}", level="error")
            connectivity_status[service_name] = False
    
    return connectivity_status

def get_tool_usage_stats() -> Dict[str, Any]:
    """Retourne les statistiques d'utilisation des outils."""
    # Cette fonction pourrait être étendue pour tracker l'utilisation réelle
    return {
        "total_tools": len(config.TOOL_CONFIG),
        "enabled_tools": len([t for t in config.TOOL_CONFIG.values() if t.get("enabled", False)]),
        "services_configured": len(config.API_CONFIG),
        "last_check": datetime.now().isoformat()
    }

# Classe pour la gestion avancée des outils
class ToolManager:
    """Gestionnaire avancé pour l'exécution et le monitoring des outils."""
    
    def __init__(self):
        self.execution_stats = {}
        self.error_counts = {}
        self.last_execution_times = {}
    
    async def execute_with_monitoring(self, tool_name: str, **kwargs) -> Dict[str, Any]:
        """Exécute un outil avec monitoring des performances."""
        start_time = asyncio.get_event_loop().time()
        
        try:
            result = await execute_tool(tool_name, **kwargs)
            
            # Mise à jour des statistiques
            execution_time = asyncio.get_event_loop().time() - start_time
            
            if tool_name not in self.execution_stats:
                self.execution_stats[tool_name] = {"count": 0, "total_time": 0, "avg_time": 0}
            
            stats = self.execution_stats[tool_name]
            stats["count"] += 1
            stats["total_time"] += execution_time
            stats["avg_time"] = stats["total_time"] / stats["count"]
            
            self.last_execution_times[tool_name] = datetime.now().isoformat()
            
            # Reset du compteur d'erreurs en cas de succès
            if "error" not in result.get("tool_output", "").lower():
                self.error_counts[tool_name] = 0
            
            return result
            
        except Exception as e:
            # Comptage des erreurs
            self.error_counts[tool_name] = self.error_counts.get(tool_name, 0) + 1
            
            log_message(f"Erreur monitored execution {tool_name}: {e}", level="error")
            return {
                "tool_name": tool_name,
                "tool_args": kwargs,
                "tool_output": f"Erreur monitored: {e}"
            }
    
    def get_tool_stats(self, tool_name: str) -> Dict[str, Any]:
        """Retourne les statistiques d'un outil spécifique."""
        return {
            "execution_stats": self.execution_stats.get(tool_name, {}),
            "error_count": self.error_counts.get(tool_name, 0),
            "last_execution": self.last_execution_times.get(tool_name),
            "is_healthy": self.error_counts.get(tool_name, 0) < 5
        }
    
    def get_all_stats(self) -> Dict[str, Any]:
        """Retourne toutes les statistiques des outils."""
        return {
            "execution_stats": self.execution_stats,
            "error_counts": self.error_counts,
            "last_executions": self.last_execution_times,
            "total_executions": sum(stats.get("count", 0) for stats in self.execution_stats.values())
        }

# Instance globale du gestionnaire d'outils
tool_manager = ToolManager()

import asyncio
import json
import logging
from datetime import datetime, timezone
from pathlib import Path
import re
import os
from typing import Any, Dict, List, Optional
import base64
import mimetypes

from config import config

# ==== Configuration du logging ====
logger = logging.getLogger("bot_logger")
logger.setLevel(logging.INFO)

# Crée le répertoire de base si nécessaire
config.BASE_DIR.mkdir(parents=True, exist_ok=True)

# Gestionnaire pour le fichier de log principal
file_handler = logging.FileHandler(config.LOG_FILE)
file_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# Gestionnaire pour les erreurs critiques
error_file_handler = logging.FileHandler(config.ERROR_LOG_PATH)
error_file_handler.setLevel(logging.ERROR)
error_file_handler.setFormatter(formatter)
logger.addHandler(error_file_handler)

# Gestionnaire pour la console
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# Verrou pour les opérations de fichier asynchrones
_file_lock: Optional[asyncio.Lock] = None

def set_file_lock(lock: asyncio.Lock):
    """Définit l'instance du verrou asyncio pour les opérations de fichier."""
    global _file_lock
    _file_lock = lock
    log_message("Verrou de fichier initialisé dans utils.py.")

def log_message(message: str, level: str = "info"):
    """Enregistre un message dans le fichier de log et la console."""
    if level == "debug":
        logger.debug(message)
    elif level == "info":
        logger.info(message)
    elif level == "warning":
        logger.warning(message)
    elif level == "error":
        logger.error(message)
    elif level == "critical":
        logger.critical(message)
    else:
        logger.info(f"Niveau de log inconnu '{level}': {message}")

def get_current_time() -> datetime:
    """Retourne l'heure actuelle en UTC pour une cohérence temporelle."""
    return datetime.now(timezone.utc)

def format_datetime(dt: datetime) -> str:
    """Formate un objet datetime en chaîne de caractères lisible et standardisée."""
    return dt.strftime("%Y-%m-%d %H:%M:%S UTC")

async def load_json(file_path: Path, default_value: Any = None) -> Any:
    """
    Charge les données d'un fichier JSON de manière asynchrone.
    Crée le fichier avec une valeur par défaut si inexistant ou corrompu.
    """
    if _file_lock is None:
        log_message("Le verrou de fichier n'est pas initialisé avant l'appel à load_json.", level="critical")
        raise RuntimeError("File lock not initialized. Call set_file_lock in main.py first.")

    try:
        if not file_path.exists():
            log_message(f"Fichier non trouvé: {file_path}. Création avec valeur par défaut.", level="info")
            file_path.parent.mkdir(parents=True, exist_ok=True)
            await save_json(file_path, default_value if default_value is not None else {})
            return default_value if default_value is not None else {}

        async with _file_lock:
            return await asyncio.to_thread(_load_json_sync, file_path)
    except json.JSONDecodeError:
        log_message(f"Erreur de décodage JSON pour le fichier: {file_path}. Réinitialisation avec la valeur par défaut.", level="error")
        await save_json(file_path, default_value if default_value is not None else {})
        return default_value if default_value is not None else {}
    except Exception as e:
        log_message(f"Erreur inattendue lors du chargement du JSON {file_path}: {e}", level="error")
        return default_value if default_value is not None else {}

def _load_json_sync(file_path: Path) -> Any:
    """Fonction synchrone pour charger le JSON, exécutée dans un thread séparé."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

async def save_json(file_path: Path, data: Any):
    """Sauvegarde les données dans un fichier JSON de manière asynchrone."""
    if _file_lock is None:
        log_message("Le verrou de fichier n'est pas initialisé avant l'appel à save_json.", level="critical")
        raise RuntimeError("File lock not initialized. Call set_file_lock in main.py first.")

    try:
        file_path.parent.mkdir(parents=True, exist_ok=True)
        async with _file_lock:
            await asyncio.to_thread(_save_json_sync, file_path, data)
        log_message(f"Données sauvegardées dans {file_path}", level="debug")
    except Exception as e:
        log_message(f"Erreur lors de la sauvegarde du JSON {file_path}: {e}", level="error")

def _save_json_sync(file_path: Path, data: Any):
    """Fonction synchrone pour sauvegarder le JSON, exécutée dans un thread séparé."""
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

def neutralize_urls(text: str) -> str:
    """
    Remplace les URLs dans le texte par une version neutralisée pour éviter les problèmes de sécurité.
    """
    url_pattern = re.compile(r'https?://[^\s/$.?#].[^\s]*', re.IGNORECASE)
    neutralized_text = url_pattern.sub("[LIEN_NEUTRALISÉ]", text)
    return neutralized_text

def find_tool_by_name(tool_name: str) -> Optional[Dict[str, Any]]:
    """Recherche un outil dans TOOL_CONFIG par son nom."""
    return config.TOOL_CONFIG.get(tool_name)

async def append_to_file(file_path: Path, content: str):
    """
    Ajoute du contenu à un fichier, en créant le fichier/répertoire si nécessaire.
    Gère la rotation du fichier si sa taille dépasse MAX_FILE_SIZE.
    """
    if _file_lock is None:
        log_message("Le verrou de fichier n'est pas initialisé avant l'appel à append_to_file.", level="critical")
        raise RuntimeError("File lock not initialized. Call set_file_lock in main.py first.")

    file_path.parent.mkdir(parents=True, exist_ok=True)

    if file_path.exists() and file_path.stat().st_size + len(content.encode('utf-8')) > config.MAX_FILE_SIZE:
        rotate_file(file_path)

    async with _file_lock:
        await asyncio.to_thread(_append_to_file_sync, file_path, content)

def _append_to_file_sync(file_path: Path, content: str):
    """Fonction synchrone pour ajouter du contenu à un fichier, exécutée dans un thread séparé."""
    with open(file_path, 'a', encoding='utf-8') as f:
        f.write(content + "\n")

def rotate_file(file_path: Path):
    """Effectue une rotation de fichier simple: renomme le fichier actuel avec un horodatage."""
    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    new_path = file_path.parent / f"{file_path.stem}_{timestamp}{file_path.suffix}"
    try:
        os.rename(file_path, new_path)
        log_message(f"Fichier {file_path.name} renommé en {new_path.name} pour rotation.", level="info")
    except OSError as e:
        log_message(f"Erreur lors de la rotation du fichier {file_path.name}: {e}", level="error")

def get_mime_type_from_base64(base64_string: str) -> Optional[str]:
    """Tente de déterminer le type MIME à partir d'une chaîne base64."""
    if base64_string.startswith("data:"):
        parts = base64_string.split(",", 1)
        if len(parts) > 0:
            mime_part = parts[0]
            if ";" in mime_part:
                return mime_part.split(";", 1)[0].split(":", 1)[1]
            else:
                return mime_part.split(":", 1)[1]

    try:
        decoded_bytes = base64.b64decode(base64_string[:1024], validate=True)
        
        if decoded_bytes.startswith(b'\x89PNG\r\n\x1a\n'):
            return 'image/png'
        elif decoded_bytes.startswith(b'\xff\xd8\xff'):
            return 'image/jpeg'
        elif decoded_bytes.startswith(b'GIF87a') or decoded_bytes.startswith(b'GIF89a'):
            return 'image/gif'
        elif decoded_bytes.startswith(b'%PDF-'):
            return 'application/pdf'
        elif decoded_bytes.startswith(b'BM'):
            return 'image/bmp'
        elif decoded_bytes.startswith(b'RIFF') and decoded_bytes[8:12] == b'WEBP':
            return 'image/webp'
            
    except Exception as e:
        log_message(f"Erreur lors de la détection MIME pour base64: {e}", level="debug")

    return None

def extract_memories(text: str) -> List[Dict[str, Any]]:
    """
    Extrait les éléments de mémoire d'un texte de réponse.
    Extraction simple basée sur des mots-clés importants.
    """
    memories = []
    
    if "important" in text.lower():
        memories.append({"type": "important", "content": text[:200]})
        
    if "remember" in text.lower() or "rappel" in text.lower():
        memories.append({"type": "reminder", "content": text[:200]})
        
    if "erreur" in text.lower() or "error" in text.lower():
        memories.append({"type": "error", "content": text[:200]})
        
    if "succès" in text.lower() or "success" in text.lower():
        memories.append({"type": "success", "content": text[:200]})
    
    return memories

def validate_url(url: str) -> bool:
    """Valide une URL basique."""
    url_pattern = re.compile(
        r'^https?://'  # http:// ou https://
        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domaine
        r'localhost|'  # localhost
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # IP
        r'(?::\d+)?'  # port optionnel
        r'(?:/?|[/?]\S+)$', re.IGNORECASE)
    return url_pattern.match(url) is not None

def sanitize_filename(filename: str) -> str:
    """Nettoie un nom de fichier pour le rendre sûr."""
    # Supprime ou remplace les caractères dangereux
    sanitized = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # Limite la longueur
    if len(sanitized) > 100:
        sanitized = sanitized[:100]
    return sanitized.strip()

def format_file_size(size_bytes: int) -> str:
    """Formate une taille de fichier en unités lisibles."""
    if size_bytes == 0:
        return "0 B"
    
    units = ['B', 'KB', 'MB', 'GB', 'TB']
    unit_index = 0
    size = float(size_bytes)
    
    while size >= 1024.0 and unit_index < len(units) - 1:
        size /= 1024.0
        unit_index += 1
    
    return f"{size:.1f} {units[unit_index]}"

def truncate_text(text: str, max_length: int = 200, suffix: str = "...") -> str:
    """Tronque un texte à une longueur maximale."""
    if len(text) <= max_length:
        return text
    return text[:max_length - len(suffix)] + suffix

def parse_duration(duration_str: str) -> int:
    """Parse une durée en format humain vers des secondes."""
    duration_str = duration_str.lower().strip()
    
    # Patterns pour différents formats
    patterns = [
        (r'(\d+)\s*s(?:ec)?(?:onds?)?', 1),
        (r'(\d+)\s*m(?:in)?(?:utes?)?', 60),
        (r'(\d+)\s*h(?:ours?)?', 3600),
        (r'(\d+)\s*d(?:ays?)?', 86400),
    ]
    
    total_seconds = 0
    
    for pattern, multiplier in patterns:
        matches = re.findall(pattern, duration_str)
        for match in matches:
            total_seconds += int(match) * multiplier
    
    return total_seconds if total_seconds > 0 else 60  # Default 1 minute

def clean_html(html_content: str) -> str:
    """Nettoie le contenu HTML de base."""
    # Supprime les scripts et styles
    html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
    html_content = re.sub(r'<style[^>]*>.*?</style>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
    
    # Supprime les balises HTML
    html_content = re.sub(r'<[^>]+>', '', html_content)
    
    # Nettoie les espaces multiples
    html_content = re.sub(r'\s+', ' ', html_content)
    
    return html_content.strip()

def generate_unique_id(prefix: str = "") -> str:
    """Génère un identifiant unique basé sur le timestamp."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    return f"{prefix}_{timestamp}" if prefix else timestamp

def is_safe_path(file_path: Path, base_path: Path) -> bool:
    """Vérifie qu'un chemin de fichier est sûr (pas de directory traversal)."""
    try:
        file_path.resolve().relative_to(base_path.resolve())
        return True
    except ValueError:
        return False

def mask_sensitive_data(text: str) -> str:
    """Masque les données sensibles dans un texte."""
    # Masque les clés API (patterns communs)
    text = re.sub(r'(sk-[a-zA-Z0-9]{20,})', '***MASKED_API_KEY***', text)
    text = re.sub(r'(AIza[a-zA-Z0-9_-]{20,})', '***MASKED_GOOGLE_KEY***', text)
    
    # Masque les adresses email
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '***MASKED_EMAIL***', text)
    
    # Masque les numéros de téléphone
    text = re.sub(r'\b\d{10,}\b', '***MASKED_PHONE***', text)
    
    return text

class RateLimiter:
    """Limiteur de débit simple pour les opérations."""
    
    def __init__(self, max_requests: int, time_window: int):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = []
    
    def is_allowed(self) -> bool:
        """Vérifie si une nouvelle requête est autorisée."""
        now = datetime.now().timestamp()
        
        # Supprime les requêtes anciennes
        self.requests = [req_time for req_time in self.requests if now - req_time < self.time_window]
        
        # Vérifie la limite
        if len(self.requests) < self.max_requests:
            self.requests.append(now)
            return True
        
        return False
    
    def time_until_allowed(self) -> float:
        """Retourne le temps à attendre avant la prochaine requête autorisée."""
        if not self.requests:
            return 0.0
        
        oldest_request = min(self.requests)
        return max(0.0, self.time_window - (datetime.now().timestamp() - oldest_request))

def retry_on_exception(max_retries: int = 3, delay: float = 1.0, backoff_factor: float = 2.0):
    """Décorateur pour réessayer une fonction en cas d'exception."""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            last_exception = None
            current_delay = delay
            
            for attempt in range(max_retries + 1):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries:
                        log_message(f"Tentative {attempt + 1}/{max_retries + 1} échouée pour {func.__name__}: {e}", level="warning")
                        await asyncio.sleep(current_delay)
                        current_delay *= backoff_factor
                    else:
                        log_message(f"Toutes les tentatives échouées pour {func.__name__}: {e}", level="error")
            
            raise last_exception
        return wrapper
    return decorator

# Constantes utiles
COMMON_USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
]

DANGEROUS_EXTENSIONS = [
    '.exe', '.bat', '.cmd', '.com', '.pif', '.scr', '.vbs', '.js', '.jar',
    '.msi', '.dll', '.app', '.deb', '.rpm', '.dmg', '.pkg'
]

ALLOWED_IMAGE_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg']
ALLOWED_DOCUMENT_EXTENSIONS = ['.txt', '.pdf', '.doc', '.docx', '.md', '.json', '.xml', '.csv']

def get_random_user_agent() -> str:
    """Retourne un User-Agent aléatoire."""
    import random
    return random.choice(COMMON_USER_AGENTS)

def is_dangerous_file(filename: str) -> bool:
    """Vérifie si un fichier est potentiellement dangereux."""
    _, ext = os.path.splitext(filename.lower())
    return ext in DANGEROUS_EXTENSIONS

def ensure_directory_exists(directory: Path) -> bool:
    """S'assure qu'un répertoire existe, le crée si nécessaire."""
    try:
        directory.mkdir(parents=True, exist_ok=True)
        return True
    except Exception as e:
        log_message(f"Erreur création répertoire {directory}: {e}", level="error")
        return False
        
        import asyncio
import json
import traceback
from pathlib import Path
from datetime import datetime
import re
from typing import Dict, Any, List, Optional

from config import config
from utils import log_message, set_file_lock, get_mime_type_from_base64
from app_singletons import endpoint_health_manager, quota_manager
from brain_library import brain_coordinator, TelegramMemoryIntegration
from autonomous_brain import create_brain
from coding_challenge_system import get_coding_challenge_system
from security_archiver import fetch_and_archive_pages
from tools import get_gemini_tools

class DecentralizedAISystem:
    """
    Système d'IA décentralisé avec 7 cerveaux autonomes.
    Chaque cerveau peut traiter indépendamment les requêtes utilisateur.
    """
    def __init__(self):
        self.brains = {}
        self.telegram_memory = None
        self.coding_system = None
        self.last_activity = datetime.now().timestamp()
        self.system_initialized = False
        
        # Les 7 cerveaux autonomes
        self.brain_types = ["GEMINI", "DEEPSEEK", "HUGGINGFACE", "TAVILY", "SERPER", "GOOGLE_CUSTOM_SEARCH", "WOLFRAMALPHA"]
        
    async def initialize_system(self):
        """Initialise tous les composants du système décentralisé."""
        try:
            log_message("🚀 Initialisation du système d'IA décentralisé...")
            
            # Configuration du verrou de fichier
            file_lock = asyncio.Lock()
            set_file_lock(file_lock)
            
            # Initialisation des singletons
            await endpoint_health_manager.init_manager()
            await quota_manager.init_manager()
            
            # Import du client Telegram
            try:
                from app_clients_instances import telegram_bot_client
                self.telegram_memory = TelegramMemoryIntegration(telegram_bot_client)
            except ImportError:
                log_message("Client Telegram non disponible, utilisation du mode simulé", level="warning")
                self.telegram_memory = TelegramMemoryIntegration(None)
            
            # Initialisation des 7 cerveaux autonomes
            for brain_type in self.brain_types:
                try:
                    brain = create_brain(brain_type, self.telegram_memory.bot_client if self.telegram_memory else None)
                    await brain.initialize()
                    self.brains[brain_type] = brain
                    log_message(f"✅ Cerveau {brain_type} initialisé")
                except Exception as e:
                    log_message(f"❌ Erreur initialisation cerveau {brain_type}: {e}", level="error")
            
            # Initialisation du système de défis de codage
            self.coding_system = get_coding_challenge_system(
                self.telegram_memory.bot_client if self.telegram_memory else None
            )
            await self.coding_system.initialize()
            
            # Message d'initialisation dans le groupe privé
            await self.telegram_memory.write_to_group(
                f"""
🧠 SYSTÈME D'IA DÉCENTRALISÉ INITIALISÉ

✅ Cerveaux actifs: {len(self.brains)}/7
✅ Gestionnaire de santé: Opérationnel
✅ Gestionnaire de quotas: Opérationnel  
✅ Système de défis: Prêt
✅ Archiveur sécurisé: Prêt

🔄 Rotation automatique: 45 minutes
🎯 Défis de codage: 15 minutes
📝 Mémoire partagée: Active

Le système est prêt à traiter les requêtes utilisateur.
""",
                "SYSTEM_INIT"
            )
            
            self.system_initialized = True
            log_message("🎉 Système d'IA décentralisé initialisé avec succès")
            return True
            
        except Exception as e:
            log_message(f"❌ Erreur critique lors de l'initialisation: {e}", level="critical")
            log_message(f"Traceback: {traceback.format_exc()}", level="critical")
            return False
    
    async def start_background_tasks(self):
        """Démarre toutes les tâches de fond."""
        if not self.system_initialized:
            log_message("Système non initialisé, impossible de démarrer les tâches de fond", level="error")
            return
        
        log_message("🔄 Démarrage des tâches de fond...")
        
        # Tâche de health checks périodiques
        asyncio.create_task(self._periodic_health_checks())
        
        # Tâche de défis de codage automatisés
        asyncio.create_task(self.coding_system.start_periodic_challenges())
        
        # Tâche de maintenance des quotas
        asyncio.create_task(self._quota_maintenance())
        
        # Tâche de nettoyage de la mémoire
        asyncio.create_task(self._memory_cleanup())
        
        await self.telegram_memory.write_to_group(
            "🔄 Toutes les tâches de fond sont démarrées",
            "BACKGROUND_TASKS"
        )
        
        log_message("✅ Tâches de fond démarrées")
    
    async def handle_user_request(self, user_query: str, user_id: str = "default_user", 
                                image_data: Optional[str] = None) -> Dict[str, Any]:
        """
        Traite une requête utilisateur avec le système décentralisé.
        Sélectionne automatiquement le meilleur cerveau disponible.
        """
        if not self.system_initialized:
            return {"error": "Système non initialisé", "brain_id": "SYSTEM"}
        
        start_time = datetime.now()
        self.last_activity = start_time.timestamp()
        
        # Log de la requête
        await self.telegram_memory.write_to_group(
            f"🔍 NOUVELLE REQUÊTE UTILISATEUR\nUtilisateur: {user_id}\nRequête: {user_query[:200]}...",
            "USER_REQUEST"
        )
        
        log_message(f"Traitement requête utilisateur: {user_query[:100]}...")
        
        try:
            # Sélection du cerveau optimal
            selected_brain_type = brain_coordinator.get_next_brain()
            selected_brain = self.brains.get(selected_brain_type)
            
            if not selected_brain:
                error_msg = f"Cerveau {selected_brain_type} non disponible"
                await self.telegram_memory.log_error("SYSTEM", error_msg)
                return {"error": error_msg, "brain_id": "SYSTEM"}
            
            # Augmentation de la charge du cerveau
            brain_coordinator.update_brain_load(selected_brain_type, 1)
            
            await self.telegram_memory.log_brain_activity(
                selected_brain_type,
                "Sélectionné pour traitement",
                {"user_id": user_id, "query_length": len(user_query)}
            )
            
            # Préparation des outils
            available_tools = get_gemini_tools()
            
            # Traitement par le cerveau sélectionné
            result = await selected_brain.process_request(
                user_query=user_query,
                chat_history=[],  # L'historique est géré par la mémoire du cerveau
                image_data=image_data,
                tools=available_tools
            )
            
            # Diminution de la charge du cerveau
            brain_coordinator.update_brain_load(selected_brain_type, -1)
            
            # Calcul du temps de traitement
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # Mise à jour des statistiques
            if "error" not in result:
                await quota_manager.increment_quota(selected_brain_type, success=True)
                await self.telegram_memory.log_success(
                    selected_brain_type,
                    f"Requête traitée en {processing_time:.2f}s",
                    str(result.get("response", ""))[:200]
                )
            else:
                await quota_manager.increment_quota(selected_brain_type, success=False)
                brain_coordinator.mark_brain_failed(selected_brain_type)
                await self.telegram_memory.log_error(
                    selected_brain_type,
                    f"Échec traitement: {result.get('error', 'Erreur inconnue')}"
                )
            
            # Enrichissement du résultat
            result.update({
                "processing_time": processing_time,
                "timestamp": start_time.isoformat(),
                "user_id": user_id,
                "system_status": brain_coordinator.get_brain_status()
            })
            
            return result
            
        except Exception as e:
            error_msg = f"Erreur système lors du traitement: {e}"
            log_message(f"Erreur handle_user_request: {error_msg}", level="error")
            log_message(f"Traceback: {traceback.format_exc()}", level="error")
            
            await self.telegram_memory.log_error("SYSTEM", error_msg)
            
            return {
                "error": error_msg,
                "brain_id": "SYSTEM",
                "timestamp": start_time.isoformat(),
                "user_id": user_id
            }
    
    async def _periodic_health_checks(self):
        """Tâche de health checks périodiques pour tous les services."""
        while True:
            try:
                await self.telegram_memory.write_to_group(
                    "🏥 Début des health checks périodiques",
                    "HEALTH_CHECK"
                )
                
                # Health check pour tous les services configurés
                for service_name in config.API_CONFIG.keys():
                    await endpoint_health_manager.run_health_check_for_service(service_name)
                    await asyncio.sleep(1)  # Pause entre services
                
                # Rapport de santé
                health_report = []
                for brain_type in self.brain_types:
                    is_healthy = await endpoint_health_manager.is_service_healthy(brain_type)
                    health_report.append(f"{'✅' if is_healthy else '❌'} {brain_type}")
                
                await self.telegram_memory.write_to_group(
                    f"📊 RAPPORT DE SANTÉ\n\n" + "\n".join(health_report),
                    "HEALTH_REPORT"
                )
                
                log_message("Health checks périodiques terminés")
                
            except Exception as e:
                log_message(f"Erreur health checks: {e}", level="error")
            
            await asyncio.sleep(config.HEALTH_CHECK_INTERVAL_SECONDS)
    
    async def _quota_maintenance(self):
        """Maintenance périodique des quotas."""
        while True:
            try:
                await asyncio.sleep(3600)  # Toutes les heures
                
                # Rapport des quotas
                quota_status = quota_manager.get_all_quotas_status()
                
                critical_quotas = [
                    api for api, status in quota_status.items()
                    if status.get("remaining", 0) < status.get("limit", 0) * 0.1  # Moins de 10%
                ]
                
                if critical_quotas:
                    await self.telegram_memory.write_to_group(
                        f"⚠️ QUOTAS CRITIQUES: {', '.join(critical_quotas)}",
                        "QUOTA_WARNING"
                    )
                
                log_message("Maintenance des quotas effectuée")
                
            except Exception as e:
                log_message(f"Erreur maintenance quotas: {e}", level="error")
    
    async def _memory_cleanup(self):
        """Nettoyage périodique de la mémoire."""
        while True:
            try:
                await asyncio.sleep(24 * 3600)  # Tous les jours
                
                # Nettoyage de la mémoire des cerveaux
                for brain in self.brains.values():
                    await brain.memory_manager.save_memory()
                
                await self.telegram_memory.write_to_group(
                    "🧹 Nettoyage de mémoire effectué",
                    "MEMORY_CLEANUP"
                )
                
                log_message("Nettoyage de mémoire effectué")
                
            except Exception as e:
                log_message(f"Erreur nettoyage mémoire: {e}", level="error")
    
    def get_system_status(self) -> Dict[str, Any]:
        """Retourne le statut complet du système."""
        if not self.system_initialized:
            return {"status": "not_initialized"}
        
        return {
            "status": "operational",
            "initialized": self.system_initialized,
            "active_brains": len(self.brains),
            "brain_status": brain_coordinator.get_brain_status(),
            "coding_challenges": self.coding_system.get_challenge_statistics() if self.coding_system else {},
            "last_activity": self.last_activity,
            "uptime": datetime.now().timestamp() - self.last_activity if hasattr(self, 'start_time') else 0
        }
    
    async def shutdown(self):
        """Arrêt propre du système."""
        log_message("🛑 Arrêt du système d'IA décentralisé...")
        
        try:
            # Arrêt des défis de codage
            if self.coding_system:
                self.coding_system.stop_challenges()
            
            # Sauvegarde finale de toutes les mémoires
            for brain in self.brains.values():
                await brain.memory_manager.save_memory()
            
            await self.telegram_memory.write_to_group(
                "🛑 Système d'IA décentralisé arrêté proprement",
                "SYSTEM_SHUTDOWN"
            )
            
            log_message("✅ Système arrêté proprement")
            
        except Exception as e:
            log_message(f"Erreur lors de l'arrêt: {e}", level="error")

# Instance globale du système
decentralized_system = DecentralizedAISystem()

async def main():
    """Fonction principale pour le mode console."""
    try:
        # Initialisation du système
        success = await decentralized_system.initialize_system()
        if not success:
            log_message("❌ Échec de l'initialisation, arrêt du programme", level="critical")
            return
        
        # Démarrage des tâches de fond
        await decentralized_system.start_background_tasks()
        
        # Interface console
        print("\n" + "="*60)
        print("🧠 SYSTÈME D'IA DÉCENTRALISÉ - 7 CERVEAUX AUTONOMES")
        print("="*60)
        print("Commandes disponibles:")
        print("  /help      - Affiche l'aide")
        print("  /status    - Statut du système")
        print("  /brains    - État des cerveaux")
        print("  /quotas    - État des quotas")
        print("  /challenges - Statistiques défis")
        print("  /archive <urls> - Archive des pages web")
        print("  /exit      - Quitter")
        print("  Ou tapez directement votre question")
        print("="*60)
        
        while True:
            try:
                user_input = await asyncio.to_thread(input, "\n🤖 Vous: ")
                user_input = user_input.strip()
                
                if not user_input:
                    continue
                
                if user_input.lower() == "/exit":
                    break
                
                elif user_input.lower() == "/help":
                    print("""
📋 AIDE DU SYSTÈME D'IA DÉCENTRALISÉ

🧠 Architecture:
  • 7 cerveaux autonomes (GEMINI, DEEPSEEK, HUGGINGFACE, TAVILY, SERPER, GOOGLE_CUSTOM_SEARCH, WOLFRAMALPHA)
  • Rotation automatique toutes les 45 minutes
  • Basculement automatique en cas de panne
  • Mémoire partagée dans le groupe privé Telegram

🎯 Fonctionnalités:
  • Traitement de requêtes utilisateur
  • Défis de codage automatisés (15 min)
  • Archivage sécurisé de pages web
  • Monitoring santé des APIs
  • Gestion intelligente des quotas

💬 Exemples d'utilisation:
  • "Explique-moi la programmation asynchrone"
  • "Crée un script Python pour analyser des données"
  • "Recherche les dernières nouvelles sur l'IA"
  • "/archive https://example.com,https://site.org"
""")
                
                elif user_input.lower() == "/status":
                    status = decentralized_system.get_system_status()
                    print(f"""
📊 STATUT SYSTÈME:
  • État: {status['status']}
  • Cerveaux actifs: {status['active_brains']}/7
  • Cerveau pour la prochaine requête: {status.get('brain_status', {}).get('active_brain_for_next_request', 'N/A')}
  • Dernière activité: {datetime.fromtimestamp(status['last_activity']).strftime('%H:%M:%S')}
""")
                
                elif user_input.lower() == "/brains":
                    brain_status = brain_coordinator.get_brain_status()
                    print("\n🧠 ÉTAT DES CERVEAUX:")
                    for brain, healthy in brain_status['brain_health'].items():
                        load = brain_status['brain_load'].get(brain, 0)
                        status_icon = "✅" if healthy else "❌"
                        print(f"  {status_icon} {brain}: Charge {load}")
                
                elif user_input.lower() == "/quotas":
                    quotas = quota_manager.get_all_quotas_status()
                    print("\n📊 ÉTAT DES QUOTAS:")
                    for api, quota_info in quotas.items():
                        if isinstance(quota_info, dict) and 'error' not in quota_info:
                            usage = quota_info['current_usage']
                            limit = quota_info['limit']
                            remaining = quota_info['remaining']
                            percent = (usage / limit * 100) if limit > 0 else 0
                            print(f"  {api}: {usage}/{limit} ({percent:.1f}%) - Restant: {remaining}")
                
                elif user_input.lower() == "/challenges":
                    if decentralized_system.coding_system:
                        stats = decentralized_system.coding_system.get_challenge_statistics()
                        print(f"""
🎯 STATISTIQUES DÉFIS DE CODAGE:
  • Total défis: {stats.get('total_challenges', 0)}
  • Participants: {stats.get('total_participants', 0)}
  • Succès: {stats.get('total_successful', 0)}
  • Taux succès: {stats.get('average_success_rate', 0):.1f}%
  • Statut: {'🔄 Actif' if stats.get('is_running') else '⏹️ Arrêté'}
""")
                    else:
                        print("❌ Système de défis non initialisé")
                
                elif user_input.startswith("/archive "):
                    urls_str = user_input[9:].strip()
                    if urls_str:
                        urls = [url.strip() for url in urls_str.split(',') if url.strip()]
                        if urls:
                            print(f"🗂️ Archivage de {len(urls)} URL(s) en cours...")
                            result = await fetch_and_archive_pages(urls, "console_user")
                            print(f"✅ {result.get('tool_output', 'Archivage terminé')}")
                        else:
                            print("❌ Aucune URL valide fournie")
                    else:
                        print("❌ Usage: /archive <url1>,<url2>,...")
                
                else:
                    # Traitement d'une requête normale
                    print("🤔 Traitement en cours...")
                    
                    # Détection simple d'image (simulation)
                    image_data = None
                    if any(ext in user_input.lower() for ext in ['.png', '.jpg', '.jpeg', '.gif']):
                        print("🖼️ Image détectée (mode simulation)")
                        image_data = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="
                    
                    # Traitement par le système décentralisé
                    response = await decentralized_system.handle_user_request(
                        user_query=user_input,
                        user_id="console_user",
                        image_data=image_data
                    )
                    
                    # Affichage de la réponse
                    if "error" in response:
                        print(f"❌ Erreur ({response.get('brain_id', 'UNKNOWN')}): {response['error']}")
                    else:
                        brain_id = response.get('brain_id', 'UNKNOWN')
                        processing_time = response.get('processing_time', 0)
                        
                        # Extraction de la réponse selon le format
                        if 'response' in response and isinstance(response['response'], dict):
                            candidates = response['response'].get('candidates', [])
                            if candidates and 'content' in candidates[0]:
                                parts = candidates[0]['content'].get('parts', [])
                                if parts and 'text' in parts[0]:
                                    answer = parts[0]['text']
                                else:
                                    answer = str(response['response'])
                            else:
                                answer = str(response['response'])
                        else:
                            answer = str(response.get('response', 'Aucune réponse'))
                        
                        print(f"\n🤖 {brain_id} ({processing_time:.2f}s): {answer}")
                        
                        # Affichage des outils utilisés
                        if 'tool_results' in response and response['tool_results']:
                            print(f"\n🔧 Outils utilisés: {len(response['tool_results'])}")
                            for tool in response['tool_results']:
                                tool_name = tool.get('tool_name', 'Inconnu')
                                print(f"  • {tool_name}")
                
            except EOFError:
                print("\n👋 Au revoir !")
                break
            except KeyboardInterrupt:
                print("\n⚠️ Interruption détectée...")
                break
            except Exception as e:
                print(f"❌ Erreur: {e}")
                log_message(f"Erreur console: {e}", level="error")
        
    except Exception as e:
        log_message(f"Erreur critique dans main(): {e}", level="critical")
        log_message(f"Traceback: {traceback.format_exc()}", level="critical")
    
    finally:
        # Arrêt propre du système
        await decentralized_system.shutdown()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n🛑 Arrêt forcé du système")
    except Exception as e:
        print(f"❌ Erreur fatale: {e}"}



import re
import hashlib
import asyncio
import httpx
import io
from datetime import datetime
from urllib.parse import urlparse

# ----------------------------
# CONFIGURATION CONSTANTS
# ----------------------------
MAX_CHUNK_SIZE = 5 * 1024 * 1024  # 5MB
PRIVATE_GROUP_ID = "VOTRE_GROUPE_ID"  # À remplacer par l'ID réel

# ----------------------------
# URL DEFANGER
# ----------------------------
class URLDefanger:
    """
    Neutralise les URLs pour empêcher les clics accidentels
    et bloque les trackers connus
    """
    def __init__(self, mode="secure"):
        self.mode = mode
        self.url_pattern = re.compile(r'https?://[^\s\]]+')
    
    def _generate_hash(self, url):
        """Génère un identifiant unique pour l'URL"""
        return hashlib.sha256(url.encode()).hexdigest()[:8]
    
    def defang_url(self, url):
        """Transforme une URL en version sécurisée"""
        if "doubleclick.net" in url:
            return "[TRACKER_BLOQUÉ]"
        
        if self.mode == "secure":
            return f"[URL_BLOQUÉE:#{self._generate_hash(url)}]"
        else:
            parsed = urlparse(url)
            return f"[URL:{parsed.netloc}/...#{self._generate_hash(url)}]"
    
    def defang_text(self, text):
        """Nettoie tout le contenu texte"""
        return self.url_pattern.sub(
            lambda m: self.defang_url(m.group(0)), 
            text
        )

# ----------------------------
# PAGE ARCHIVER
# ----------------------------
class SecurePageArchiver:
    """
    Télécharge, sécurise et archive des pages web
    avec gestion des gros fichiers et protection anti-tracking
    """
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
            "Accept-Language": "fr-FR,fr;q=0.9",
            "Accept-Encoding": "gzip, deflate"
        }
    
    async def fetch_page(self, url):
        """Télécharge une page avec gestion robuste des erreurs"""
        try:
            async with httpx.AsyncClient(
                timeout=30.0,
                headers=self.headers,
                follow_redirects=True,
                http2=True
            ) as client:
                return await client.get(url)
        except Exception as e:
            print(f"🚨 Erreur de téléchargement [{url}]: {str(e)[:200]}")
            return None

    async def secure_content(self, url, content):
        """Applique les protections de sécurité au contenu"""
        header = (
            f"⚠️ ATTENTION - NE PAS CLIQUER LES LIENS ⚠️\n"
            f"URL originale : {url}\n"
            f"Horodatage : {datetime.utcnow().isoformat()}\n"
            f"----------------------------------------\n\n"
        )
        return header + URLDefanger().defang_text(content)

    async def send_chunk(self, chunk, url, user_id, chunk_index):
        """Envoie un fragment de contenu sécurisé"""
        fname = (
            f"SAFE_{user_id}_"
            f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_"
            f"p{chunk_index}.txt"
        )
        
        # En production, décommenter ces lignes :
        # await bot_instance.send_document(
        #     chat_id=PRIVATE_GROUP_ID,
        #     document=io.BytesIO(chunk.encode()),
        #     caption=f"🛡️ Fragment {chunk_index+1} | {url[:30]}...",
        #     filename=fname
        # )
        print(f"[SIMULATION] Envoi fragment {chunk_index}: {fname}")

# ----------------------------
# MAIN ARCHIVING FUNCTION
# ----------------------------
async def fetch_and_archive_pages(links, user_id, context=None):
    """
    Télécharge, sécurise et archive des pages web
    Version optimisée avec :
    - Désactivation des liens dangereux
    - Découpage des gros fichiers
    - Protection contre les trackers
    """
    archiver = SecurePageArchiver()
    defanger = URLDefanger(mode="secure")
    
    for idx, url in enumerate(links):
        try:
            # Phase 1: Téléchargement
            response = await archiver.fetch_page(url)
            if not response or response.status_code != 200:
                print(f"❌ Échec téléchargement [{url}]")
                continue
                
            # Phase 2: Sécurisation du contenu
            secured = await archiver.secure_content(url, response.text)
            
            # Phase 3: Découpage et envoi
            chunks = [
                secured[i:i+MAX_CHUNK_SIZE] 
                for i in range(0, len(secured), MAX_CHUNK_SIZE)
            ]
            
            for i, chunk in enumerate(chunks):
                await archiver.send_chunk(chunk, url, user_id, i)
            
            # En production, décommenter :
            # append_long_memory(user_id, f"Page archivée: {url}")
            # append_chat_history(user_id, "page", url)
            
            print(f"✅ Archivage réussi: {url} ({len(chunks)} fragments)")

        except Exception as e:
            error_msg = f"⛑️ Erreur d'archivage [{url}]: {str(e)[:200]}"
            # log_api_error(error_msg)
            print(error_msg)

# ----------------------------
# EXEMPLE D'UTILISATION
# ----------------------------
async def main():
    # Liste de test avec sites variés
    test_urls = [
        "https://www.example.com",
        "https://fr.wikipedia.org",
        "https://www.gouvernement.fr"
    ]
    
    await fetch_and_archive_pages(test_urls, "user_12345")

if __name__ == "__main__":
    asyncio.run(main()) 


# -*- coding: utf-8 -*-
# Challenge généré par DeepSeek-R1
# Date: 2025-07-17
# Type: OPTIMISATION

# ----------------------------
# IA/DEV TOOLS (pyflakes, black, ast, etc.)
# ----------------------------

import os
import io
import contextlib
import subprocess
import asyncio
import time
import random
import json
import base64
import httpx
import ast
import difflib
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from functools import lru_cache

# Assuming these are available in the environment or defined elsewhere
# from pyflakes.api import check, Reporter
# import black
# import pytesseract
# from PIL import Image
# from your_config_module import config, PRIVATE_GROUP_ID, DAILY_CHALLENGE_PATH, API_CONFIG
# from your_orchestrator_module import orchestrator, quota_manager, endpoint_health_manager
# from your_memory_module import memory_manager
# from your_bot_types import ContextTypes
# from your_cig_module import CIG # Assuming CIG is a function for AI calls

# Global variable for verbose mode
VERBOSE = os.getenv("VERBOSE_MODE", "false").lower() == "true"

def syntax_highlight(code: str) -> str:
    """
    Met en évidence la syntaxe du code Python pour l'affichage dans un terminal,
    si pygments est disponible et si la sortie est un TTY.
    """
    from sys import stdout
    if stdout.isatty():  # Uniquement dans un terminal
        try:
            from pygments import highlight
            from pygments.lexers import PythonLexer
            from pygments.formatters import TerminalFormatter
            return highlight(code, PythonLexer(), TerminalFormatter(bg="dark"))
        except ImportError:
            # Fallback if pygments is not installed
            if VERBOSE:
                print("[DEBUG] Pygments non trouvé, retour du code brut pour la coloration syntaxique.")
            pass
    return code  # Retour brut si couleurs non disponibles ou non TTY

def check_code(code: str) -> str:
    """
    Vérifie le code Python pour les erreurs de style et les problèmes potentiels
    en utilisant pyflakes (simulation si non disponible).
    """
    try:
        # Assuming Reporter and check are imported from pyflakes.api
        from pyflakes.api import check, Reporter
        out = io.StringIO()
        reporter = Reporter(out, out)
        check(code, filename="<string>", reporter=reporter)
        return out.getvalue()
    except ImportError:
        if VERBOSE:
            print("[DEBUG] Pyflakes non trouvé, simulation de la vérification de code.")
        # Simple simulation if pyflakes is not available
        warnings = []
        if "import os" in code and ("os.remove" in code or "os.system" in code):
            warnings.append("AVERTISSEMENT: Utilisation potentielle de fonctions 'os' dangereuses détectée.")
        if "while True" in code and "sleep" not in code:
            warnings.append("AVERTISSEMENT: Boucle infinie potentielle détectée sans pause.")
        return "\n".join(warnings) if warnings else "Aucun problème majeur détecté (simulation)."


def format_code(code: str, max_length: int = 1000) -> str:
    """
    Formate le code Python en utilisant Black et tronque la sortie si elle est trop longue.
    """
    try:
        # Assuming black is imported
        import black
        formatted = black.format_str(code, mode=black.Mode())
        if len(formatted) > max_length:
            return (
                f"⚠️ Code formaté (tronqué à {max_length} caractères)\n"
                "━━━━━━━━━━━━━━━━━━\n"
                f"{formatted[:max_length//2]}\n...\n{formatted[-max_length//2:]}\n"
                "━━━━━━━━━━━━━━━━━━"
            )
        return formatted
    except ImportError:
        if VERBOSE:
            print("[DEBUG] Black non trouvé, retour du code brut pour le formatage.")
        return code # Return original code if black is not installed
    except Exception as e:
        return f"❌ Format error: {e}"

def extract_functions(code: str):
    """
    Extrait les noms des fonctions définies dans le code Python.
    """
    try:
        tree = ast.parse(code)
        return [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
    except Exception as e:
        return f"❌ AST error: {e}"

@lru_cache(maxsize=100)
def analyze_code_structure(code: str):
    """
    Analyse la structure AST du code Python et retourne une représentation indentée.
    Utilise un cache LRU pour les analyses répétées.
    """
    try:
        tree = ast.parse(code)
        return ast.dump(tree, indent=2)  # Indentation ajoutée
    except Exception as e:
        return f"❌ Erreur d'analyse AST: {e}"

def read_image_text(image_path: str) -> str:
    """
    Extrait le texte d'une image en utilisant Tesseract OCR.
    """
    try:
        # Assuming pytesseract and Image are imported
        import pytesseract
        from PIL import Image
        return pytesseract.image_to_string(Image.open(image_path))
    except ImportError:
        return "❌ OCR error: pytesseract ou Pillow non installés."
    except Exception as e:
        return f"❌ OCR error: {e}"

def run_python(code_str: str):
    """
    Exécute une chaîne de code Python dans l'environnement actuel.
    """
    try:
        exec_globals = {}
        exec(code_str, exec_globals)
        return exec_globals
    except Exception as e:
        return f"❌ Python error: {e}"

def run_shell(cmd: str) -> str:
    """
    Exécute une commande shell et retourne sa sortie.
    """
    try:
        result = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
        return result.decode()
    except subprocess.CalledProcessError as e:
        return f"❌ Shell error: {e.output.decode()}"

# Pour les exécutions en sandbox
executor = ThreadPoolExecutor(max_workers=1)

async def run_in_sandbox(code: str, language: str = "python") -> str:
    """
    Exécute du code Python ou Shell dans une sandbox sécurisée avec mesure du temps.
    """
    start_time = time.perf_counter()
    
    # Assuming filter_bad_code is defined elsewhere
    # if filter_bad_code(code):
    #     return "❌ Sécurité: Le code contient des motifs potentiellement dangereux et n'a pas été exécuté."
    
    # Placeholder for filter_bad_code if not defined
    if "import os" in code and ("os.remove" in code or "os.system" in code):
        return "❌ Sécurité: Le code contient des motifs potentiellement dangereux (os.remove/os.system) et n'a pas été exécuté."


    loop = asyncio.get_running_loop()
    if language == "python":
        result = await loop.run_in_executor(executor, _run_python_sync, code)
    elif language == "shell":
        result = await loop.run_in_executor(executor, _run_shell_sync, code)
    else:
        result = "❌ Langage non supporté pour la sandbox."
    
    elapsed = time.perf_counter() - start_time
    if "❌" in result:
        return f"{result}\n⏱️ Temps avant erreur: {elapsed:.2f}s"
    return f"{result}\n⌛ Exécuté en: {elapsed:.2f}s"

def _run_python_sync(code: str) -> str:
    """
    Exécute le code Python de manière synchrone dans un environnement restreint.
    Capture la sortie standard et les erreurs.
    """
    old_stdout = io.StringIO()
    old_stderr = io.StringIO()
    with contextlib.redirect_stdout(old_stdout), contextlib.redirect_stderr(old_stderr):
        try:
            # Restrict builtins for security
            exec(code, {'__builtins__': {}})
            output = old_stdout.getvalue()
            error = old_stderr.getvalue()
            if error:
                return f"🐍 Sortie partielle:\n{output}\n\n🔴 Erreurs:\n{error}"
            return f"✅ Sortie:\n{output}"
        except Exception as e:
            return (
                "❌ ERREUR D'EXÉCUTION\n"
                f"Type: {type(e).__name__}\n"
                f"Détails: {str(e)}\n\n"
                "--- Sortie standard ---\n"
                f"{old_stdout.getvalue()}\n\n"
                "--- Logs d'erreur ---\n"
                f"{old_stderr.getvalue()}"
            )

def _run_shell_sync(command: str) -> str:
    """
    Exécute une commande shell de manière synchrone et sécurisée.
    """
    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            check=True,
            timeout=10
        )
        output = result.stdout
        error = result.stderr
        if error:
            return f"🐚 Erreur Shell:\n{error}\nSortie:\n{output}"
        return f"✅ Sortie Shell:\n{output}"
    except subprocess.CalledProcessError as e:
        return f"❌ Erreur d'exécution Shell (Code: {e.returncode}):\n{e.stderr}\nSortie:\n{e.stdout}"
    except subprocess.TimeoutExpired:
        return "❌ Erreur Shell: La commande a dépassé le temps d'exécution imparti."
    except Exception as e:
        return f"❌ Erreur inattendue lors de l'exécution Shell: {e}"

async def analyze_python_code(code: str) -> str:
    """
    Analyse le code Python pour la syntaxe, la compilation et les problèmes potentiels
    (comme l'utilisation de os.remove).
    """
    try:
        ast.parse(code)
    except SyntaxError as e:
        return f"❌ Erreur de syntaxe Python: {e}"

    # Ajout pour attraper les erreurs de compilation (ex: variables non définies)
    try:
        compile(code, '<string>', 'exec')
    except Exception as e:
        return f"❌ Erreur de compilation Python: {e}\nCode analysé:\n{code[:500]}"

    formatted_code = code # Assuming format_code would be called here if needed
    pyflakes_output = []
    if "import os" in code and "os.remove" in code:
        pyflakes_output.append("AVERTISSEMENT: Utilisation potentielle de os.remove détectée.")

    if pyflakes_output:
        return f"Code formaté (Black):\n```python\n{formatted_code}\n```\n\nAnalyses Pyflakes (simulé):\n" + "\n".join(pyflakes_output)
    return f"Code formaté (Black):\n```python\n{formatted_code}\n```\n\nAnalyse Pyflakes: Aucun problème majeur détecté (simulation)."

# --- OCR Tool ---
async def perform_ocr(image_url: str, api_key: str, endpoint: str) -> str:
    """
    Effectue une reconnaissance optique de caractères (OCR) sur une image via une API externe.
    """
    try:
        async with httpx.AsyncClient(timeout=10) as client:
            img_response = await client.get(image_url)
            img_response.raise_for_status()

        img_data = base64.b64encode(img_response.content).decode('utf-8')
        headers = {"Content-Type": "application/json", "Apikey": api_key}
        payload = {"base64_image": img_data}

        ocr_endpoint = f"{endpoint}/image/recognize/extractText" if "cloudmersive" in endpoint.lower() else endpoint

        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.post(ocr_endpoint, json=payload, headers=headers)
            response.raise_for_status()
            result = response.json()

        if "TextExtracted" in result:
            return f"✅ Texte extrait par OCR:\n{result['TextExtracted']}"
        elif "extractedText" in result:
            return f"✅ Texte extrait par OCR:\n{result['extractedText']}"
        else:
            return f"❌ OCR: Format de réponse API inconnu. Réponse brute: {result}"

    except httpx.HTTPStatusError as e:
        # Assuming log_message is defined elsewhere
        # log_message(f"Erreur HTTP/réseau lors de l'OCR: {e.response.status_code} - {e.response.text}", level="error")
        if VERBOSE:
            print(f"[ERROR] Erreur HTTP/réseau lors de l'OCR: {e.response.status_code} - {e.response.text}")
        return f"❌ Erreur lors de l'OCR (réseau/API): {e}"
    except httpx.RequestError as e:
        # log_message(f"Erreur de requête lors de l'OCR: {e}", level="error")
        if VERBOSE:
            print(f"[ERROR] Erreur de requête lors de l'OCR: {e}")
        return f"❌ Erreur lors de l'OCR (requête): {e}"
    except json.JSONDecodeError:
        return "❌ OCR: Réponse non JSON de l'API."
    except Exception as e:
        # log_message(f"Erreur inattendue lors de l'OCR: {e}", level="error")
        if VERBOSE:
            print(f"[ERROR] Erreur inattendue lors de l'OCR: {e}")
        return f"❌ Erreur inattendue lors de l'OCR: {e}"


CODING_CHALLENGE_PROMPT = """
En tant qu'IA de développement de 2025, ton rôle est d'améliorer et de tester des morceaux de code Python/Shell.
Tu as accès à une sandbox sécurisée pour exécuter le code.
Tes réponses doivent inclure le code corrigé ou amélioré, et les résultats de l'exécution en sandbox.
Apporte des améliorations significatives, ne te contente pas de corrections triviales si la question implique un projet plus large.
Pense à l'efficacité du code et à l'optimisation des ressources.
Chaque version doit être une amélioration nette de la précédente, inédite.
Commence par un commentaire indiquant ce qui a été amélioré.
Le code doit être direct, lisible, et prêt à être utilisé.
"""

# -------------------------------------------------------------------------
# CODING CHALLENGE TOUTES IA EN PARALLÈLE
# -------------------------------------------------------------------------
CODING_CHALLENGE_ENABLED = True
# Assuming DAILY_CHALLENGE_PATH is a Path object from pathlib
# LAST_CHALLENGE_FILE = DAILY_CHALLENGE_PATH / "last_challenge.py"
# HISTORY_DIR = DAILY_CHALLENGE_PATH / "history"
# HISTORY_DIR.mkdir(exist_ok=True)

# Placeholder for DAILY_CHALLENGE_PATH and HISTORY_DIR if not defined
class PathPlaceholder:
    def __init__(self, path_str):
        self.path_str = path_str
    def __truediv__(self, other):
        return PathPlaceholder(f"{self.path_str}/{other}")
    def write_text(self, content, encoding):
        if VERBOSE:
            print(f"[DEBUG] Simulating write to {self.path_str}: {content[:100]}...")
    def mkdir(self, exist_ok=True):
        if VERBOSE:
            print(f"[DEBUG] Simulating mkdir {self.path_str}, exist_ok={exist_ok}")
    def __str__(self):
        return self.path_str

DAILY_CHALLENGE_PATH = PathPlaceholder("daily_challenges")
HISTORY_DIR = DAILY_CHALLENGE_PATH / "history"
HISTORY_DIR.mkdir(exist_ok=True)


def diff_text(old_text, new_text):
    """
    Génère un diff unifié entre deux chaînes de texte.
    """
    diff = difflib.unified_diff(
        old_text.splitlines(), new_text.splitlines(), lineterm=""
    )
    return "\n".join(diff)

async def coding_challenge_loop():
    """
    Boucle principale pour l'exécution périodique des défis de codage par les IA.
    """
    while True:
        if not CODING_CHALLENGE_ENABLED:
            await asyncio.sleep(900)
            continue

        challenge_types = [
            "ALGORITHME", "OPTIMISATION", 
            "DEBUG", "IA CREATIVE", "SCRIPT UTILE"
        ]
        challenge_type = random.choice(challenge_types)
        
        prompt = f"""
[ DÉFI {challenge_type} - {datetime.now().strftime('%Y-%m-%d')} ]
Tu es une IA experte en programmation Python. Génère un script clair, correct et optimisé, 100% en français.
Ne dis pas que tu ne sais pas, ne t'excuse pas, ne formule pas de réponses vagues.
Chaque version doit être une amélioration nette de la précédente.
Commence par un commentaire indiquant ce qui a été amélioré.
Le code doit être direct, lisible, et prêt à être utilisé.
"""

        # Version universelle pour toutes les configurations d'API
        ia_list = []
        
        # Placeholder for config object
        class ConfigPlaceholder:
            OPENROUTER_KEY = "dummy_openrouter_key"
            TAVILY_KEYS = ["dummy_tavily_key_1"]
            SERPER_KEY = "dummy_serper_key"
            HUGGINGFACE_KEYS = ["dummy_hf_key_1"]
            WOLFRAM_APP_ID = "dummy_wolfram_id"
            GOOGLE_API_KEY = "dummy_google_key"
            GOOGLE_CX_LIST = ["dummy_cx_1"]
            PRIVATE_GROUP_ID = None # Set to a dummy value if needed for testing
            bot_instance = None # Placeholder for a bot instance

        config = ConfigPlaceholder()

        # Configuration OpenRouter
        if hasattr(config, 'OPENROUTER_KEYS'):
            ia_list.extend([("OpenRouter", k) for k in config.OPENROUTER_KEYS])
        elif hasattr(config, 'OPENROUTER_KEY'):
            ia_list.append(("OpenRouter", config.OPENROUTER_KEY))
        
        # Configuration Tavily
        if hasattr(config, 'TAVILY_KEYS'):
            ia_list.extend([(f"Tavily-{i+1}", k) for i, k in enumerate(config.TAVILY_KEYS)])
        
        # Configuration Serper
        if hasattr(config, 'SERPER_KEYS'):
            ia_list.extend([("Serper", k) for k in config.SERPER_KEYS])
        elif hasattr(config, 'SERPER_KEY'):
            ia_list.append(("Serper", config.SERPER_KEY))
        
        # Configuration HuggingFace
        if hasattr(config, 'HUGGINGFACE_KEYS'):
            ia_list.extend([("HuggingFace", k) for k in config.HUGGINGFACE_KEYS])
        elif hasattr(config, 'HF_TOKEN'):
            ia_list.append(("HuggingFace", config.HF_TOKEN))
        
        # Configuration Wolfram
        if hasattr(config, 'WOLFRAM_APP_IDS'):
            ia_list.extend([("Wolfram", k) for k in config.WOLFRAM_APP_IDS])
        elif hasattr(config, 'WOLFRAM_APP_ID'):
            ia_list.append(("Wolfram", config.WOLFRAM_APP_ID))
        
        # Configuration Google
        if hasattr(config, 'GOOGLE_API_KEYS'):
            for i, k in enumerate(config.GOOGLE_API_KEYS):
                ia_list.append((f"GoogleCX-{i+1}", k))
        elif hasattr(config, 'GOOGLE_API_KEY') and hasattr(config, 'GOOGLE_CX_LIST'):
            for i, cx in enumerate(config.GOOGLE_CX_LIST):
                ia_list.append((f"GoogleCX-{i+1}", config.GOOGLE_API_KEY))

        async def call_ia(nom, cle):
            """
            Appelle une IA spécifique pour générer du code et le sauvegarde.
            """
            try:
                # Assuming CIG is defined elsewhere for AI calls
                # r = await CIG(prompt, api_key=cle, model_name=nom)
                r = f"# Code généré par {nom} pour le défi {challenge_type}\nprint('Hello from {nom}')" # Dummy response for testing
                
                if r and len(r.strip()) > 20:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    fpath = DAILY_CHALLENGE_PATH / f"challenge_{nom}_{timestamp}.py"
                    header = (
                        f"# -*- coding: utf-8 -*-\n"
                        f"# Challenge généré par {nom}\n"
                        f"# Date: {timestamp}\n"
                        f"# Type: {challenge_type}\n\n"
                    )
                    with open(str(fpath), "w", encoding="utf-8") as f: # Use str(fpath) for PathPlaceholder compatibility
                        f.write(header + r.strip())
                    
                    if hasattr(config, 'PRIVATE_GROUP_ID') and hasattr(config, 'bot_instance') and config.PRIVATE_GROUP_ID and config.bot_instance:
                        await config.bot_instance.send_message(
                            chat_id=config.PRIVATE_GROUP_ID,
                            text=f"💻 <b>Code généré par {nom}</b> :\n<pre>{r[:600]}</pre>",
                            parse_mode="HTML"
                        )
                    return r
                else:
                    if hasattr(config, 'PRIVATE_GROUP_ID') and hasattr(config, 'bot_instance') and config.PRIVATE_GROUP_ID and config.bot_instance:
                        await config.bot_instance.send_message(
                            chat_id=config.PRIVATE_GROUP_ID,
                            text=f"⚠️ <b>{nom}</b> n'a pas généré de code valable cette fois.",
                            parse_mode="HTML"
                        )
                    return None
            except Exception as e:
                if hasattr(config, 'PRIVATE_GROUP_ID') and hasattr(config, 'bot_instance') and config.PRIVATE_GROUP_ID and config.bot_instance:
                    await config.bot_instance.send_message(
                        chat_id=config.PRIVATE_GROUP_ID,
                        text=f"❌ <b>{nom}</b> erreur lors de l'appel IA : {e}",
                        parse_mode="HTML"
                    )
                return None

        results = await asyncio.gather(*[call_ia(n, k) for n, k in ia_list])
        await asyncio.sleep(900)

async def start_background_tasks(app):
    """
    Démarre les tâches de fond pour le défi de codage.
    """
    asyncio.create_task(coding_challenge_loop())
    

async def coding_challenge_periodic(context): # ContextTypes.DEFAULT_TYPE
    """
    Tâche périodique pour lancer les défis de codage et gérer les réponses des IA.
    """
    # Assuming log_message is defined elsewhere
    # log_message("Lancement de la tâche de défi de codage...")
    if VERBOSE:
        print("[DEBUG] Lancement de la tâche de défi de codage...")
    
    # Placeholder for memory_manager, orchestrator, quota_manager, PRIVATE_GROUP_ID
    class DummyMemoryManager:
        async def get_group_memory(self, group_id, limit): return "Dummy memory data."
        def update_ia_status(self, ia_name, status, error=None): pass
        async def save_group_memory(self, group_id, role, content): pass
    
    class DummyOrchestrator:
        def __init__(self):
            self.api_clients = {"DEEPSEEK": self, "HUGGINGFACE": self, "GEMINI_API": self}
        async def query(self, prompt): return f"# Dummy code from {self.name}\nprint('Hello from {self.name}')"
        def get(self, name):
            self.name = name
            return self

    class DummyQuotaManager:
        async def check_and_update_quota(self, ia_name): return True

    memory_manager = DummyMemoryManager()
    orchestrator = DummyOrchestrator()
    quota_manager = DummyQuotaManager()
    PRIVATE_GROUP_ID = "dummy_group_id" # Set a dummy ID if context.bot.send_message is used

    group_mem = await memory_manager.get_group_memory(PRIVATE_GROUP_ID, limit=50)
    full_prompt = f"{CODING_CHALLENGE_PROMPT}\n\nMémoire récente du groupe:\n{group_mem}\n\nDéfi du jour:"
    
    relevant_ias = ["DEEPSEEK", "HUGGINGFACE", "GEMINI_API"]
    results = []
    
    for ia_name in relevant_ias:
        ia_client = orchestrator.api_clients.get(ia_name)
        if ia_client and await quota_manager.check_and_update_quota(ia_name):
            try:
                if VERBOSE:
                    print(f"[DEBUG] IA {ia_name} tente de résoudre le défi...")
                # log_message(f"IA {ia_name} tente de résoudre le défi...")
                resp = await ia_client.query(full_prompt)
                results.append((ia_name, resp))
                memory_manager.update_ia_status(ia_name, True)
            except Exception as e:
                # log_message(f"Erreur avec {ia_name}: {e}", level="error")
                if VERBOSE:
                    print(f"[ERROR] Erreur avec {ia_name}: {e}")
                memory_manager.update_ia_status(ia_name, False, str(e))
            await asyncio.sleep(0.5)

    if results:
        for name, resp in results:
            code_content = resp
            if resp.startswith("```python") and resp.endswith("```"):
                code_content = resp[9:-3].strip()
            
            fname = f"challenge_{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py"
            fpath = DAILY_CHALLENGE_PATH / fname
            header = (
                f"# -*- coding: utf-8 -*-\n"
                f"# Challenge généré par {name}\n"
                f"# Date: {datetime.now().strftime('%Y%m%d_%H%M%S')}\n"
                f"# Type: CODING_CHALLENGE\n\n"
            )
            try:
                with open(str(fpath), "w", encoding="utf-8") as f: # Use str(fpath) for PathPlaceholder compatibility
                    f.write(header + code_content)
                # log_message(f"Défi sauvegardé: {fpath}")
                if VERBOSE:
                    print(f"[DEBUG] Défi sauvegardé: {fpath}")
            except Exception as e:
                # log_message(f"Erreur sauvegarde {fname}: {e}", level="error")
                if VERBOSE:
                    print(f"[ERROR] Erreur sauvegarde {fname}: {e}")

            # Assuming context.bot.send_message is available
            if PRIVATE_GROUP_ID and hasattr(context, 'bot') and hasattr(context.bot, 'send_message'):
                display_code = code_content[:1500] + "..." if len(code_content) > 1500 else code_content
                await context.bot.send_message(
                    chat_id=PRIVATE_GROUP_ID,
                    text=f"💻 <b>Défi {name}</b> :\n<pre>{display_code}</pre>",
                    parse_mode="HTML"
                )
            
            await memory_manager.save_group_memory(
                PRIVATE_GROUP_ID, 
                "bot", 
                f"Défi codage {name} : {code_content[:100]}"
            )
    else:
        # log_message("Aucune IA n'a généré de code valide", level="warning")
        if VERBOSE:
            print("[DEBUG] Aucune IA n'a généré de code valide")
        if PRIVATE_GROUP_ID and hasattr(context, 'bot') and hasattr(context.bot, 'send_message'):
            await context.bot.send_message(
                chat_id=PRIVATE_GROUP_ID,
                text="😔 Aucune IA n'a pu générer de code pour le défi cette fois-ci."
            )

async def periodic_health_check(context): # ContextTypes.DEFAULT_TYPE
    """
    Effectue des vérifications de santé périodiques pour les services API.
    """
    # log_message("Lancement des health checks périodiques...")
    if VERBOSE:
        print("[DEBUG] Lancement des health checks périodiques...")
    
    # Placeholder for API_CONFIG and endpoint_health_manager
    class DummyEndpointHealthManager:
        async def run_health_check_for_service(self, service_name):
            if VERBOSE:
                print(f"[DEBUG] Simulating health check for {service_name}")

    API_CONFIG = {"SERVICE_A": {}, "SERVICE_B": {}}
    endpoint_health_manager = DummyEndpointHealthManager()

    for service_name in API_CONFIG.keys():
        await endpoint_health_manager.run_health_check_for_service(service_name)
    # log_message("Health checks terminés.")
    if VERBOSE:
        print("[DEBUG] Health checks terminés.")

async def send_structured_report(context, report_data: dict): # ContextTypes.DEFAULT_TYPE
    """
    Envoie un rapport structuré d'action de l'IA à un groupe privé.
    """
    try:
        report_text = f"📊 **Rapport d'Action IA**\n\n"
        report_text += f"**Timestamp**: `{report_data.get('timestamp')}`\n"
        report_text += f"**Agent**: `{report_data.get('agent_name')}`\n"
        report_text += f"**Intention**: `{report_data.get('intention')}`\n"
        report_text += f"**Requête**: `{report_data.get('user_query')}`\n"
        
        primary_ai = report_data.get('primary_ai_used', 'N/A')
        if isinstance(primary_ai, dict) and 'name' in primary_ai:
            primary_ai = primary_ai['name']
        report_text += f"**IA Primaire**: `{primary_ai}`\n"
        
        tools = report_data.get('tools_called', [])
        if tools:
            report_text += "**Outils Appelés**:\n"
            for tool in tools:
                tool_result = str(tool['result'])
                if len(tool_result) > 100:
                    tool_result = tool_result[:100] + "..."
                escaped_params = json.dumps(tool['params'], indent=2)
                escaped_params = escaped_params.replace('_', '\\_').replace('*', '\\*').replace('`', '\\`')
                report_text += f"- `{tool['name']}` (Params: ```json\n{escaped_params}\n```, Résultat: `{tool_result}`)\n"
        else:
            report_text += "**Outils Appelés**: Aucun\n"
        
        final_resp = report_data.get('final_response', '')
        if len(final_resp) > 500:
            final_resp = final_resp[:500] + "..."
        final_resp = final_resp.replace('_', '\\_').replace('*', '\\*').replace('`', '\\`')
        report_text += f"**Réponse Finale**: `{final_resp}`\n"
        report_text += f"**Durée**: `{report_data.get('duration', 0):.2f}s`\n"
        report_text += f"**Erreur**: `{report_data.get('error', 'Non')}`\n"

        if PRIVATE_GROUP_ID and hasattr(context, 'bot') and hasattr(context.bot, 'send_message'):
            await context.bot.send_message(
                chat_id=PRIVATE_GROUP_ID, 
                text=report_text, 
                parse_mode='MarkdownV2'
            )
    except Exception as e:
        # log_message(f"Erreur envoi rapport: {e}", level="error")
        if VERBOSE:
            print(f"[ERROR] Erreur envoi rapport: {e}")

def show_help():
    """
    Affiche un menu d'aide avec les commandes disponibles.
    """
    print("""
📋 AIDE :
- /run [code] : Exécute du Python
- /format [code] : Formate du code
- /demo : Mode démo
- /help : Affiche ce menu
""")

def show_version():
    """
    Affiche la version actuelle du bot et la date de dernière mise à jour.
    """
    VERSION = "1.1.0"
    LAST_UPDATE = "2023-11-20"
    print(f"🤖 Bot Version {VERSION} | {LAST_UPDATE}")

def fix_common_errors(code: str) -> str:
    """
    Applique des corrections automatiques basiques au code.
    """
    fixes = {
        "print(": "print(",  # Example: corrects unclosed quotes (if any)
        "def  ": "def ",     # Extra spaces
        "= =": "=="          # Misspelled operator
    }
    for error, fix in fixes.items():
        code = code.replace(error, fix)
    return code

def format_error(e) -> str:
    """
    Formate une exception en un message d'erreur clair et visuel.
    """
    return f"""
⚠️⚠️⚠️ ERREUR ⚠️⚠️⚠️
Type : {type(e).__name__}
Message : {str(e)}
——————————————
"""

blagues = [
    "Pourquoi les devs préfèrent le noir ? Parce que la lumière attire les bugs 🐛",
    "Comment un dev nettoie-t-il sa maison ? Il fait rm -rf / 😱",
    "2 devs se rencontrent. Le premier dit 'Ça va ?'. Le second répond '404' 😐"
]

def check_inactivity(last_activity: float):
    """
    Vérifie l'inactivité du bot et affiche une blague si le seuil est dépassé.
    """
    if time.time() - last_activity > 300:  # 5 min
        print("💤 Je m'ennuie... Tiens, une blague :")
        print(random.choice(blagues))
        return True # Indicate that a joke was told
    return False

def warmup_ai(model, iterations: int = 3):
    """
    Préchauffe un modèle d'IA avec des requêtes factices pour réduire la latence initiale.
    """
    dummy_prompts = ["print('hello')", "def test(): pass", "1+1"]
    for _ in range(iterations):
        for prompt in dummy_prompts:
            # Assuming model has a .generate method
            if hasattr(model, 'generate'):
                model.generate(prompt)
            else:
                if VERBOSE:
                    print(f"[DEBUG] Modèle {model} n'a pas de méthode 'generate' pour le préchauffage.")


@lru_cache(maxsize=100)
def generate_code_cached(prompt: str, temperature: float = 0.7) -> str:
    """
    Génère du code en utilisant un modèle d'IA avec un cache pour les prompts répétés.
    """
    # Assuming ai_model is defined globally or passed
    # return ai_model.generate(prompt, temperature=temperature)
    return f"# Cached code for: {prompt[:50]}" # Dummy return

def batch_generate(prompts: list[str], max_workers: int = 4) -> list[str]:
    """
    Génère du code pour plusieurs prompts en parallèle en utilisant un pool de threads.
    """
    # Assuming ai_model is defined globally or passed
    # This would require ai_model to be thread-safe or a new instance per thread
    def _generate_single(p):
        # return ai_model.generate(p)
        return f"# Batch generated code for: {p[:50]}" # Dummy return

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        return list(executor.map(_generate_single, prompts))

def adaptive_temp(prompt: str) -> float:
    """
    Adapte la température de génération de l'IA en fonction du contenu du prompt.
    Réduit la température pour les prompts techniques.
    """
    technical_keywords = ["optimiser", "algorithme", "complexité", "performance", "debug"]
    return 0.3 if any(kw in prompt.lower() for kw in technical_keywords) else 0.7

# Example of how to call show_version and show_help if this were a main script
if __name__ == "__main__":
    show_version()
    show_help()
    # Example of using verbose mode
    # os.environ["VERBOSE_MODE"] = "true"
    # VERBOSE = os.getenv("VERBOSE_MODE", "false").lower() == "true"
    # print(f"Verbose mode is {'ON' if VERBOSE else 'OFF'}")

    # Example of a dummy context for periodic tasks
    class DummyBot:
        async def send_message(self, chat_id, text, parse_mode):
            print(f"\n--- BOT MESSAGE to {chat_id} ({parse_mode}) ---\n{text}\n--------------------")

    class DummyContext:
        def __init__(self):
            self.bot = DummyBot()

    # To run async functions for demonstration
    async def main_demo():
        print("\n--- Running a dummy coding challenge periodic task ---")
        await coding_challenge_periodic(DummyContext())
        print("\n--- Running a dummy health check ---")
        await periodic_health_check(DummyContext())
        
        print("\n--- Testing format_error ---")
        try:
            1/0
        except Exception as e:
            print(format_error(e))

        print("\n--- Testing run_in_sandbox (python error) ---")
        result_py_error = await run_in_sandbox("print(undefined_variable)", language="python")
        print(result_py_error)

        print("\n--- Testing run_in_sandbox (shell success) ---")
        result_shell_success = await run_in_sandbox("echo 'Hello from shell'", language="shell")
        print(result_shell_success)

        print("\n--- Testing analyze_python_code (compilation error) ---")
        code_with_compilation_error = "def my_func():\n    x = y + 1"
        analysis_result = await analyze_python_code(code_with_compilation_error)
        print(analysis_result)

        print("\n--- Testing check_inactivity ---")
        last_activity = time.time() - 301 # 5 min and 1 second ago
        check_inactivity(last_activity)
        last_activity = time.time() - 10 # 10 seconds ago
        check_inactivity(last_activity) # Should not print a joke

        print("\n--- Testing adaptive_temp ---")
        print(f"Temp for 'optimiser un algorithme': {adaptive_temp('optimiser un algorithme')}")
        print(f"Temp for 'write a story': {adaptive_temp('write a story')}")

    # Run the async demo
    # asyncio.run(main_demo())


# config.py
import os
import json
from datetime import datetime, timezone, date, timedelta
from pathlib import Path

# ----------------------------
# CONFIGURATION & CONSTANTES GLOBALES
# ----------------------------

# ==== Chemins de fichiers & Limites ====
# Assure que le temps est toujours en UTC pour une cohérence globale
os.environ["TZ"] = "UTC"

# Répertoire de base pour toutes les sauvegardes et données
BASE_DIR = Path(__file__).resolve().parent / "sauvegardes"
# Chemin du fichier de log des erreurs critiques
ERROR_LOG_PATH = BASE_DIR / "erreurs.log"
# Chemin du fichier de log général du bot (pour le suivi des opérations)
LOG_FILE = BASE_DIR / "bot_log.log"

# Répertoires spécifiques pour les données utilisateur et les défis de code
DAILY_CHALLENGE_PATH = Path(__file__).resolve().parent / "defis_code"
HISTORY_DIR = DAILY_CHALLENGE_PATH / "history" # Pour l'historique des défis de code

# Fichiers globaux pour le statut des IA et les quotas
IA_STATUS_FILE = BASE_DIR / "ia_status.json"
QUOTAS_FILE = BASE_DIR / "quotas.json"
ENDPOINT_HEALTH_FILE = BASE_DIR / "endpoint_health.json"

# Fichiers spécifiques à l'utilisateur (stockés dans sauvegardes/{user_id}/)
USER_CHAT_HISTORY_FILE = "chat_history.json"
USER_LONG_MEMORY_FILE = "long_term_memory.json"
GROUP_CHAT_HISTORY_FILE = "group_chat_history.json" # Pour la mémoire de groupe
ARCHIVES_DIR = "archives" # Sous-répertoire pour l'archivage des pages web

# Taille maximale des fichiers pour la rotation/compression des logs et l'archivage
MAX_FILE_SIZE = 5 * 1024 * 1024  # 5 MB

# Paramètres de mémoire et de cache
MAX_CACHE_SIZE = 20       # Nombre de messages récents à garder en cache pour la similarité
MAX_LONG_TERM_MEMORY = 50 # Nombre d'entrées max dans la mémoire à long terme

# Assurez-vous que les répertoires nécessaires existent
BASE_DIR.mkdir(parents=True, exist_ok=True)
DAILY_CHALLENGE_PATH.mkdir(exist_ok=True)
HISTORY_DIR.mkdir(exist_ok=True)

# ==== Telegram Bot Configuration ====
# Token de votre bot Telegram (à remplacer par votre vrai token en production)
TELEGRAM_BOT_TOKEN = "7902342551:AAG6r1QA2GTMZcmcsWHi36Ivd_PVeMXULOs"
# ID du groupe privé utilisé pour les logs, rapports et archivage
PRIVATE_GROUP_ID = -1002845235344

# ==== Configuration du Bot ====
BOT_NAME = "Assistant IA"
BOT_DESCRIPTION = "un assistant polyvalent capable de converser, d'exécuter du code, d'analyser des images et d'archiver des informations."
BOT_PERSONALITY = "toujours serviable, précis, éthique et proactif dans l'apprentissage."
BOT_INSTRUCTIONS = "Réponds aux questions, exécute les commandes, et utilise tes outils pour fournir les meilleures informations. Sois concis mais complet."

# ==== Clés API Individuelles (centralisées pour la clarté) ====
# Récupérer les clés API depuis les variables d'environnement pour la production
# ou les définir ici pour le développement local (moins sécurisé)
APIFLASH_KEY = os.getenv("APIFLASH_KEY", "3a3cc886a18e41109e0cebc0745b12de")
DEEPSEEK_KEY_1 = os.getenv("DEEPSEEK_KEY_1", "sk-ef08317d125947b3a1ce5916592bef00")
DEEPSEEK_KEY_2 = os.getenv("DEEPSEEK_KEY_2", "sk-d73750d96142421cb1098c7056dd7f01")
CRAWLBASE_KEY_1 = os.getenv("CRAWLBASE_KEY_1", "x41P6KNU8J86yF9JV1nqSw")
CRAWLBASE_KEY_2 = os.getenv("CRAWLBASE_KEY_2", "FOg3R0v_aLxzHkYIdjPgVg")
DETECTLANGUAGE_KEY = os.getenv("DETECTLANGUAGE_KEY", "ebdc8ccc2ee75eda3ab122b08ffb1e8d")
GUARDIAN_KEY = os.getenv("GUARDIAN_KEY", "07c622c1-af05-4c24-9f37-37d219be76a0")
IP2LOCATION_KEY = os.getenv("IP2LOCATION_KEY", "11103C239EA8EA6DF2473BB445EC32F1")
SERPER_KEY = os.getenv("SERPER_KEY", "047b30db1df999aaa9c293f2048037d40c651439")
SHODAN_KEY = os.getenv("SHODAN_KEY", "umdSaWOfVq9Wt2F4wWdXiKh1zjLailzn")
TAVILY_KEY_1 = os.getenv("TAVILY_KEY_1", "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")
TAVILY_KEY_2 = os.getenv("TAVILY_KEY_2", "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")
TAVILY_KEY_3 = os.getenv("TAVILY_KEY_3", "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")
TAVILY_KEY_4 = os.getenv("TAVILY_KEY_4", "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")
WEATHERAPI_KEY = os.getenv("WEATHERAPI_KEY", "332bcdba457d4db4836175513250407")
WOLFRAM_APP_ID_1 = os.getenv("WOLFRAM_APP_ID_1", "96LX77-G8PGKJ3T7V")
WOLFRAM_APP_ID_2 = os.getenv("WOLFRAM_APP_ID_2", "96LX77-PYHRRET363")
WOLFRAM_APP_ID_3 = os.getenv("WOLFRAM_APP_ID_3", "96LX77-P9HPAYWRGL")
GREYNOISE_KEY = os.getenv("GREYNOISE_KEY", "5zNe9E6c2UNDhU09iVXbMaB04UpHAw5hNm5rHCK24fCLvI2cP33NNOpL7nhkDETG")
LOGINRADIUS_KEY = os.getenv("LOGINRADIUS_KEY", "073b2fbedf82409da2ca6f37b97e8c6a")
JSONBIN_KEY = os.getenv("JSONBIN_KEY", "$2a$10$npWSB7v1YcoqLkyPpz0PZOV5ES5vBs6JtTWVyVDXK3j3FDYYS5BPO")
HUGGINGFACE_KEY_1 = os.getenv("HUGGINGFACE_KEY_1", "hf_KzifJEYPZBXSSNcapgb3ISkPJLioDozyPC")
HUGGINGFACE_KEY_2 = os.getenv("HUGGINGFACE_KEY_2", "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")
HUGGINGFACE_KEY_3 = os.getenv("HUGGINGFACE_KEY_3", "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")
HUGGINGFACE_NEW_KEY = os.getenv("HUGGINGFACE_NEW_KEY", "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")
TWILIO_SID = os.getenv("TWILIO_SID", "SK84cc4d335650f9da168cd779f26e00e5")
TWILIO_SECRET = os.getenv("TWILIO_SECRET", "spvz5uwPE8ANYOI5Te4Mehm7YwKOZ4Lg")
ABSTRACTAPI_EMAIL_KEY_1 = os.getenv("ABSTRACTAPI_EMAIL_KEY_1", "2ffd537411ad407e9c9a7eacb7a97311")
ABSTRACTAPI_EMAIL_KEY_2 = os.getenv("ABSTRACTAPI_EMAIL_KEY_2", "5b00ade4e60e4a388bd3e749f4f66e28")
ABSTRACTAPI_EMAIL_KEY_3 = os.getenv("ABSTRACTAPI_EMAIL_KEY_3", "f4106df7b93e4db6855cb7949edc4a20")
ABSTRACTAPI_GENERIC_KEY = os.getenv("ABSTRACTAPI_GENERIC_KEY", "020a4dcd3e854ac0b19043491d79df92")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyABnzGG2YoTNY0uep-akgX1rfuvAsp049Q") # Clé pour GeminiApiClient
GOOGLE_API_KEYS = [
    os.getenv("GOOGLE_API_KEY_1", "AIzaSyAk6Ph25xuIY3b5o-JgdL652MvK4usp8Ms"),
    os.getenv("GOOGLE_API_KEY_2", "AIzaSyDuccmfiPSk4042NeJCYIjA8EOXPo1YKXU"),
    os.getenv("GOOGLE_API_KEY_3", "AIzaSyAQq6o9voefaDxkAEORf7W-IB3QbotIkwY"),
    os.getenv("GOOGLE_API_KEY_4", "AIzaSyDYaYrQQ7cwYFm8TBpyGM3dJweOGOYl7qw"),
]
GOOGLE_CX_LIST = [
    "3368510e864b74936",
    "e745c9ca0ffb94659"
]
PULSEDIVE_KEY = os.getenv("PULSEDIVE_KEY", "201bb09342f35d365889d7d0ca0fdf8580ebee0f1e7644ce70c99a46c1d47171")
RANDOMMER_KEY = os.getenv("RANDOMMER_KEY", "29d907df567b4226bf64b924f9e26c00")
STORMGLASS_KEY = os.getenv("STORMGLASS_KEY", "7ad5b888-5900-11f0-80b9-0242ac130006-7ad5b996-5900-11f0-80b9-0242ac130006")
TOMORROW_KEY = os.getenv("TOMORROW_KEY", "bNh6KpmddRGY0dzwvmQugVtG4Uf5Y2w1")
CLOUDMERSIVE_KEY = os.getenv("CLOUDMERSIVE_KEY", "4d407015-ce22-45d7-a2e1-b88ab6380084")
OPENWEATHER_API_KEY = os.getenv("OPENWEATHER_API_KEY", "c80075b7332716a418e47033463085ef")
MOCKAROO_KEY = os.getenv("MOCKAROO_KEY", "282b32d0")
OPENPAGERANK_KEY = os.getenv("OPENPAGERANK_KEY", "w848ws8s0848g4koosgooc0sg4ggogcggw4o4cko")
RAPIDAPI_KEY = os.getenv("RAPIDAPI_KEY", "d4d1f58d8emsh58d888c711b7400p1bcebejsn2cc04dce6efe")
OCR_API_KEY = os.getenv("OCR_API_KEY", "K82679097388957") # Clé pour OCRApiClient (une seule clé pour la classe dédiée)
OCR_API_KEYS = [ # Clés OCR pour les endpoints multiples si utilisés par APIClient générique
    os.getenv("OCR_API_KEY_1", "K82679097388957"),
    os.getenv("OCR_API_KEY_2", "K81079143888957"),
    os.getenv("OCR_API_KEY_3", "K84281517488957")
]

# ==== Configuration unifiée des APIs et Endpoints ====
# Cette configuration est utilisée par EndpointHealthManager et APIClient
API_CONFIG = {
    "APIFLASH": [
        {"key": APIFLASH_KEY, "endpoint_name": "URL to Image", "url": "https://api.apiflash.com/v1/urltoimage", "method": "GET", "key_field": "access_key", "key_location": "param", "health_check_params": {"url": "https://example.com"}, "timeout": 10}
    ],
    "DEEPSEEK": [
        {"key": DEEPSEEK_KEY_1, "endpoint_name": "Models List (Key 1)", "url": "https://api.deepseek.com/v1/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 5},
        {"key": DEEPSEEK_KEY_2, "endpoint_name": "Models List (Key 2)", "url": "https://api.deepseek.com/v1/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 5},
        {"key": DEEPSEEK_KEY_1, "endpoint_name": "Chat Completions", "url": "https://api.deepseek.com/v1/chat/completions", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"model": "deepseek-chat", "stream": False}, "health_check_json": {"model": "deepseek-chat", "messages": [{"role": "user", "content": "hello"}]}, "timeout": 30}
    ],
    "CRAWLBASE": [
        {"key": CRAWLBASE_KEY_1, "endpoint_name": "HTML Scraping", "url": "https://api.crawlbase.com", "method": "GET", "key_field": "token", "key_location": "param", "health_check_params": {"url": "https://example.com"}, "timeout": 15},
        {"key": CRAWLBASE_KEY_2, "endpoint_name": "JS Scraping (JavaScript Token)", "url": "https://api.crawlbase.com", "method": "GET", "key_field": "token", "key_location": "param", "fixed_params": {"javascript": "true"}, "health_check_params": {"url": "https://example.com", "javascript": "true"}, "timeout": 20}
    ],
    "DETECTLANGUAGE": [
        {"key": DETECTLANGUAGE_KEY, "endpoint_name": "Language Detection", "url": "https://ws.detectlanguage.com/0.2/detect", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "health_check_json": {"q": "hello"}, "timeout": 5}
    ],
    "GUARDIAN": [
        {"key": GUARDIAN_KEY, "endpoint_name": "News Search", "url": "https://content.guardianapis.com/search", "method": "GET", "key_field": "api-key", "key_location": "param", "fixed_params": {"show-fields": "headline,trailText"}, "health_check_params": {"q": "test"}, "timeout": 10},
        {"key": GUARDIAN_KEY, "endpoint_name": "Sections", "url": "https://content.guardianapis.com/sections", "method": "GET", "key_field": "api-key", "key_location": "param", "health_check_params": {"q": "news"}, "timeout": 5}
    ],
    "IP2LOCATION": [
        {"key": IP2LOCATION_KEY, "endpoint_name": "IP Geolocation", "url": "https://api.ip2location.io/", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"ip": "8.8.8.8"}, "timeout": 5}
    ],
    "SERPER": [
        {"key": SERPER_KEY, "endpoint_name": "Search", "url": "https://google.serper.dev/search", "method": "POST", "key_field": "X-API-KEY", "key_location": "header", "health_check_json": {"q": "test"}, "timeout": 10},
        {"key": SERPER_KEY, "endpoint_name": "Images Search", "url": "https://google.serper.dev/images", "method": "POST", "key_field": "X-API-KEY", "key_location": "header", "health_check_json": {"q": "test"}, "timeout": 10}
    ],
    "SHODAN": [
        {"key": SHODAN_KEY, "endpoint_name": "Host Info", "url": "https://api.shodan.io/shodan/host/8.8.8.8", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"ip": "8.8.8.8"}, "timeout": 10},
        {"key": SHODAN_KEY, "endpoint_name": "API Info", "url": "https://api.shodan.io/api-info", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 5}
    ],
    "TAVILY": [
        {"key": TAVILY_KEY_1, "endpoint_name": "Search (Key 1)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15},
        {"key": TAVILY_KEY_2, "endpoint_name": "Search (Key 2)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15},
        {"key": TAVILY_KEY_3, "endpoint_name": "Search (Key 3)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15},
        {"key": TAVILY_KEY_4, "endpoint_name": "Search (Key 4)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15}
    ],
    "WEATHERAPI": [
        {"key": WEATHERAPI_KEY, "endpoint_name": "Current Weather", "url": "http://api.weatherapi.com/v1/current.json", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"q": "London"}, "timeout": 5},
        {"key": WEATHERAPI_KEY, "endpoint_name": "Forecast", "url": "http://api.weatherapi.com/v1/forecast.json", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"days": 1}, "health_check_params": {"q": "London", "days": 1}, "timeout": 5}
    ],
    "WOLFRAMALPHA": [
        {"key": WOLFRAM_APP_ID_1, "endpoint_name": "Query (AppID 1)", "url": "http://api.wolframalpha.com/v2/query", "method": "GET", "key_field": "appid", "key_location": "param", "fixed_params": {"format": "plaintext", "output": "json"}, "health_check_params": {"input": "2+2"}, "timeout": 10},
        {"key": WOLFRAM_APP_ID_2, "endpoint_name": "Query (AppID 2)", "url": "http://api.wolframalpha.com/v2/query", "method": "GET", "key_field": "appid", "key_location": "param", "fixed_params": {"format": "plaintext", "output": "json"}, "health_check_params": {"input": "2+2"}, "timeout": 10},
        {"key": WOLFRAM_APP_ID_3, "endpoint_name": "Query (AppID 3)", "url": "http://api.wolframalpha.com/v2/query", "method": "GET", "key_field": "appid", "key_location": "param", "fixed_params": {"format": "plaintext", "output": "json"}, "health_check_params": {"input": "2+2"}, "timeout": 10}
    ],
    "CLOUDMERSIVE": [
        {"key": CLOUDMERSIVE_KEY, "endpoint_name": "Domain Check", "url": "https://api.cloudmersive.com/validate/domain/check", "method": "POST", "key_field": "Apikey", "key_location": "header", "health_check_json": {"domain": "example.com"}, "timeout": 10}
    ],
    "GREYNOISE": [
        {"key": GREYNOISE_KEY, "endpoint_name": "IP Analysis", "url": "https://api.greynoise.io/v3/community/", "method": "GET", "key_field": "key", "key_location": "header", "health_check_url_suffix": "1.1.1.1", "timeout": 10}
    ],
    "PULSEDIVE": [
        {"key": PULSEDIVE_KEY, "endpoint_name": "API Info", "url": "https://pulsedive.com/api/info.php", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"key": PULSEDIVE_KEY}, "timeout": 5},
        {"key": PULSEDIVE_KEY, "endpoint_name": "Analyze IP", "url": "https://pulsedive.com/api/v1/analyze", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"indicator": "8.8.8.8", "type": "ip"}, "timeout": 10},
        {"key": PULSEDIVE_KEY, "endpoint_name": "Explore", "url": "https://pulsedive.com/api/v1/explore", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"query": "type='ip'"}, "timeout": 10}
    ],
    "STORMGLASS": [
        {"key": STORMGLASS_KEY, "endpoint_name": "Weather Point", "url": "https://api.stormglass.io/v2/weather/point", "method": "GET", "key_field": "Authorization", "key_location": "header", "health_check_params": {"lat": 0, "lng": 0, "params": "airTemperature", "start": 0, "end": 0}, "timeout": 10}
    ],
    "LOGINRADIUS": [
        {"key": LOGINRADIUS_KEY, "endpoint_name": "Ping", "url": "https://api.loginradius.com/identity/v2/auth/ping", "method": "GET", "timeout": 5}
    ],
    "JSONBIN": [
        {"key": JSONBIN_KEY, "endpoint_name": "Bin Access", "url": "https://api.jsonbin.io/v3/b", "method": "GET", "key_field": "X-Master-Key", "key_location": "header", "health_check_url_suffix": "60c7b0e0f8c2a3b4c5d6e7f0", "timeout": 10},
        {"key": JSONBIN_KEY, "endpoint_name": "Bin Create", "url": "https://api.jsonbin.io/v3/b", "method": "POST", "key_field": "X-Master-Key", "key_location": "header", "health_check_json": {"record": {"test": "health"}}, "timeout": 10}
    ],
    "HUGGINGFACE": [
        {"key": HUGGINGFACE_KEY_1, "endpoint_name": "Models List (Key 1)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_KEY_1, "endpoint_name": "BERT Inference", "url": "https://api-inference.huggingface.co/models/bert-base-uncased", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "health_check_json": {"inputs": "test"}, "timeout": 30},
        {"key": HUGGINGFACE_KEY_2, "endpoint_name": "Models List (Key 2)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_KEY_3, "endpoint_name": "Models List (Key 3)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_NEW_KEY, "endpoint_name": "Models List (New Key)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_NEW_KEY, "endpoint_name": "BERT Inference (New Key)", "url": "https://api-inference.huggingface.co/models/bert-base-uncased", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "health_check_json": {"inputs": "test"}, "timeout": 30}
    ],
    "TWILIO": [
        {"key": (TWILIO_SID, TWILIO_SECRET), "endpoint_name": "Accounts", "url": "https://api.twilio.com/2010-04-01/Accounts", "method": "GET", "key_location": "auth_basic", "timeout": 10},
        {"key": (TWILIO_SID, TWILIO_SECRET), "endpoint_name": "Account Balance", "url": f"https://api.twilio.com/2010-04-01/Accounts/{TWILIO_SID}/Balance.json", "method": "GET", "key_location": "auth_basic", "timeout": 10}
    ],
    "ABSTRACTAPI": [
        {"key": ABSTRACTAPI_EMAIL_KEY_1, "endpoint_name": "Email Validation (Key 1)", "url": "https://emailvalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"email": "test@example.com"}, "timeout": 10},
        {"key": ABSTRACTAPI_EMAIL_KEY_2, "endpoint_name": "Email Validation (Key 2)", "url": "https://emailvalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"email": "test@example.com"}, "timeout": 10},
        {"key": ABSTRACTAPI_EMAIL_KEY_3, "endpoint_name": "Email Validation (Key 3)", "url": "https://emailvalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"email": "test@example.com"}, "timeout": 10},
        {"key": ABSTRACTAPI_GENERIC_KEY, "endpoint_name": "Exchange Rates", "url": "https://exchange-rates.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"base": "USD"}, "timeout": 10},
        {"key": ABSTRACTAPI_GENERIC_KEY, "endpoint_name": "Holidays", "url": "https://holidays.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "fixed_params": {"country": "US", "year": datetime.now().year}, "health_check_params": {"country": "US", "year": datetime.now().year}, "timeout": 10},
        {"key": ABSTRACTAPI_GENERIC_KEY, "endpoint_name": "Phone Validation", "url": "https://phonevalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"phone": "1234567890"}, "timeout": 10}
    ],
    "GEMINI_API": [ # Note: This is for the generic APIClient, GeminiApiClient class uses GEMINI_API_KEY directly
        {"key": GEMINI_API_KEY, "endpoint_name": "Generic Models Endpoint", "url": "https://generativelanguage.googleapis.com/v1beta/models", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": GEMINI_API_KEY, "endpoint_name": "Embed Content", "url": "https://generativelanguage.googleapis.com/v1beta/models/embedding-001:embedContent", "method": "POST", "key_field": "key", "key_location": "param", "health_check_json": {"content": {"parts": [{"text": "test"}]}}, "timeout": 30},
        {"key": GEMINI_API_KEY, "endpoint_name": "Generate Content", "url": "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent", "method": "POST", "key_field": "key", "key_location": "param", "health_check_json": {"contents": [{"parts": [{"text": "hello"}]}]}, "timeout": 60}
    ],
    "GOOGLE_CUSTOM_SEARCH": [
        {"key": GOOGLE_API_KEYS[i], "endpoint_name": f"Search (Key {i+1}, CX {j+1})", "url": "https://www.googleapis.com/customsearch/v1", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"cx": GOOGLE_CX_LIST[j]}, "health_check_params": {"q": "test"}, "timeout": 10}
        for i in range(len(GOOGLE_API_KEYS)) for j in range(len(GOOGLE_CX_LIST))
    ],
    "RANDOMMER": [
        {"key": RANDOMMER_KEY, "endpoint_name": "Generate Phone", "url": "https://randommer.io/api/Phone/Generate", "method": "GET", "key_field": "X-Api-Key", "key_location": "header", "fixed_params": {"CountryCode": "US", "Quantity": 1}, "health_check_params": {"CountryCode": "US", "Quantity": 1}, "timeout": 10}
    ],
    "TOMORROW.IO": [
        {"key": TOMORROW_KEY, "endpoint_name": "Timelines", "url": "https://api.tomorrow.io/v4/timelines", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"location": "London", "fields": ["temperature"], "units": "metric", "timesteps": ["1h"]}, "timeout": 15}
    ],
    "OPENWEATHERMAP": [
        {"key": OPENWEATHER_API_KEY, "endpoint_name": "Current Weather", "url": "https://api.openweathermap.org/data/2.5/weather", "method": "GET", "key_field": "appid", "key_location": "param", "health_check_params": {"q": "London"}, "timeout": 5}
    ],
    "MOCKAROO": [
        {"key": MOCKAROO_KEY, "endpoint_name": "Data Generation", "url": "https://api.mockaroo.com/api/generate.json", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "health_check_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Types", "url": "https://api.mockaroo.com/api/types", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Schemas", "url": "https://api.mockaroo.com/api/schemas", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Account", "url": "https://api.mockaroo.com/api/account", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Generate CSV", "url": "https://api.mockaroo.com/api/generate.csv", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "health_check_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Status", "url": "https://api.mockaroo.com/api/status", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10}
    ],
    "OPENPAGERANK": [
        {"key": OPENPAGERANK_KEY, "endpoint_name": "Domain Rank", "url": "https://openpagerank.com/api/v1.0/getPageRank", "method": "GET", "key_field": "API-OPR", "key_location": "header", "fixed_params": {"domains[]": "google.com"}, "timeout": 10}
    ],
    "RAPIDAPI": [
        {"key": RAPIDAPI_KEY, "endpoint_name": "Programming Joke", "url": "https://jokeapi-v2.p.rapidapi.com/joke/Programming", "method": "GET", "key_field": "X-RapidAPI-Key", "key_location": "header", "fixed_headers": {"X-RapidAPI-Host": "jokeapi-v2.p.rapidapi.com"}, "timeout": 10},
        {"key": RAPIDAPI_KEY, "endpoint_name": "Currency List Quotes", "url": "https://currency-exchange.p.rapidapi.com/listquotes", "method": "GET", "key_field": "X-RapidAPI-Key", "key_location": "header", "fixed_headers": {"X-RapidAPI-Host": "currency-exchange.p.rapidapi.com"}, "timeout": 10},
        {"key": RAPIDAPI_KEY, "endpoint_name": "Random Fact", "url": "https://random-facts2.p.rapidapi.com/getfact", "method": "GET", "key_field": "X-RapidAPI-Key", "key_location": "header", "fixed_headers": {"X-RapidAPI-Host": "random-facts2.p.rapidapi.com"}, "timeout": 10}
    ],
    "OCR_API": [ # Note: This is for the generic APIClient, OCRApiClient class uses OCR_API_KEY directly
        {"key": OCR_API_KEYS[0], "endpoint_name": "OCR Space (Key 1)", "url": "https://api.ocr.space/parse/image", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"base64Image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="}, "timeout": 30},
        {"key": OCR_API_KEYS[1], "endpoint_name": "OCR Space (Key 2)", "url": "https://api.ocr.space/parse/image", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"base64Image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="}, "timeout": 30},
        {"key": OCR_API_KEYS[2], "endpoint_name": "OCR Space (Key 3)", "url": "https://api.ocr.space/parse/image", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"base64Image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="}, "timeout": 30},
    ]
}

# ==== Quotas API (Définitions des limites pour le QuotaManager) ====
# Ces valeurs sont utilisées pour le suivi et la gestion des quotas d'utilisation.
# Mettre None pour indiquer une limite illimitée.
API_QUOTAS = {
    "GEMINI": { # Renommé pour correspondre à la clé dans API_CONFIG
        "monthly": 1000000, # Exemple: 1 million de tokens par mois
        "daily": 50000,    # Exemple: 50 000 tokens par jour
        "hourly": 5000,    # Exemple: 5 000 tokens par heure
        "rate_limit_per_sec": 5 # Exemple: 5 requêtes par seconde
    },
    "OCR_API": { # Nom interne utilisé par OCRApiClient
        "monthly": 25000,  # Exemple: 25 000 requêtes par mois (free tier)
        "daily": None,
        "hourly": None,
        "rate_limit_per_sec": 1 # Exemple: 1 requête par seconde
    },
    # Ajouter les quotas pour toutes les APIs listées dans API_CONFIG si elles ont des limites
    # Utiliser les valeurs du premier snippet si non spécifiées ici
    "APIFLASH": {"monthly": 100, "daily": 3, "hourly": 3},
    "DEEPSEEK": {"monthly": None, "hourly": 50},
    "CRAWLBASE": {"monthly": 1000, "daily": 33, "hourly": 1},
    "DETECTLANGUAGE": {"daily": 1000, "hourly": 41},
    "GUARDIAN": {"daily": 5000, "rate_limit_per_sec": 12},
    "IP2LOCATION": {"monthly": 50, "daily": 2, "hourly": 2},
    "SERPER": {"monthly": 2500, "daily": 83, "hourly": 3},
    "SHODAN": {"monthly": 100, "daily": 3, "hourly": 3},
    "TAVILY": {"monthly": 1000, "daily": 33, "hourly": 1},
    "WEATHERAPI": {"monthly": None},
    "WOLFRAMALPHA": {"monthly": None, "hourly": 67},
    "CLOUDMERSIVE": {"monthly": 25, "daily": 1, "hourly": 1},
    "GREYNOISE": {"monthly": 100, "daily": 3, "hourly": 3},
    "PULSEDIVE": {"monthly": 50, "daily": 2, "hourly": 2},
    "STORMGLASS": {"monthly": None},
    "LOGINRADIUS": {"monthly": 25000, "daily": 833, "hourly": 34},
    "JSONBIN": {"monthly": 10000, "daily": 333, "hourly": 13},
    "HUGGINGFACE": {"hourly": 100},
    "TWILIO": {"monthly": 15},
    "ABSTRACTAPI": {"monthly": 250, "rate_limit_per_sec": 1, "daily": 8, "hourly": 1},
    "MOCKAROO": {"monthly": 200, "daily": 7, "hourly": 1},
    "OPENPAGERANK": {"monthly": 1000, "daily": 33, "hourly": 1},
    "RAPIDAPI": {"monthly": None, "hourly": 30},
    "GOOGLE_CUSTOM_SEARCH": {"daily": 100, "hourly": 4},
    "RANDOMMER": {"monthly": 1000, "daily": 100, "hourly": 4},
    "TOMORROW.IO": {"monthly": None},
    "OPENWEATHERMAP": {"monthly": 1000000, "daily": 100, "hourly": 4},
}


# Modèle Gemini et ses paramètres
GEMINI_MODEL_NAME = "gemini-1.5-flash-latest" # Ou "gemini-pro"
GEMINI_TEMPERATURE = 0.7
GEMINI_TOP_P = 0.95
GEMINI_TOP_K = 40
GEMINI_MAX_OUTPUT_TOKENS = 2048
GEMINI_SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]


# ==== Bot Behavior Configuration ====
API_COOLDOWN_DURATION_SECONDS = 300 # Durée du cooldown en secondes (5 minutes)
API_ROTATION_INTERVAL_MINUTES = 10 # Intervalle de rotation des APIs en minutes

# Quota Burning Configuration
# Ratio de quota restant en dessous duquel le mode "brûlage" s'active (ex: 0.2 signifie 20% ou moins restant)
BURN_QUOTA_THRESHOLD_RATIO = 0.2
# Fenêtre de temps avant la réinitialisation du quota où le "brûlage" peut s'activer (en heures)
BURN_QUOTA_BEFORE_RESET_HOURS = 6

# Prompts pour l'auto-génération de contenu technique afin de "brûler" le quota.
# Ces prompts sont choisis aléatoirement pour les APIs en mode "burn".
AUTO_BURN_PROMPTS = {
    "GEMINI": [ # Renommé pour correspondre à la clé dans API_QUOTAS
        "Génère un script Python qui utilise une API REST pour récupérer des données et les stocker dans une base de données NoSQL.",
        "Explique en détail les principes de l'architecture microservices et comment ils se comparent aux architectures monolithiques.",
        "Décris les étapes pour déployer une application web Flask sur un serveur cloud (AWS EC2 ou Google Cloud Run).",
        "Écris un tutoriel sur l'utilisation de Docker Compose pour orchestrer plusieurs conteneurs (par exemple, une application web et une base de données).",
        "Analyse les avantages et les inconvénients des bases de données relationnelles vs non-relationnelles pour un projet de grande envergure.",
        "Propose un plan de test complet pour une application web critique, incluant tests unitaires, d'intégration, de bout en bout et de performance.",
        "Génère un exemple de code JavaScript pour une application React qui gère l'état avec Redux ou Context API.",
        "Explique le concept de CI/CD (intégration et livraison continues) et son importance dans le développement logiciel moderne.",
        "Décris les meilleures pratiques de sécurité pour une API RESTful, incluant l'authentification, l'autorisation et la protection contre les attaques courantes.",
        "Écris un algorithme de tri efficace (par exemple, Quicksort ou Mergesort) et explique sa complexité temporelle et spatiale."
    ],
    "OCR_API": [ # Renommé pour correspondre à la clé dans API_QUOTAS
        "Décris les défis techniques de l'OCR sur des documents manuscrits et les approches modernes pour les surmonter.",
        "Explique comment l'OCR peut être utilisée dans le domaine de la gestion documentaire ou de l'automatisation des processus métier.",
        "Quelles sont les métriques d'évaluation courantes pour les performances d'un système OCR ?",
        "Comment la pré-traitement d'image (bruit, binarisation, redressement) affecte-t-il la précision de l'OCR ?",
        "Compare les différentes technologies OCR disponibles sur le marché (cloud vs on-premise, open-source vs propriétaires)."
    ]
}

# ==== Paramètres de Sécurité et Filtrage ====
FORBIDDEN_WORDS = ["fuck", "shit", "bitch", "asshole", "pute", "enculé", "haine", "stupide", "détruire", "conflit", "malveillance", "idiot", "nul", "débile"]

# ==== IA PROMPTS (Exemples, à affiner selon tes besoins spécifiques pour chaque IA) ====
GENERAL_IA_PROMPT = """
Tu es une IA de l'année 2025, experte en information, programmation et résolution de problèmes.
Ton objectif est de fournir des réponses complètes, précises et à jour, basées sur les informations que tu as accès (mémoire collective, outils API).
Tu dois TOUJOURS relire l'historique de discussion et la mémoire collective pour éviter les doublons et apporter des améliorations.
Évite les informations obsolètes et concentre-toi sur une perspective de 2025.
Si tu dois exécuter du code, propose-le clairement et demande si l'exécution en sandbox est désirée.
N'hésite pas à croiser les informations de plusieurs sources.
"""

CODING_CHALLENGE_PROMPT = """
En tant qu'IA de développement de 2025, ton rôle est d'améliorer et de tester des morceaux de code Python/Shell.
Tu as accès à une sandbox sécurisée pour exécuter le code.
Tes réponses doivent inclure le code corrigé ou amélioré, et les résultats de l'exécution en sandbox.
Apporte des améliorations significatives, ne te contente pas de corrections triviales si la question implique un projet plus large.
Pense à l'efficacité du code et à l'optimisation des ressources.
Chaque version doit être une amélioration nette de la précédente, inédite.
Commence par un commentaire indiquant ce qui a été amélioré.
Le code doit être direct, lisible, et prêt à être utilisé.
"""

# ==== Tool Reformulation Configuration ====
TOOL_RETRY_MAX_ATTEMPTS = 3


# utils.py
import os
import json
import gzip
import shutil
import hashlib
import difflib
import re
import logging
import io
import contextlib
import fcntl
import traceback
import asyncio
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Union, List, Dict, Any, Optional

# Configure logging pour tout le script
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Global lock for file operations to ensure atomic writes and prevent race conditions
_file_lock: Optional[asyncio.Lock] = None

def set_file_lock(lock_instance: asyncio.Lock):
    """
    Permet d'injecter l'instance d'asyncio.Lock après l'initialisation de l'event loop.
    Ceci est crucial pour la gestion des accès concurrents aux fichiers.
    """
    global _file_lock
    _file_lock = lock_instance

def _acquire_file_lock_sync(f):
    """
    Acquires an exclusive lock on a file using fcntl (Unix-like systems).
    This prevents other processes from writing to the file simultaneously.
    """
    try:
        if os.name == 'posix' and fcntl:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
    except Exception as e:
        logging.warning(f"Could not acquire file lock: {e}")

def _release_file_lock_sync(f):
    """
    Releases an exclusive lock on a file using fcntl (Unix-like systems).
    """
    try:
        if os.name == 'posix' and fcntl:
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
    except Exception as e:
        logging.warning(f"Could not release file lock: {e}")

def get_user_dir(uid: Union[int, str]) -> Path:
    """
    Retourne le répertoire de sauvegarde spécifique à un utilisateur, le créant si nécessaire.
    Chaque utilisateur (ou groupe privé) a son propre répertoire pour stocker les données.
    """
    p = BASE_DIR / str(uid)
    p.mkdir(parents=True, exist_ok=True)
    return p

def rotate_log_if_needed(path: Path):
    """
    Fait pivoter le fichier log (ou tout fichier de données) si sa taille dépasse MAX_FILE_SIZE.
    Un nouveau fichier est créé avec un horodatage pour l'archivage, et le fichier original est réinitialisé.
    """
    if path.exists() and path.stat().st_size > MAX_FILE_SIZE:
        timestamp = int(datetime.now().timestamp())
        # Renomme le fichier existant pour l'archiver
        new_path = path.with_suffix(f".old_{timestamp}{path.suffix}.gz")
        try:
            # Compresse et déplace l'ancien fichier
            with open(path, "rb") as f_in, gzip.open(new_path, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out)
            path.unlink() # Supprime l'original
            logging.info(f"Log rotated and compressed: {path} -> {new_path}")
        except Exception as e:
            log_message(f"[Rotation log] Erreur lors de la compression/rotation de {path}: {e}\n{traceback.format_exc()}", level="error")
        # Le fichier sera recréé vide lors de la prochaine sauvegarde

def compress_if_large(path: Path):
    """
    Compresse le fichier s'il dépasse 1MB après une écriture, puis le renomme pour garder le nom original.
    Ceci est une mesure de gestion de l'espace disque pour les fichiers de données.
    """
    try:
        # Vérifie si le fichier existe et si sa taille est supérieure à 1MB
        if path.exists() and path.stat().st_size > 1_000_000:
            gz_path = path.with_suffix(path.suffix + ".gz") # Chemin temporaire pour le fichier compressé
            with open(path, "rb") as f_in, gzip.open(gz_path, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out) # Copie et compresse
            path.unlink() # Supprime le fichier original non compressé
            # Renomme le fichier .gz pour qu'il ait le nom original, simulant une compression "in-place"
            gz_path.rename(path)
            logging.info(f"File compressed: {path}")
    except Exception as e:
        log_message(f"[Compression auto] Erreur : {e}\n{traceback.format_exc()}", level="error")

async def load_json(filepath: Path, default_value: Union[Dict, List] = None) -> Union[Dict, List]:
    """
    Charge un fichier JSON de manière asynchrone.
    Retourne `default_value` si le fichier n'existe pas, est vide ou est corrompu.
    Utilise un verrou global pour la sécurité des accès concurrents.
    """
    if default_value is None:
        default_value = {} # Default to empty dict if not specified

    if _file_lock:
        async with _file_lock:
            return _load_json_sync(filepath, default_value)
    else:
        # Fallback for synchronous loading if lock is not set (e.g., during early initialization)
        return _load_json_sync(filepath, default_value)

def _load_json_sync(filepath: Path, default_value: Union[Dict, List]) -> Union[Dict, List]:
    """
    Charge un fichier JSON de manière synchrone avec verrouillage de fichier.
    """
    if not filepath.exists():
        logging.info(f"Fichier non trouvé: {filepath}. Création d'un fichier vide.")
        _save_json_sync(filepath, default_value) # Crée un fichier vide avec la valeur par défaut
        return default_value

    # Ouvre le fichier en mode lecture/écriture pour pouvoir le vider en cas de corruption
    with open(filepath, 'r+', encoding='utf-8') as f:
        _acquire_file_lock_sync(f) # Acquiert le verrou avant de lire
        try:
            f.seek(0) # Se positionne au début du fichier
            content = f.read()
            if not content:
                logging.warning(f"Fichier vide: {filepath}. Retourne la valeur par défaut.")
                return default_value
            return json.loads(content)
        except json.JSONDecodeError as e:
            logging.error(f"Erreur de décodage JSON dans {filepath}: {e}. Le fichier sera réinitialisé.")
            # Si le fichier est corrompu, le réinitialise avec la valeur par défaut
            f.seek(0)
            f.truncate()
            json.dump(default_value, f, indent=4, ensure_ascii=False)
            return default_value
        except Exception as e:
            logging.error(f"Erreur inattendue lors du chargement de {filepath}: {e}. Retourne la valeur par défaut.")
            return default_value
        finally:
            _release_file_lock_sync(f) # Relâche le verrou

async def save_json(filepath: Path, data: Union[Dict, List]):
    """
    Sauvegarde les données dans un fichier JSON de manière asynchrone et atomique,
    avec rotation et compression si nécessaire.
    Utilise un verrou global pour la sécurité des accès concurrents.
    """
    if _file_lock:
        async with _file_lock:
            _save_json_sync(filepath, data)
    else:
        # Fallback for synchronous saving if lock is not set
        _save_json_sync(filepath, data)

def _save_json_sync(filepath: Path, data: Union[Dict, List]):
    """
    Sauvegarde les données dans un fichier JSON de manière synchrone et atomique,
    avec verrouillage de fichier.
    """
    rotate_log_if_needed(filepath) # Effectue la rotation avant la sauvegarde
    temp_filepath = filepath.with_suffix(filepath.suffix + ".tmp") # Utilise un fichier temporaire pour l'atomicité
    try:
        with temp_filepath.open('w', encoding='utf-8') as f:
            _acquire_file_lock_sync(f) # Acquiert le verrou avant d'écrire
            try:
                json.dump(data, f, indent=4, ensure_ascii=False)
            finally:
                _release_file_lock_sync(f) # Relâche le verrou
        os.replace(temp_filepath, filepath) # Remplace l'ancien fichier par le nouveau de manière atomique
        compress_if_large(filepath) # Compresse le fichier après la sauvegarde si nécessaire
    except Exception as e:
        logging.error(f"Erreur lors de la sauvegarde atomique de {filepath}: {e}")
        if temp_filepath.exists():
            os.remove(temp_filepath) # Nettoie le fichier temporaire en cas d'erreur

def get_current_time():
    """
    Retourne l'heure UTC actuelle comme objet datetime.
    """
    return datetime.now(timezone.utc) # Utilise datetime.now(timezone.utc) pour être explicite sur UTC

def format_datetime(dt_obj: datetime) -> str:
    """
    Formate un objet datetime en chaîne de caractères lisible (UTC).
    """
    return dt_obj.strftime("%Y-%m-%d %H:%M:%S UTC")

def is_within_time_window(target_time: datetime, start_minutes_before: int, end_minutes_after: int) -> bool:
    """
    Vérifie si l'heure actuelle est dans une fenêtre de temps spécifiée autour d'une heure cible.
    """
    now = get_current_time()
    window_start = target_time - timedelta(minutes=start_minutes_before)
    window_end = target_time + timedelta(minutes=end_minutes_after)
    return window_start <= now <= window_end

def log_message(message: str, level: str = "info"):
    """
    Log un message avec un niveau spécifié.
    Les messages d'erreur critiques sont dirigés vers un fichier de log d'erreurs séparé.
    """
    if level == "error":
        error_logger = logging.getLogger("erreurs_api")
        if not error_logger.handlers: # Configurer le logger d'erreurs si pas déjà fait
            eh = logging.FileHandler(ERROR_LOG_PATH, encoding="utf-8")
            eh.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S"))
            error_logger.addHandler(eh)
            error_logger.setLevel(logging.ERROR)
        error_logger.error(message)
    else:
        # Utilise le logger par défaut pour les autres niveaux
        if level == "info":
            logging.info(message)
        elif level == "warning":
            logging.warning(message)
        elif level == "debug":
            logging.debug(message)
        else:
            logging.debug(message) # Fallback pour les niveaux non reconnus

def neutralize_urls(text: str) -> str:
    """
    Remplace les URLs dans le texte par un placeholder pour prévenir les problèmes de lien direct
    et la fuite d'informations sensibles dans les logs ou la mémoire.
    """
    # Remplace http(s):// par hxxp(s)://
    text = re.sub(r"https?://", lambda m: m.group(0).replace("t", "x", 1), text)
    # Remplace www. par wxx.
    text = re.sub(r"www\.", "wxx.", text)
    # Remplace .com, .net, .org par [.]com, [.]net, [.]org
    text = re.sub(r"\.com", "[.]com", text)
    text = re.sub(r"\.net", "[.]net", text)
    text = re.sub(r"\.org", "[.]org", text)
    return text

def clean_html_tags(text: str) -> str:
    """
    Supprime les balises HTML d'une chaîne de caractères en utilisant une expression régulière.
    """
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

def hash_text(t: str) -> str:
    """
    Calcule le hachage SHA256 d'une chaîne de caractères.
    """
    return hashlib.sha256(t.encode('utf-8')).hexdigest()

def extract_keywords(text: str) -> List[str]:
    """
    Extrait les mots-clés les plus fréquents d'un texte.
    Retourne une liste des 5 mots les plus fréquents (de 4 caractères ou plus).
    """
    # Trouve tous les mots de 4 caractères ou plus (incluant les accents)
    words = re.findall(r'\b[a-zA-Zéèêôàùçîïœ]{4,}\b', text.lower())
    freq = {}
    for w in words:
        freq[w] = freq.get(w, 0) + 1
    # Trie les mots par fréquence décroissante
    keywords = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [w for w,_ in keywords[:5]] # Retourne seulement les mots

def tag_conversation(text: str) -> str:
    """
    Génère un tag de conversation basé sur les mots-clés extraits du texte.
    """
    words = extract_keywords(text)
    return f"#tags : {', '.join(words)}"

def unique_preserve_order(seq: List[Any]) -> List[Any]:
    """
    Élimine les doublons d'une séquence tout en préservant l'ordre original des éléments.
    """
    seen = set()
    result = []
    for item in seq:
        if item not in seen:
            seen.add(item)
            result.append(item)
    return result

def similar(a: str, b: str) -> float:
    """
    Calcule la similarité entre deux chaînes de caractères en utilisant le ratio de SequenceMatcher.
    Retourne un ratio de 0 à 1, où 1 indique une identité parfaite.
    """
    return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio()

def is_code(text: str) -> bool:
    """
    Détecte si le texte ressemble à du code (Python ou autre) en cherchant des motifs courants.
    """
    return bool(re.search(r"^\s*(def |class |import |print\()", text, re.MULTILINE)) or text.strip().startswith("```")

def is_python_code_block(text: str) -> bool:
    """
    Détecte si le texte est un bloc de code Python formaté en Markdown.
    """
    return text.strip().startswith("```python") and text.strip().endswith("```")

# api_clients.py
import time
import httpx
import json
import base64
import asyncio
import re
from typing import Dict, Any, Optional, Union, List, Tuple

# Import des constantes et fonctions utilitaires depuis les modules locaux
from config import API_CONFIG, ENDPOINT_HEALTH_FILE, OCR_API_KEYS, GEMINI_API_KEY
from utils import load_json, save_json, get_current_time, format_datetime, log_message, neutralize_urls

class EndpointHealthManager:
    """
    Gère la santé des endpoints API et sélectionne le meilleur endpoint disponible
    en fonction de critères comme la latence, le taux de succès et le nombre d'erreurs.
    C'est un singleton pour s'assurer qu'il n'y a qu'une seule instance de gestionnaire de santé.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Implémente le patron de conception Singleton."""
        if cls._instance is None:
            cls._instance = super(cls, cls).__new__(cls)
            cls._instance._initialized = False # Indicateur pour l'initialisation asynchrone
        return cls._instance

    def __init__(self):
        """Initialise le gestionnaire. L'initialisation réelle se fait via `init_manager`."""
        if self._initialized:
            return
        self.health_status = {}
        # `_initialized` est géré par `init_manager` pour les opérations asynchrones

    async def init_manager(self):
        """
        Initialise le gestionnaire de santé de manière asynchrone.
        Charge l'état de santé persistant et s'assure que tous les endpoints sont suivis.
        """
        if not self._initialized:
            self.health_status = await load_json(ENDPOINT_HEALTH_FILE, {})
            self._initialize_health_status()
            self._initialized = True
            log_message("Gestionnaire de santé des endpoints initialisé.")

    def _initialize_health_status(self):
        """
        Initialise ou met à jour le statut de santé pour tous les endpoints configurés dans `API_CONFIG`.
        Ajoute les nouveaux endpoints et s'assure que toutes les clés nécessaires sont présentes.
        """
        updated = False
        for service_name, endpoints_config in API_CONFIG.items():
            if service_name not in self.health_status:
                self.health_status[service_name] = {}
                updated = True
            for endpoint_config in endpoints_config:
                # Crée une clé unique pour chaque endpoint en combinant son nom et sa clé API
                endpoint_key = f"{endpoint_config['endpoint_name']}-{str(endpoint_config['key'])}"
                if endpoint_key not in self.health_status[service_name]:
                    self.health_status[service_name][endpoint_key] = {
                        "latency": 0.0,
                        "success_rate": 1.0, # Commence avec un taux de succès parfait (sain)
                        "last_checked": None,
                        "error_count": 0,
                        "total_checks": 0,
                        "is_healthy": True # Présumé sain au début
                    }
                    updated = True
        if updated:
            # Sauvegarde l'état mis à jour de manière asynchrone en tâche de fond
            asyncio.create_task(save_json(ENDPOINT_HEALTH_FILE, self.health_status))
            log_message("Statut de santé des endpoints initialisé/mis à jour.")

    async def run_health_check_for_service(self, service_name: str):
        """
        Exécute des checks de santé pour tous les endpoints d'un service donné.
        Tente d'appeler l'endpoint avec des paramètres de santé prédéfinis.
        """
        endpoints_config = API_CONFIG.get(service_name)
        if not endpoints_config:
            log_message(f"Aucune configuration d'endpoint trouvée pour le service: {service_name}", level="warning")
            return

        log_message(f"Lancement du health check pour le service: {service_name}")
        for endpoint_config in endpoints_config:
            endpoint_key = f"{endpoint_config['endpoint_name']}-{str(endpoint_config['key'])}"
            start_time = time.monotonic()
            success = False
            try:
                request_method = endpoint_config.get("method", "GET")
                url = endpoint_config["url"]

                # Prépare les paramètres/données pour le health check
                params = endpoint_config.get("health_check_params", endpoint_config.get("fixed_params", {})).copy()
                json_data = endpoint_config.get("health_check_json", endpoint_config.get("fixed_json", {})).copy()
                headers = endpoint_config.get("fixed_headers", {}).copy()
                auth = None # Pour l'authentification de base (Basic Auth)

                check_timeout = endpoint_config.get("timeout", 5) # Timeout spécifique pour le health check

                # Ajoute un suffixe à l'URL si spécifié (ex: pour les APIs basées sur des chemins d'accès)
                if "health_check_url_suffix" in endpoint_config:
                    url += endpoint_config["health_check_url_suffix"]

                # Gère l'insertion de la clé API selon sa localisation (paramètre, en-tête, Basic Auth)
                key_field = endpoint_config.get("key_field")
                key_location = endpoint_config.get("key_location")
                key_prefix = endpoint_config.get("key_prefix", "") # Préfixe pour les clés dans les en-têtes (ex: "Bearer ")
                api_key = endpoint_config["key"]

                if key_field and key_location:
                    if key_location == "param":
                        params[key_field] = api_key
                    elif key_location == "header":
                        headers[key_field] = f"{key_prefix}{api_key}"
                    elif key_location == "auth_basic":
                        if isinstance(api_key, tuple) and len(api_key) == 2:
                            auth = httpx.BasicAuth(api_key[0], api_key[1])
                        else:
                            log_message(f"Clé API pour auth_basic non valide pour {service_name}:{endpoint_key}", level="error")
                            success = False
                            continue # Passe à l'endpoint suivant

                async with httpx.AsyncClient(timeout=check_timeout) as client:
                    response = await client.request(request_method, url, params=params, headers=headers, json=json_data, auth=auth)
                    response.raise_for_status() # Lève une exception pour les codes d'état HTTP 4xx/5xx
                    success = True
            except httpx.HTTPStatusError as e:
                log_level = "warning"
                # Les codes 4xx (sauf 429 - Too Many Requests) indiquent souvent une erreur client
                # (clé invalide, paramètre manquant) qui ne se résoudra pas avec un réessai
                # et n'indique pas forcément un problème de "santé" du service lui-même.
                # Nous les loguons en debug pour ne pas surcharger les logs.
                if 400 <= e.response.status_code < 500 and e.response.status_code != 429:
                    log_level = "debug"
                log_message(f"Health check pour {endpoint_key} ({service_name}) a échoué (HTTP {e.response.status_code}): {e.response.text}", level=log_level)
                success = False
            except httpx.RequestError as e:
                # Erreurs réseau (connexion, timeout, DNS, etc.)
                log_message(f"Health check pour {endpoint_key} ({service_name}) a échoué (Réseau): {e}", level="warning")
                success = False
            except Exception as e:
                # Autres erreurs inattendues
                log_message(f"Health check pour {endpoint_key} ({service_name}) a échoué (Inattendu): {e}", level="error")
                success = False
            finally:
                latency = time.monotonic() - start_time # Calcule la latence du check
                self.update_endpoint_health(service_name, endpoint_key, success, latency)
        log_message(f"Health check terminé pour le service: {service_name}")

    def update_endpoint_health(self, service_name: str, endpoint_key: str, success: bool, latency: float):
        """
        Met à jour le statut de santé d'un endpoint spécifique.
        Utilise une moyenne glissante pour le taux de succès et la latence.
        """
        # S'assure que la structure de données existe
        if service_name not in self.health_status:
            self.health_status[service_name] = {}
        if endpoint_key not in self.health_status[service_name]:
            self.health_status[service_name][endpoint_key] = {
                "latency": 0.0,
                "success_rate": 1.0,
                "last_checked": None,
                "error_count": 0,
                "total_checks": 0,
                "is_healthy": True
            }

        status = self.health_status[service_name][endpoint_key]
        status["total_checks"] += 1
        status["last_checked"] = format_datetime(get_current_time())

        alpha = 0.1 # Facteur de lissage pour les moyennes glissantes (0.1 signifie 10% de la nouvelle valeur, 90% de l'ancienne)
        if success:
            status["error_count"] = max(0, status["error_count"] - 1) # Diminue le compteur d'erreurs en cas de succès
            status["success_rate"] = status["success_rate"] * (1 - alpha) + 1.0 * alpha # Augmente le taux de succès
            status["latency"] = status["latency"] * (1 - alpha) + latency * alpha # Met à jour la latence
        else:
            status["error_count"] += 1 # Incrémente le compteur d'erreurs
            status["success_rate"] = status["success_rate"] * (1 - alpha) + 0.0 * alpha # Diminue le taux de succès
            # Si échec, pénalise la latence pour rendre l'endpoint moins attrayant (valeur arbitraire élevée)
            status["latency"] = status["latency"] * (1 - alpha) + 10.0 * alpha

        # Détermine si l'endpoint est sain basé sur le nombre d'erreurs consécutives ou le taux de succès
        if status["error_count"] >= 3 or status["success_rate"] < 0.5:
            status["is_healthy"] = False
        else:
            status["is_healthy"] = True

        # Sauvegarde l'état mis à jour de manière asynchrone
        asyncio.create_task(save_json(ENDPOINT_HEALTH_FILE, self.health_status))
        log_message(f"Santé de {service_name}:{endpoint_key} mise à jour: Succès: {success}, Latence: {latency:.2f}s, Taux Succès: {status['success_rate']:.2f}, Sain: {status['is_healthy']}", level="debug" if not status["is_healthy"] else "info")

    def get_best_endpoint(self, service_name: str) -> Optional[Dict]:
        """
        Sélectionne le meilleur endpoint pour un service donné basé sur son statut de santé.
        Priorise les endpoints sains, puis les moins mauvais en cas d'absence d'endpoints sains.
        """
        service_health = self.health_status.get(service_name)
        if not service_health:
            log_message(f"Aucune donnée de santé pour le service {service_name}. Retourne None.", level="warning")
            return None

        best_endpoint_key = None
        best_score = -float('inf')

        # Filtre les endpoints actuellement considérés comme sains
        healthy_endpoints = [
            (key, status) for key, status in service_health.items() if status["is_healthy"]
        ]

        if not healthy_endpoints:
            log_message(f"Aucun endpoint sain pour le service {service_name}. Tentative de sélection d'un endpoint non sain.", level="warning")
            all_endpoints = service_health.items()
            if not all_endpoints:
                return None # Aucun endpoint du tout

            # Si aucun endpoint sain, choisit le "moins mauvais" : moins d'erreurs, meilleure latence
            sorted_endpoints = sorted(all_endpoints, key=lambda item: (item[1]["error_count"], item[1]["latency"]))
            best_endpoint_key = sorted_endpoints[0][0]
            log_message(f"Fallback: Endpoint {best_endpoint_key} sélectionné pour {service_name} (non sain).", level="warning")
        else:
            # Calcule un score pour chaque endpoint sain pour choisir le meilleur
            for endpoint_key, status in healthy_endpoints:
                # Score = (Taux de succès * 100) - (Latence * 10) - (Compteur d'erreurs * 5)
                # Favorise le succès, pénalise la latence et les erreurs
                score = (status["success_rate"] * 100) - (status["latency"] * 10) - (status["error_count"] * 5)
                if score > best_score:
                    best_score = score
                    best_endpoint_key = endpoint_key
            log_message(f"Meilleur endpoint sélectionné pour {service_name}: {best_endpoint_key} (Score: {best_score:.2f})")

        if best_endpoint_key:
            # Une fois la clé du meilleur endpoint trouvée, on récupère sa configuration complète
            # depuis `API_CONFIG` pour l'utiliser dans la requête.
            for endpoint_config in API_CONFIG.get(service_name, []):
                current_endpoint_key = f"{endpoint_config['endpoint_name']}-{str(endpoint_config['key'])}"
                if current_endpoint_key == best_endpoint_key:
                    return endpoint_config
        return None # Aucun endpoint approprié trouvé

# Instancier le gestionnaire de santé des endpoints (sera initialisé dans main.py)
endpoint_health_manager = EndpointHealthManager()

def set_endpoint_health_manager_global(manager: EndpointHealthManager):
    """
    Permet d'injecter l'instance du gestionnaire de santé des endpoints.
    Ceci est utilisé pour s'assurer que tous les clients API utilisent la même instance.
    """
    global endpoint_health_manager
    endpoint_health_manager = manager

class APIClient:
    """
    Classe de base pour tous les clients API.
    Elle gère la sélection dynamique d'endpoints, les réessais en cas d'échec
    et l'intégration avec le gestionnaire de santé des endpoints.
    """
    def __init__(self, name: str, endpoint_health_manager: EndpointHealthManager):
        self.name = name
        self.endpoints_config = API_CONFIG.get(name, []) # Récupère la config des endpoints pour cette API
        self.endpoint_health_manager = endpoint_health_manager
        if not self.endpoints_config:
            log_message(f"Client API {self.name} initialisé sans configuration d'endpoint.", level="error")

    async def _make_request(self, params: Optional[Dict] = None, headers: Optional[Dict] = None,
                            json_data: Optional[Dict] = None, timeout: Optional[int] = None,
                            max_retries: int = 3, initial_delay: float = 1.0,
                            url: Optional[str] = None, method: Optional[str] = None,
                            key_field: Optional[str] = None, key_location: Optional[str] = None,
                            api_key: Optional[Union[str, Tuple[str, str]]] = None,
                            fixed_params: Optional[Dict] = None, fixed_headers: Optional[Dict] = None,
                            fixed_json: Optional[Dict] = None) -> Optional[Union[Dict, str, bytes]]:
        """
        Méthode interne pour effectuer les requêtes HTTP en utilisant le meilleur endpoint avec réessais.

        Args:
            params (Dict, optional): Paramètres de requête à ajouter à l'URL.
            headers (Dict, optional): En-têtes HTTP supplémentaires.
            json_data (Dict, optional): Données JSON à envoyer dans le corps de la requête (pour POST/PUT).
            timeout (int, optional): Timeout pour la requête en secondes.
            max_retries (int): Nombre maximal de tentatives en cas d'échec.
            initial_delay (float): Délai initial entre les réessais (exponentiel).
            url (str, optional): URL spécifique à utiliser (si non basé sur un endpoint configuré).
            method (str, optional): Méthode HTTP (GET, POST, etc.).
            key_field (str, optional): Nom du champ pour la clé API.
            key_location (str, optional): Où placer la clé API ('param', 'header', 'auth_basic').
            api_key (Union[str, Tuple[str, str]], optional): Clé API à utiliser.
            fixed_params (Dict, optional): Paramètres fixes définis dans la configuration de l'endpoint.
            fixed_headers (Dict, optional): En-têtes fixes définis dans la configuration de l'endpoint.
            fixed_json (Dict, optional): Données JSON fixes définies dans la configuration de l'endpoint.

        Returns:
            Union[Dict, str, bytes, None]: La réponse de l'API (JSON décodé, texte brut, ou bytes pour les images),
                                          ou un dictionnaire d'erreur en cas d'échec.
        """

        selected_endpoint_config = None
        endpoint_key_for_health = "Dynamic" # Clé par défaut pour les requêtes dynamiques (non configurées)

        if url and method: # Si l'URL et la méthode sont fournies directement (requête dynamique)
            selected_endpoint_config = {
                "url": url,
                "method": method,
                "key_field": key_field,
                "key_location": key_location,
                "key": api_key,
                "fixed_params": fixed_params if fixed_params is not None else {},
                "fixed_headers": fixed_headers if fixed_headers is not None else {},
                "fixed_json": fixed_json if fixed_json is not None else {},
                "endpoint_name": "Dynamic", # Nom générique pour les endpoints dynamiques
                "timeout": timeout if timeout is not None else 30
            }
            if api_key:
                # Crée une clé de santé unique pour les requêtes dynamiques avec clé API
                endpoint_key_for_health = f"Dynamic-{str(api_key)}"
            log_message(f"Requête dynamique pour {self.name} vers {url}")
        else: # Sélectionne le meilleur endpoint configuré via le gestionnaire de santé
            selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
            if not selected_endpoint_config:
                log_message(f"Aucun endpoint sain ou disponible pour {self.name}.", level="error")
                return {"error": True, "message": f"Aucun endpoint sain ou disponible pour {self.name}."}
            # Construit la clé de santé pour l'endpoint sélectionné
            endpoint_key_for_health = f"{selected_endpoint_config['endpoint_name']}-{str(selected_endpoint_config['key'])}"
            log_message(f"Endpoint sélectionné pour {self.name}: {selected_endpoint_config['endpoint_name']}")
            # Utilise le timeout de la config de l'endpoint si non spécifié
            timeout = timeout if timeout is not None else selected_endpoint_config.get("timeout", 30)

        url_to_use = selected_endpoint_config["url"]
        method_to_use = selected_endpoint_config["method"]

        # Initialise les paramètres, en-têtes et données JSON avec les valeurs fixes de la config
        request_params = selected_endpoint_config.get("fixed_params", {}).copy()
        request_headers = selected_endpoint_config.get("fixed_headers", {}).copy()
        request_json_data = selected_endpoint_config.get("fixed_json", {}).copy()
        auth = None # Pour l'authentification de base

        # Fusionne les paramètres/en-têtes/données JSON fournis avec les valeurs fixes
        if params:
            request_params.update(params)
        if headers:
            request_headers.update(headers)
        if json_data:
            request_json_data.update(json_data)

        # Gère l'insertion de la clé API pour la requête
        key_field_to_use = selected_endpoint_config.get("key_field")
        key_location_to_use = selected_endpoint_config.get("key_location")
        key_prefix = selected_endpoint_config.get("key_prefix", "")
        api_key_to_use = selected_endpoint_config["key"]

        if key_field_to_use and key_location_to_use:
            if key_location_to_use == "param":
                request_params[key_field_to_use] = api_key_to_use
            elif key_location_to_use == "header":
                request_headers[key_field_to_use] = f"{key_prefix}{api_key_to_use}"
            elif key_location_to_use == "auth_basic":
                if isinstance(api_key_to_use, tuple) and len(api_key_to_use) == 2:
                    auth = httpx.BasicAuth(api_key_to_use[0], api_key_to_use[1])
                else:
                    log_message(f"Clé API pour auth_basic non valide pour {self.name}:{endpoint_key_for_health}", level="error")
                    # Marque l'endpoint comme non sain et retourne une erreur
                    self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, 0.0)
                    return {"error": True, "message": "Configuration d'authentification basique invalide."}

        current_delay = initial_delay # Délai initial pour les réessais
        for attempt in range(max_retries):
            start_time = time.monotonic()
            success = False
            try:
                async with httpx.AsyncClient(timeout=timeout) as client:
                    response = await client.request(method_to_use, url_to_use, params=request_params, headers=request_headers, json=request_json_data, auth=auth)
                    response.raise_for_status() # Lève une exception pour les codes d'état HTTP 4xx/5xx
                    success = True

                    content_type = response.headers.get("Content-Type", "").lower()
                    if "application/json" in content_type:
                        try:
                            return response.json() # Tente de décoder la réponse JSON
                        except json.JSONDecodeError:
                            log_message(f"API {self.name} réponse non JSON valide (tentative {attempt+1}/{max_retries}): {response.text[:200]}...", level="warning")
                            if attempt < max_retries - 1: # Si ce n'est pas la dernière tentative, réessaie
                                await asyncio.sleep(current_delay)
                                current_delay *= 2 # Augmente le délai de manière exponentielle
                                continue
                            return {"error": True, "message": "Réponse API non JSON valide.", "raw_response": response.text}
                    else:
                        log_message(f"API {self.name} a renvoyé un Content-Type non JSON: {content_type}", level="info")
                        return response.content # Retourne le contenu brut (ex: pour les images)

            except httpx.HTTPStatusError as e:
                log_message(f"API {self.name} erreur HTTP (tentative {attempt+1}/{max_retries}): {e.response.status_code} - {e.response.text}", level="warning")
                # Ne pas réessayer pour les erreurs client (4xx) sauf 429 (Too Many Requests)
                if 400 <= e.response.status_code < 500 and e.response.status_code != 429:
                    log_message(f"API {self.name}: Erreur client {e.response.status_code}, pas de réessai.", level="error")
                    self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, e.response.elapsed.total_seconds())
                    return {"error": True, "status_code": e.response.status_code, "message": e.response.text}

                if attempt < max_retries - 1:
                    log_message(f"API {self.name}: Réessai dans {current_delay:.2f}s...", level="info")
                    await asyncio.sleep(current_delay)
                    current_delay *= 2
            except httpx.RequestError as e:
                log_message(f"API {self.name} erreur de requête (tentative {attempt+1}/{max_retries}): {e}", level="warning")
                if attempt < max_retries - 1:
                    log_message(f"API {self.name}: Réessai dans {current_delay:.2f}s...", level="info")
                    await asyncio.sleep(current_delay)
                    current_delay *= 2
            except Exception as e:
                log_message(f"API {self.name} erreur inattendue (tentative {attempt+1}/{max_retries}): {e}", level="error")
                self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, time.monotonic() - start_time)
                return {"error": True, "message": str(e)}
            finally:
                # Met à jour la santé de l'endpoint même en cas d'échec pour la sélection future
                if not success:
                    latency = time.monotonic() - start_time
                    self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, latency)

        log_message(f"API {self.name}: Toutes les tentatives ont échoué après {max_retries} réessais.", level="error")
        return {"error": True, "message": f"Échec de la requête après {max_retries} tentatives."}

    async def query(self, *args, **kwargs) -> Any:
        """
        Méthode abstraite pour interroger l'API.
        Doit être implémentée par chaque sous-classe de client API.
        """
        raise NotImplementedError("La méthode query doit être implémentée par les sous-classes.")

# --- Clients API Spécifiques ---
# Chaque classe de client API hérite de `APIClient` et implémente la méthode `query`
# pour interagir avec une API spécifique.

class DeepSeekClient(APIClient):
    def __init__(self):
        super().__init__("DEEPSEEK", endpoint_health_manager)

    async def query(self, prompt: str, model: str = "deepseek-chat") -> str:
        """Interroge l'API DeepSeek pour des complétions de chat."""
        payload = {"model": model, "messages": [{"role": "user", "content": prompt}]}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            content = response.get("choices", [{}])[0].get("message", {}).get("content")
            return content if content else "DeepSeek: Pas de contenu de réponse trouvé."
        return f"DeepSeek: Erreur: {response.get('message', 'Inconnu')}" if response else "DeepSeek: Réponse vide ou erreur interne."

class SerperClient(APIClient):
    def __init__(self):
        super().__init__("SERPER", endpoint_health_manager)

    async def query(self, query_text: str) -> str:
        """Effectue une recherche web via l'API Serper."""
        payload = {"q": query_text}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            organic_results = response.get("organic", [])
            if organic_results:
                snippet = organic_results[0].get("snippet", "Pas de snippet.")
                link = organic_results[0].get("link", "")
                return f"Serper (recherche web):\n{snippet} {neutralize_urls(link)}"
            return "Serper: Aucune information trouvée."
        return f"Serper: Erreur: {response.get('message', 'Inconnu')}" if response else "Serper: Réponse vide ou erreur interne."

class WolframAlphaClient(APIClient):
    def __init__(self):
        super().__init__("WOLFRAMALPHA", endpoint_health_manager)

    async def query(self, input_text: str) -> str:
        """Interroge WolframAlpha pour des calculs ou des faits."""
        params = {"input": input_text}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            pods = response.get("queryresult", {}).get("pods", [])
            if pods:
                for pod in pods:
                    if pod.get("title") in ["Result", "Input interpretation", "Decimal approximation"]:
                        subpods = pod.get("subpods", [])
                        if subpods and subpods[0].get("plaintext"):
                            return f"WolframAlpha:\n{subpods[0]['plaintext']}"
                if pods and pods[0].get("subpods") and pods[0]["subpods"][0].get("plaintext"):
                    return f"WolframAlpha:\n{pods[0]['subpods'][0]['plaintext']}"
            return "WolframAlpha: Pas de résultat clair."
        return f"WolframAlpha: Erreur: {response.get('message', 'Inconnu')}" if response else "WolframAlpha: Réponse vide ou erreur interne."

class TavilyClient(APIClient):
    def __init__(self):
        super().__init__("TAVILY", endpoint_health_manager)

    async def query(self, query_text: str, max_results: int = 3) -> str:
        """Effectue une recherche web avancée via l'API Tavily."""
        payload = {"query": query_text, "max_results": max_results}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            results = response.get("results", [])
            answer = response.get("answer", "Aucune réponse directe trouvée.")

            output = f"Tavily (recherche web):\nRéponse directe: {answer}\n"
            if results:
                output += "Extraits pertinents:\n"
                for i, res in enumerate(results[:max_results]):
                    output += f"- {res.get('title', 'N/A')}: {res.get('content', 'N/A')} {neutralize_urls(res.get('url', ''))}\n"
            return output
        return f"Tavily: Erreur: {response.get('message', 'Inconnu')}" if response else "Tavily: Réponse vide ou erreur interne."

class ApiFlashClient(APIClient):
    def __init__(self):
        super().__init__("APIFLASH", endpoint_health_manager)

    async def query(self, url: str) -> str:
        """Capture une capture d'écran d'une URL via ApiFlash."""
        params = {"url": url, "format": "jpeg", "full_page": "true"}
        response_content = await self._make_request(params=params)

        if isinstance(response_content, bytes):
            selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
            if selected_endpoint_config:
                capture_url = f"{selected_endpoint_config['url']}?access_key={selected_endpoint_config['key']}&url={url}&format=jpeg&full_page=true"
                return f"ApiFlash (capture d'écran): {neutralize_urls(capture_url)} (Vérifiez le lien pour l'image)"
            return "ApiFlash: Impossible de générer l'URL de capture."
        elif isinstance(response_content, dict) and response_content.get("error"):
            return f"ApiFlash: Erreur: {response_content.get('message', 'Inconnu')}"
        else:
            log_message(f"ApiFlash a renvoyé un type de réponse inattendu: {type(response_content)}", level="warning")
            return f"ApiFlash: Réponse inattendue de l'API. {response_content}"

class CrawlbaseClient(APIClient):
    def __init__(self):
        super().__init__("CRAWLBASE", endpoint_health_manager)

    async def query(self, url: str, use_js: bool = False) -> str:
        """Scrape le contenu HTML ou JavaScript d'une URL via Crawlbase."""
        params = {"url": url, "format": "json"}

        selected_endpoint_config = None
        if use_js:
            for config in API_CONFIG.get(self.name, []):
                if "JS Scraping" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break

        if not selected_endpoint_config:
            selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)

        if not selected_endpoint_config:
            return f"Crawlbase: Aucun endpoint sain ou disponible pour {self.name}."

        response = await self._make_request(
            params=params,
            url=selected_endpoint_config["url"],
            method=selected_endpoint_config["method"],
            key_field=selected_endpoint_config["key_field"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            fixed_params=selected_endpoint_config.get("fixed_params", {}),
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            body = response.get("body")
            if body:
                try:
                    decoded_body = base64.b64decode(body).decode('utf-8', errors='ignore')
                    return f"Crawlbase (contenu web):\n{decoded_body[:1000]}..."
                except Exception:
                    return f"Crawlbase (contenu web - brut):\n{body[:1000]}..."
            return "Crawlbase: Contenu non trouvé."
        return f"Crawlbase: Erreur: {response.get('message', 'Inconnu')}" if response else "Crawlbase: Réponse vide ou erreur interne."

class DetectLanguageClient(APIClient):
    def __init__(self):
        super().__init__("DETECTLANGUAGE", endpoint_health_manager)

    async def query(self, text: str) -> str:
        """Détecte la langue d'un texte via DetectLanguage API."""
        payload = {"q": text}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            detections = response.get("data", {}).get("detections", [])
            if detections:
                first_detection = detections[0]
                lang = first_detection.get("language")
                confidence = first_detection.get("confidence")
                return f"Langue détectée: {lang} (confiance: {confidence})"
            return "DetectLanguage: Aucune langue détectée."
        return f"DetectLanguage: Erreur: {response.get('message', 'Inconnu')}" if response else "DetectLanguage: Réponse vide ou erreur interne."

class GuardianClient(APIClient):
    def __init__(self):
        super().__init__("GUARDIAN", endpoint_health_manager)

    async def query(self, query_text: str) -> str:
        """Recherche des articles de presse via l'API The Guardian."""
        params = {"q": query_text}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            results = response.get("response", {}).get("results", [])
            if results:
                output = "Articles The Guardian:\n"
                for res in results[:3]: # Limite à 3 articles pour la concision
                    output += f"- {res.get('webTitle', 'N/A')}: {res.get('fields', {}).get('trailText', 'N/A')} {neutralize_urls(res.get('webUrl', ''))}\n"
                return output
            return "Guardian: Aucun article trouvé."
        return f"Guardian: Erreur: {response.get('message', 'Inconnu')}" if response else "Guardian: Réponse vide ou erreur interne."

class IP2LocationClient(APIClient):
    def __init__(self):
        super().__init__("IP2LOCATION", endpoint_health_manager)

    async def query(self, ip_address: str) -> str:
        """Géolocalise une adresse IP via IP2Location API."""
        params = {"ip": ip_address}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            if "country_name" in response:
                return f"IP2Location (Géolocalisation IP {ip_address}): Pays: {response['country_name']}, Ville: {response.get('city_name', 'N/A')}"
            return "IP2Location: Informations non trouvées."
        return f"IP2Location: Erreur: {response.get('message', 'Inconnu')}" if response else "IP2Location: Réponse vide ou erreur interne."

class ShodanClient(APIClient):
    def __init__(self):
        super().__init__("SHODAN", endpoint_health_manager)

    async def query(self, query_text: str = "") -> str:
        """
        Interroge Shodan pour des informations sur un hôte IP ou des informations sur la clé API.
        Si `query_text` est une IP, tente de récupérer les infos de l'hôte.
        Sinon, ou en cas d'échec, retourne les infos de la clé API.
        """
        if re.match(r"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$", query_text): # Vérifie si c'est une adresse IP
            selected_endpoint_config = None
            for config in API_CONFIG.get(self.name, []):
                if "Host Info" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break
            if selected_endpoint_config:
                # Construire l'URL pour la recherche d'hôte
                url = f"{selected_endpoint_config['url'].rstrip('/')}/{query_text}"
                response = await self._make_request(
                    params={"key": selected_endpoint_config["key"]},
                    url=url,
                    method="GET",
                    key_field=selected_endpoint_config["key_field"],
                    key_location=selected_endpoint_config["key_location"],
                    api_key=selected_endpoint_config["key"],
                    timeout=selected_endpoint_config.get("timeout")
                )
                if response and not response.get("error"):
                    return f"Shodan (info hôte {query_text}): Pays: {response.get('country_name', 'N/A')}, Ports: {response.get('ports', 'N/A')}, Vulnérabilités: {response.get('vulns', 'Aucune')}"
                return f"Shodan (info hôte): Erreur: {response.get('message', 'Inconnu')}" if response else "Shodan: Réponse vide ou erreur interne."
            else:
                return "Shodan: Endpoint 'Host Info' non configuré."
        else:
            # Si pas d'IP ou si la recherche d'hôte n'est pas applicable, retourne les infos de la clé API
            response = await self._make_request() # Utilise le premier endpoint disponible (API Info)
            if response and not response.get("error"):
                return f"Shodan (info clé): Requêtes restantes: {response.get('usage_limits', {}).get('query_credits', 'N/A')}, Scan crédits: {response.get('usage_limits', {}).get('scan_credits', 'N/A')}"
            return f"Shodan: Erreur: {response.get('message', 'Inconnu')}" if response else "Shodan: Réponse vide ou erreur interne."

class WeatherAPIClient(APIClient):
    def __init__(self):
        super().__init__("WEATHERAPI", endpoint_health_manager)

    async def query(self, location: str) -> str:
        """Récupère les conditions météorologiques actuelles pour une localisation via WeatherAPI."""
        params = {"q": location}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            current = response.get("current", {})
            location_info = response.get("location", {})
            if current and location_info:
                return (
                    f"Météo à {location_info.get('name', 'N/A')}, {location_info.get('country', 'N/A')}:\n"
                    f"Température: {current.get('temp_c', 'N/A')}°C, "
                    f"Conditions: {current.get('condition', {}).get('text', 'N/A')}, "
                    f"Vent: {current.get('wind_kph', 'N/A')} km/h"
                )
            return "WeatherAPI: Données météo non trouvées."
        return f"WeatherAPI: Erreur: {response.get('message', 'Inconnu')}" if response else "WeatherAPI: Réponse vide ou erreur interne."

class CloudmersiveClient(APIClient):
    def __init__(self):
        super().__init__("CLOUDMERSIVE", endpoint_health_manager)

    async def query(self, domain: str) -> str:
        """Vérifie la validité et le type d'un domaine via Cloudmersive API."""
        payload = {"domain": domain}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            return f"Cloudmersive (vérification de domaine {domain}): Valide: {response.get('ValidDomain', 'N/A')}, Type: {response.get('DomainType', 'N/A')}"
        return f"Cloudmersive: Erreur: {response.get('message', 'Inconnu')}" if response else "Cloudmersive: Réponse vide ou erreur interne."

class GreyNoiseClient(APIClient):
    def __init__(self):
        super().__init__("GREYNOISE", endpoint_health_manager)

    async def query(self, ip_address: str) -> str:
        """Analyse une adresse IP pour détecter des activités 'bruit' (malveillantes) via GreyNoise."""
        selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
        if not selected_endpoint_config:
            return f"GreyNoise: Aucun endpoint sain ou disponible pour {self.name}."

        # Construit l'URL avec l'adresse IP à la fin
        url = f"{selected_endpoint_config['url'].rstrip('/')}/{ip_address}"
        method = selected_endpoint_config["method"]
        headers = {selected_endpoint_config["key_field"]: selected_endpoint_config["key"]}

        response = await self._make_request(
            headers=headers,
            url=url,
            method=method,
            key_field=selected_endpoint_config["key_field"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if response.get("noise"):
                return f"GreyNoise (IP {ip_address}): C'est une IP 'bruit' (malveillante). Classification: {response.get('classification', 'N/A')}, Nom d'acteur: {response.get('actor', 'N/A')}"
            return f"GreyNoise (IP {ip_address}): Pas de 'bruit' détecté. Statut: {response.get('status', 'N/A')}"
        return f"GreyNoise: Erreur: {response.get('message', 'Inconnu')}" if response else "GreyNoise: Réponse vide ou erreur interne."

class PulsediveClient(APIClient):
    def __init__(self):
        super().__init__("PULSEDIVE", endpoint_health_manager)

    async def query(self, indicator: str, type: str = "auto") -> str:
        """Analyse un indicateur de menace (IP, domaine, URL) via Pulsedive."""
        params = {"indicator": indicator, "type": type}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            if response.get("results"):
                result = response["results"][0] # Prend le premier résultat
                return (
                    f"Pulsedive (Analyse {indicator}): Type: {result.get('type', 'N/A')}, "
                    f"Risk: {result.get('risk', 'N/A')}, "
                    f"Description: {result.get('description', 'N/A')[:200]}..." # Tronque la description
                )
            return "Pulsedive: Aucun résultat d'analyse trouvé."
        return f"Pulsedive: Erreur: {response.get('message', 'Inconnu')}" if response else "Pulsedive: Réponse vide ou erreur interne."

class StormGlassClient(APIClient):
    def __init__(self):
        super().__init__("STORMGLASS", endpoint_health_manager)

    async def query(self, lat: float, lng: float, params: str = "airTemperature,waveHeight") -> str:
        """Récupère les données météorologiques maritimes pour une coordonnée via StormGlass."""
        now = int(time.time())
        request_params = {
            "lat": lat,
            "lng": lng,
            "params": params,
            "start": now,
            "end": now + 3600 # Prévisions pour la prochaine heure (3600 secondes)
        }
        response = await self._make_request(params=request_params)
        if response and not response.get("error"):
            data = response.get("hours", [])
            if data:
                first_hour = data[0]
                # Accède aux valeurs spécifiques des paramètres demandés
                temp = first_hour.get('airTemperature', [{}])[0].get('value', 'N/A')
                wave_height = first_hour.get('waveHeight', [{}])[0].get('value', 'N/A')
                return f"StormGlass (Météo maritime à {lat},{lng}): Température air: {temp}°C, Hauteur vagues: {wave_height}m"
            return "StormGlass: Données non trouvées."
        return f"StormGlass: Erreur: {response.get('message', 'Inconnu')}" if response else "StormGlass: Réponse vide ou erreur interne."

class LoginRadiusClient(APIClient):
    def __init__(self):
        super().__init__("LOGINRADIUS", endpoint_health_manager)

    async def query(self) -> str:
        """Effectue un simple ping à l'API LoginRadius pour vérifier sa disponibilité."""
        response = await self._make_request()
        if response and not response.get("error"):
            return f"LoginRadius (Ping API): Statut: {response.get('Status', 'N/A')}, Message: {response.get('Message', 'N/A')}"
        return f"LoginRadius: Erreur: {response.get('message', 'Inconnu')}" if response else "LoginRadius: Réponse vide ou erreur interne."

class JsonbinClient(APIClient):
    def __init__(self):
        super().__init__("JSONBIN", endpoint_health_manager)

    async def query(self, data: Optional[Dict[str, Any]] = None, private: bool = True, bin_id: Optional[str] = None) -> str:
        """
        Crée un nouveau 'bin' JSON ou accède à un bin existant via Jsonbin.io.
        `data` est pour la création, `bin_id` pour l'accès.
        """
        if bin_id: # Accès à un bin existant
            selected_endpoint_config = None
            for config in API_CONFIG.get(self.name, []):
                if "Bin Access" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break
            if not selected_endpoint_config:
                return f"Jsonbin: Aucun endpoint d'accès de bin sain ou disponible pour {self.name}."

            url = f"{selected_endpoint_config['url'].rstrip('/')}/{bin_id}"
            method = "GET"
            headers = {selected_endpoint_config["key_field"]: selected_endpoint_config["key"]}

            response = await self._make_request(
                headers=headers,
                url=url,
                method=method,
                timeout=selected_endpoint_config.get("timeout")
            )
            if response and not response.get("error"):
                return f"Jsonbin (Accès bin {bin_id}):\n{json.dumps(response, indent=2)}"
            return f"Jsonbin (Accès bin): Erreur: {response.get('message', 'Inconnu')}" if response else "Jsonbin: Réponse vide ou erreur interne."

        else: # Création d'un nouveau bin
            selected_endpoint_config = None
            for config in API_CONFIG.get(self.name, []):
                if "Bin Create" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break

            if not selected_endpoint_config:
                return f"Jsonbin: Aucun endpoint de création de bin sain ou disponible pour {self.name}."

            url = selected_endpoint_config["url"]
            method = "POST"
            headers = {selected_endpoint_config["key_field"]: selected_endpoint_config["key"], "Content-Type": "application/json"}
            payload = {"record": data if data is not None else {}, "private": private}

            response = await self._make_request(
                json_data=payload,
                headers=headers,
                url=url,
                method=method,
                timeout=selected_endpoint_config.get("timeout")
            )

            if response and not response.get("error"):
                return f"Jsonbin (Création de bin): ID: {response.get('metadata', {}).get('id', 'N/A')}, URL: {neutralize_urls(response.get('metadata', {}).get('url', 'N/A'))}"
            return f"Jsonbin (Création de bin): Erreur: {response.get('message', 'Inconnu')}" if response else "Jsonbin: Réponse vide ou erreur interne."

class HuggingFaceClient(APIClient):
    def __init__(self):
        super().__init__("HUGGINGFACE", endpoint_health_manager)

    async def query(self, model_name: str = "distilbert-base-uncased-finetuned-sst-2-english", input_text: str = "Hello world") -> str:
        """Effectue une inférence sur un modèle HuggingFace (ex: classification de texte, génération)."""
        selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
        if not selected_endpoint_config:
            return f"HuggingFace: Aucun endpoint sain ou disponible pour {self.name}."

        inference_url = f"https://api-inference.huggingface.co/models/{model_name}"

        headers = {
            selected_endpoint_config["key_field"]: f"{selected_endpoint_config['key_prefix']}{selected_endpoint_config['key']}",
            "Content-Type": "application/json"
        }
        payload = {"inputs": input_text}

        response = await self._make_request(
            json_data=payload,
            headers=headers,
            url=inference_url,
            method="POST",
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if isinstance(response, list) and response:
                first_result = response[0]
                if isinstance(first_result, list) and first_result:
                    return f"HuggingFace ({model_name} - {first_result[0].get('label')}): Score {first_result[0].get('score', 'N/A'):.2f}"
                elif isinstance(first_result, dict) and "generated_text" in first_result:
                    return f"HuggingFace ({model_name}): {first_result.get('generated_text')}"
            return f"HuggingFace ({model_name}): Réponse non parsée. {response}"
        return f"HuggingFace: Erreur: {response.get('message', 'Inconnu')}" if response else "HuggingFace: Réponse vide ou erreur interne."

class TwilioClient(APIClient):
    def __init__(self):
        super().__init__("TWILIO", endpoint_health_manager)

    async def query(self) -> str:
        """Récupère le solde du compte Twilio."""
        selected_endpoint_config = None
        for config in API_CONFIG.get(self.name, []):
            if "Account Balance" in config.get("endpoint_name", ""):
                selected_endpoint_config = config
                break
        if not selected_endpoint_config:
            if self.endpoints_config:
                selected_endpoint_config = self.endpoints_config[0]
            else:
                return f"Twilio: Aucune configuration d'endpoint disponible pour {self.name}."

        response = await self._make_request(
            url=selected_endpoint_config["url"],
            method=selected_endpoint_config["method"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            timeout=selected_endpoint_config.get("timeout")
        )
        if response and not response.get("error"):
            return f"Twilio (Balance): {response.get('balance', 'N/A')} {response.get('currency', 'N/A')}"
        return f"Twilio: Erreur: {response.get('message', 'Inconnu')}" if response else "Twilio: Réponse vide ou erreur interne."

class AbstractAPIClient(APIClient):
    def __init__(self):
        super().__init__("ABSTRACTAPI", endpoint_health_manager)

    async def query(self, input_value: str, api_type: str) -> str:
        """
        Interroge diverses APIs d'AbstractAPI (validation email/téléphone, taux de change, jours fériés).
        `input_value` dépend du `api_type`.
        """
        params = {}
        target_endpoint_name = ""

        if api_type == "PHONE_VALIDATION":
            params["phone"] = input_value
            target_endpoint_name = "Phone Validation"
        elif api_type == "EMAIL_VALIDATION":
            params["email"] = input_value
            target_endpoint_name = "Email Validation"
        elif api_type == "EXCHANGE_RATES":
            params["base"] = input_value if input_value else "USD"
            target_endpoint_name = "Exchange Rates"
        elif api_type == "HOLIDAYS":
            params["country"] = input_value if input_value else "US"
            params["year"] = datetime.now().year
            target_endpoint_name = "Holidays"
        else:
            return f"AbstractAPI: Type d'API '{api_type}' non supporté pour la requête."

        selected_endpoint_config = None
        for config in API_CONFIG.get(self.name, []):
            if target_endpoint_name in config["endpoint_name"]:
                selected_endpoint_config = config
                break

        if not selected_endpoint_config:
            return f"AbstractAPI: Aucun endpoint sain ou disponible pour {self.name} pour le type {api_type}."

        response = await self._make_request(
            params=params,
            url=selected_endpoint_config["url"],
            method=selected_endpoint_config["method"],
            key_field=selected_endpoint_config["key_field"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            fixed_params=selected_endpoint_config.get("fixed_params", {}),
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if api_type == "PHONE_VALIDATION":
                return (
                    f"AbstractAPI (Validation Tél): Numéro: {response.get('phone', 'N/A')}, "
                    f"Valide: {response.get('valid', 'N/A')}, "
                    f"Pays: {response.get('country', {}).get('name', 'N/A')}"
                )
            elif api_type == "EMAIL_VALIDATION":
                return (
                    f"AbstractAPI (Validation Email): Email: {response.get('email', 'N/A')}, "
                    f"Valide: {response.get('is_valid_format', 'N/A')}, "
                    f"Deliverable: {response.get('is_deliverable', 'N/A')}"
                )
            elif api_type == "EXCHANGE_RATES":
                return f"AbstractAPI (Taux de change): Base: {response.get('base', 'N/A')}, Taux (USD): {response.get('exchange_rates', {}).get('USD', 'N/A')}"
            elif api_type == "HOLIDAYS":
                holidays = [h.get('name', 'N/A') for h in response if h.get('name')]
                return f"AbstractAPI (Jours fériés {params.get('country', 'US')} {params.get('year')}): {', '.join(holidays[:5])}..." if holidays else "Aucun jour férié trouvé."
            return f"AbstractAPI ({api_type}): Réponse brute: {response}"
        return f"AbstractAPI ({api_type}): Erreur: {response.get('message', 'Inconnu')}" if response else "AbstractAPI: Réponse vide ou erreur interne."

class GeminiAPIClient:
    def __init__(self):
        self.api_key = GEMINI_API_KEY
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models/"
        self.model_name = "gemini-1.5-flash-latest"
        self.headers = {
            "Content-Type": "application/json",
        }
        from config import GEMINI_TEMPERATURE, GEMINI_TOP_P, GEMINI_TOP_K, GEMINI_MAX_OUTPUT_TOKENS, GEMINI_SAFETY_SETTINGS
        self.generation_config = {
            "temperature": GEMINI_TEMPERATURE,
            "top_p": GEMINI_TOP_P,
            "top_k": GEMINI_TOP_K,
            "max_output_tokens": GEMINI_MAX_OUTPUT_TOKENS,
        }
        self.safety_settings = GEMINI_SAFETY_SETTINGS
        log_message(f"GeminiApiClient initialisé avec le modèle par défaut: {self.model_name}")

    async def generate_content(self, prompt: str, chat_history: List[Dict], image_data: Optional[str] = None, model: Optional[str] = None) -> Dict:
        """
        Génère du contenu textuel ou multimodal en utilisant l'API Gemini.
        `chat_history` est une liste de dictionnaires au format Gemini (role, parts).
        `image_data` est une chaîne base64 de l'image avec son préfixe mimeType (ex: "data:image/png;base64,...").
        `model` permet de spécifier un modèle différent si nécessaire.
        """
        model_to_use = model if model else self.model_name
        url = f"{self.base_url}{model_to_use}:generateContent?key={self.api_key}"

        contents = []
        for msg in chat_history:
            role = "user" if msg["role"] == "user" else "model"
            contents.append({"role": role, "parts": [{"text": msg["content"]}]})

        user_parts = [{"text": prompt}]
        if image_data:
            if "," in image_data:
                mime_type_part, base64_data = image_data.split(",", 1)
                mime_type = mime_type_part.split(":", 1)[1].split(";", 1)[0]
            else:
                mime_type = "image/jpeg"
                base64_data = image_data

            user_parts.append({
                "inlineData": {
                    "mimeType": mime_type,
                    "data": base64_data
                }
            })
            log_message(f"Image ajoutée au prompt Gemini (mimeType: {mime_type}).")

        contents.append({"role": "user", "parts": user_parts})

        payload = {
            "contents": contents,
            "generationConfig": self.generation_config,
            "safetySettings": self.safety_settings
        }

        log_message(f"Appel à Gemini API pour le modèle {model_to_use}...")
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(url, headers=self.headers, json=payload)
                response.raise_for_status()
                result = response.json()
                log_message(f"Réponse Gemini reçue: {json.dumps(result, indent=2)}")
                return result
        except httpx.HTTPStatusError as e:
            log_message(f"Erreur HTTP Gemini API: {e.response.status_code} - {e.response.text}", level="error")
            return {"error": f"Erreur HTTP Gemini: {e.response.status_code} - {e.response.text}"}
        except httpx.RequestError as e:
            log_message(f"Erreur de requête Gemini API: {e}", level="error")
            return {"error": f"Erreur de requête Gemini: {e}"}
        except json.JSONDecodeError as e:
            log_message(f"Erreur de décodage JSON Gemini API: {e} - Réponse brute: {response.text}", level="error")
            return {"error": f"Erreur de décodage JSON Gemini: {e}"}
        except Exception as e:
            log_message(f"Erreur inattendue Gemini API: {e}\n{traceback.format_exc()}", level="error")
            return {"error": f"Erreur inattendue Gemini: {e}"}

class GoogleCustomSearchClient(APIClient):
    def __init__(self):
        super().__init__("GOOGLE_CUSTOM_SEARCH", endpoint_health_manager)

    async def query(self, query_text: str) -> str:
        """Effectue une recherche personnalisée Google via l'API Custom Search."""
        params = {"q": query_text}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            items = response.get("items", [])
            if items:
                output = "Google Custom Search:\n"
                for item in items[:3]:
                    output += f"- {item.get('title', 'N/A')}: {item.get('snippet', 'N/A')} {neutralize_urls(item.get('link', ''))}\n"
                return output
            return "Google Custom Search: Aucun résultat trouvé."
        return f"Google Custom Search: Erreur: {response.get('message', 'Inconnu')}" if response else "Google Custom Search: Réponse vide ou erreur interne."

class RandommerClient(APIClient):
    def __init__(self):
        super().__init__("RANDOMMER", endpoint_health_manager)

    async def query(self, country_code: str = "US", quantity: int = 1) -> str:
        """Génère des numéros de téléphone aléatoires via Randommer.io."""
        params = {"CountryCode": country_code, "Quantity": quantity}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            if isinstance(response, list) and response:
                return f"Randommer (Numéros de téléphone): {', '.join(response)}"
            return f"Randommer: {response}"
        return f"Randommer: Erreur: {response.get('message', 'Inconnu')}" if response else "Randommer: Réponse vide ou erreur interne."

class TomorrowIOClient(APIClient):
    def __init__(self):
        super().__init__("TOMORROW.IO", endpoint_health_manager)

    async def query(self, location: str, fields: Optional[List[str]] = None) -> str:
        """Récupère les prévisions météorologiques via Tomorrow.io."""
        if fields is None:
            fields = ["temperature", "humidity", "windSpeed"]
        payload = {"location": location, "fields": fields, "units": "metric", "timesteps": ["1h"]}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            data = response.get("data", {}).get("timelines", [{}])[0].get("intervals", [{}])[0].get("values", {})
            if data:
                output = f"Météo (Tomorrow.io) à {location}:\n"
                for field in fields:
                    output += f"- {field.capitalize()}: {data.get(field, 'N/A')}\n"
                return output
            return "Tomorrow.io: Données météo non trouvées."
        return f"Tomorrow.io: Erreur: {response.get('message', 'Inconnu')}" if response else "Tomorrow.io: Réponse vide ou erreur interne."

class OpenWeatherMapClient(APIClient):
    def __init__(self):
        super().__init__("OPENWEATHERMAP", endpoint_health_manager)

    async def query(self, location: str) -> str:
        """Récupère les conditions météorologiques actuelles via OpenWeatherMap."""
        params = {"q": location}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            main_data = response.get("main", {})
            weather_desc = response.get("weather", [{}])[0].get("description", "N/A")
            if main_data:
                temp_kelvin = main_data.get('temp', 'N/A')
                feels_like_kelvin = main_data.get('feels_like', 'N/A')

                temp_celsius = f"{temp_kelvin - 273.15:.2f}" if isinstance(temp_kelvin, (int, float)) else "N/A"
                feels_like_celsius = f"{feels_like_kelvin - 273.15:.2f}" if isinstance(feels_like_kelvin, (int, float)) else "N/A"

                return (
                    f"Météo (OpenWeatherMap) à {location}:\n"
                    f"Température: {temp_celsius}°C, "
                    f"Ressenti: {feels_like_celsius}°C, "
                    f"Humidité: {main_data.get('humidity', 'N/A')}%, "
                    f"Conditions: {weather_desc}"
                )
            return "OpenWeatherMap: Données météo non trouvées."
        return f"OpenWeatherMap: Erreur: {response.get('message', 'Inconnu')}" if response else "OpenWeatherMap: Réponse vide ou erreur interne."

class MockarooClient(APIClient):
    def __init__(self):
        super().__init__("MOCKAROO", endpoint_health_manager)

    async def query(self, count: int = 1, fields_json: Optional[str] = None) -> str:
        """Génère des données de test via Mockaroo."""
        params = {"count": count}
        if fields_json:
            params["fields"] = fields_json
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            return f"Mockaroo (Génération de données):\n{json.dumps(response, indent=2)}"
        return f"Mockaroo: Erreur: {response.get('message', 'Inconnu')}" if response else "Mockaroo: Réponse vide ou erreur interne."

class OpenPageRankClient(APIClient):
    def __init__(self):
        super().__init__("OPENPAGERANK", endpoint_health_manager)

    async def query(self, domains: List[str]) -> str:
        """Récupère le PageRank de domaines via OpenPageRank."""
        params = {"domains[]": domains}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            results = response.get("response", [])
            if results:
                output = "OpenPageRank (Classement de domaine):\n"
                for res in results:
                    output += f"- Domaine: {res.get('domain', 'N/A')}, PageRank: {res.get('page_rank', 'N/A')}\n"
                return output
            return "OpenPageRank: Aucun résultat trouvé."
        return f"OpenPageRank: Erreur: {response.get('message', 'Inconnu')}" if response else "OpenPageRank: Réponse vide ou erreur interne."

class RapidAPIClient(APIClient):
    def __init__(self):
        super().__init__("RAPIDAPI", endpoint_health_manager)

    async def query(self, api_name: str, **kwargs) -> str:
        """
        Interroge diverses APIs disponibles via RapidAPI (blagues, taux de change, faits aléatoires).
        `api_name` spécifie l'API RapidAPI à utiliser.
        """
        selected_endpoint_config = None
        for config in API_CONFIG.get(self.name, []):
            if api_name.lower() in config["endpoint_name"].lower():
                selected_endpoint_config = config
                break

        if not selected_endpoint_config:
            return f"RapidAPI: Endpoint pour '{api_name}' non trouvé ou non configuré."

        url = selected_endpoint_config["url"]
        method = selected_endpoint_config["method"]

        request_params = selected_endpoint_config.get("fixed_params", {}).copy()
        request_headers = selected_endpoint_config.get("fixed_headers", {}).copy()
        request_json_data = selected_endpoint_config.get("fixed_json", {}).copy()

        if method == "GET":
            request_params.update(kwargs)
        elif method == "POST":
            request_json_data.update(kwargs)

        headers = {
            selected_endpoint_config["key_field"]: selected_endpoint_config["key"],
            "X-RapidAPI-Host": selected_endpoint_config["fixed_headers"].get("X-RapidAPI-Host")
        }

        response = await self._make_request(
            params=request_params,
            headers=headers,
            json_data=request_json_data,
            url=url,
            method=method,
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if api_name.lower() == "programming joke":
                return f"RapidAPI (Blague de Programmation): {response.get('setup', '')} - {response.get('punchline', '')}"
            elif api_name.lower() == "currency list quotes":
                return f"RapidAPI (Devises): {json.dumps(response, indent=2)}"
            elif api_name.lower() == "random fact":
                return f"RapidAPI (Fait Aléatoire): {response.get('text', 'N/A')}"
            return f"RapidAPI ({api_name}): {json.dumps(response, indent=2)}"
        return f"RapidAPI ({api_name}): Erreur: {response.get('message', 'Inconnu')}" if response else "RapidAPI: Réponse vide ou erreur interne."

class OCRApiClient:
    def __init__(self):
        from config import OCR_API_KEY
        self.api_key = OCR_API_KEY
        self.base_url = "https://api.ocr.space/parse/image"
        log_message("OCRApiClient initialisé.")

    async def query(self, image_base64: str) -> str:
        """
        Effectue une requête OCR à l'API OCR.space.
        `image_base64` doit être la chaîne base64 de l'image, incluant le préfixe mimeType
        (ex: "data:image/png;base64,...").
        """
        payload = {
            "base64Image": image_base64,
            "language": "fre",
            "isOverlayRequired": False,
            "OCREngine": 2
        }
        headers = {
            "apikey": self.api_key,
            "Content-Type": "application/json"
        }

        log_message("Appel à OCR.space API...")
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(self.base_url, headers=headers, json=payload)
                response.raise_for_status()
                result = response.json()

                if result.get("IsErroredOnProcessing"):
                    error_message = result.get("ErrorMessage", ["Erreur inconnue lors du traitement OCR."])
                    log_message(f"Erreur OCR.space: {error_message}", level="error")
                    return f"❌ Erreur OCR: {', '.join(error_message)}"

                parsed_text = ""
                if "ParsedResults" in result and result["ParsedResults"]:
                    for parsed_result in result["ParsedResults"]:
                        parsed_text += parsed_result.get("ParsedText", "") + "\n"

                if parsed_text.strip():
                    log_message("OCR.space: Texte extrait avec succès.")
                    return parsed_text.strip()
                else:
                    log_message("OCR.space: Aucun texte extrait.", level="warning")
                    return "Aucun texte n'a pu être extrait de l'image."

        except httpx.HTTPStatusError as e:
            log_message(f"Erreur HTTP OCR.space API: {e.response.status_code} - {e.response.text}", level="error")
            return f"❌ Erreur HTTP OCR: {e.response.status_code} - {e.response.text}"
        except httpx.RequestError as e:
            log_message(f"Erreur de requête OCR.space API: {e}", level="error")
            return f"❌ Erreur de requête OCR: {e}"
        except json.JSONDecodeError as e:
            log_message(f"Erreur de décodage JSON OCR.space API: {e} - Réponse brute: {response.text}", level="error")
            return {"error": f"Erreur de décodage JSON OCR: {e}"}
        except Exception as e:
            log_message(f"Erreur inattendue OCR.space API: {e}\n{traceback.format_exc()}", level="error")
            return f"❌ Erreur inattendue OCR: {e}"


# Instancier tous les clients API en leur passant le gestionnaire de santé
# Note: GeminiApiClient et OCRApiClient sont instanciés séparément car ils gèrent leurs clés directement.
ALL_API_CLIENTS = [
    DeepSeekClient(), SerperClient(), WolframAlphaClient(), TavilyClient(),
    ApiFlashClient(), CrawlbaseClient(), DetectLanguageClient(), GuardianClient(),
    IP2LocationClient(), ShodanClient(), WeatherAPIClient(),
    CloudmersiveClient(), GreyNoiseClient(), PulsediveClient(), StormGlassClient(),
    LoginRadiusClient(), JsonbinClient(),
    HuggingFaceClient(), TwilioClient(), AbstractAPIClient(),
    GoogleCustomSearchClient(), RandommerClient(), TomorrowIOClient(),
    OpenWeatherMapClient(),
    MockarooClient(), OpenPageRankClient(), RapidAPIClient()
]

# memory_and_quotas.py
import json
import asyncio
import random
import traceback
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, Any, Optional, List, Union

# Imports des constantes depuis config.py
from config import (
    API_QUOTAS, API_COOLDOWN_DURATION_SECONDS,
    USER_CHAT_HISTORY_FILE, USER_LONG_MEMORY_FILE,
    IA_STATUS_FILE, QUOTAS_FILE, GROUP_CHAT_HISTORY_FILE, PRIVATE_GROUP_ID,
    BURN_QUOTA_THRESHOLD_RATIO, BURN_QUOTA_BEFORE_RESET_HOURS,
    API_ROTATION_INTERVAL_MINUTES # Ajouté pour la récupération de diversification
)

# Imports depuis utils.py
from utils import (
    load_json, save_json, get_current_time, format_datetime, log_message,
    neutralize_urls, extract_keywords, tag_conversation, unique_preserve_order,
    similar, get_user_dir
)

class MemoryManager:
    """
    Gère la mémoire à court et long terme du bot, ainsi que l'historique des conversations
    pour les utilisateurs individuels et les groupes.
    C'est un singleton pour s'assurer qu'il n'y a qu'une seule instance de gestionnaire de mémoire.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Implémente le patron de conception Singleton."""
        if cls._instance is None:
            cls._instance = super(cls, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """Initialise les structures de données pour la mémoire."""
        if self._initialized:
            return
        self.chat_history: Dict[Union[int, str], List[Dict]] = {} # {user_id: [messages]}
        self.long_term_memory: Dict[Union[int, str], List[str]] = {} # {user_id: [facts]}
        self.ia_status: Dict[str, Dict[str, Any]] = {} # Statut global des IA
        self.group_chat_history: Dict[int, List[Dict]] = {} # {group_id: [messages]}
        # `_initialized` est géré par `init_manager` pour les opérations asynchrones

    async def init_manager(self):
        """
        Initialise le gestionnaire de mémoire de manière asynchrone.
        Charge les statuts persistants et les historiques de groupe.
        """
        if not self._initialized:
            # Charge le statut global des IA
            self.ia_status = await load_json(IA_STATUS_FILE, {})
            self._initialize_ia_status()

            # Charge l'historique du groupe privé spécifique
            group_dir = get_user_dir(PRIVATE_GROUP_ID)
            self.group_chat_history[PRIVATE_GROUP_ID] = await load_json(group_dir / GROUP_CHAT_HISTORY_FILE, [])

            self._initialized = True
            log_message("Gestionnaire de mémoire initialisé.")

    def _initialize_ia_status(self):
        """
        Initialise ou met à jour le statut des IA si elles ne sont pas déjà présentes
        ou si leur statut est obsolète. S'assure que toutes les clés nécessaires sont là.
        """
        updated = False
        now = get_current_time()

        default_ia_status_keys = {
            "last_used": None, "last_error": None, "error_count": 0,
            "cooldown_until": None, "success_count": 0, "current_score": 1.0,
            "last_rotation_check": format_datetime(now), "diversification_score": 1.0
        }

        for client_name in API_QUOTAS.keys(): # Itère sur toutes les APIs définies dans les quotas
            if client_name not in self.ia_status:
                self.ia_status[client_name] = default_ia_status_keys.copy()
                updated = True
            else:
                for key, default_value in default_ia_status_keys.items():
                    if key not in self.ia_status[client_name]:
                        self.ia_status[client_name][key] = default_value
                        updated = True

                # Met à jour `last_rotation_check` si trop ancien pour permettre la récupération du score de diversification
                last_check_str = self.ia_status[client_name].get("last_rotation_check")
                if last_check_str:
                    try:
                        last_check_dt = datetime.strptime(last_check_str, "%Y-%m-%d %H:%M:%S UTC")
                        if (now - last_check_dt).total_seconds() > API_ROTATION_INTERVAL_MINUTES * 60 * 2:
                            self.ia_status[client_name]["last_rotation_check"] = format_datetime(now)
                            updated = True
                    except ValueError:
                        self.ia_status[client_name]["last_rotation_check"] = format_datetime(now)
                        updated = True
                        log_message(f"last_rotation_check malformé pour {client_name}, réinitialisation.", level="warning")

        # Supprime les noms d'IA qui ne sont plus définis dans `API_QUOTAS`
        current_api_names = set(API_QUOTAS.keys())
        ia_names_to_remove = [name for name in self.ia_status if name not in current_api_names]
        for name in ia_names_to_remove:
            del self.ia_status[name]
            updated = True
            log_message(f"IA '{name}' trouvée dans ia_status.json mais non définie dans API_QUOTAS. Supprimée.", level="warning")

        if updated:
            asyncio.create_task(save_json(IA_STATUS_FILE, self.ia_status))
            log_message("Statut des IA initialisé/mis à jour.")

    async def _update_and_save_history(self, user_id: Union[int, str], file_path: Path, history_dict: Dict, max_entries: int, entry: Dict):
        """Helper to update history and save."""
        hist = history_dict.get(user_id, await load_json(file_path, []))
        hist.append(entry)
        hist = hist[-max_entries:]
        history_dict[user_id] = hist
        asyncio.create_task(save_json(file_path, hist))

    async def add_message_to_history(self, user_id: Union[int, str], role: str, content: str, max_log_entries: int = 100):
        """
        Ajoute un message à l'historique de la conversation d'un utilisateur.
        Gère également le log général de l'utilisateur et le taggage des messages.
        """
        user_dir = get_user_dir(user_id)
        chat_history_path = user_dir / USER_CHAT_HISTORY_FILE
        log_path = user_dir / "log.json"

        neutralized_content = neutralize_urls(content)
        timestamp = format_datetime(get_current_time())

        # Add to user chat history
        await self._update_and_save_history(
            user_id, chat_history_path, self.chat_history, max_log_entries,
            {"role": role, "content": neutralized_content, "timestamp": timestamp}
        )

        # Add to general user log
        log_entry_content = neutralized_content[:500]
        log_entry = {"time": timestamp, "role": role, "text": log_entry_content}
        if role == "user":
            log_entry["tags"] = tag_conversation(content)
        
        user_log = await load_json(log_path, [])
        await self._update_and_save_history(
            user_id, log_path, {user_id: user_log}, max_log_entries, log_entry
        ) # Pass user_log as a dict for consistency with _update_and_save_history

        log_message(f"Message ajouté à l'historique de {user_id} par {role}.")

    async def get_chat_history(self, user_id: Union[int, str], limit: int = 10) -> List[Dict]:
        """
        Retourne les N derniers messages de l'historique de conversation d'un utilisateur.
        Charge l'historique depuis le fichier si non déjà en mémoire.
        """
        user_dir = get_user_dir(user_id)
        chat_history_path = user_dir / USER_CHAT_HISTORY_FILE
        if user_id not in self.chat_history:
            self.chat_history[user_id] = await load_json(chat_history_path, [])
        return self.chat_history[user_id][-limit:]

    async def save_group_memory(self, group_id: int, role: str, text: str, max_items: int = 1000):
        """
        Sauvegarde l'historique de chat pour un groupe spécifique.
        Utilise l'ID du groupe comme un ID utilisateur pour la structure de répertoire.
        """
        group_dir = get_user_dir(group_id)
        group_history_path = group_dir / GROUP_CHAT_HISTORY_FILE

        hist = self.group_chat_history.get(group_id, await load_json(group_history_path, []))
        hist.append({"time": format_datetime(get_current_time()), "role": role, "text": neutralize_urls(text)})
        hist = hist[-max_items:]
        self.group_chat_history[group_id] = hist
        asyncio.create_task(save_json(group_history_path, hist))
        log_message(f"Message ajouté à la mémoire de groupe {group_id} par {role}.")

    async def get_group_memory(self, group_id: int, limit: int = 20) -> str:
        """
        Récupère les N derniers messages de la mémoire de groupe.
        Retourne une chaîne formatée pour être utilisée comme contexte.
        """
        group_dir = get_user_dir(group_id)
        group_history_path = group_dir / GROUP_CHAT_HISTORY_FILE
        if group_id not in self.group_chat_history:
            self.group_chat_history[group_id] = await load_json(group_history_path, [])

        if not isinstance(self.group_chat_history[group_id], list):
            self.group_chat_history[group_id] = []

        recent_messages = [f"{l['role']} : {l['text']}" for l in self.group_chat_history[group_id][-limit:] if l.get("role") != "bot"]
        return "\n".join(recent_messages)

    async def add_to_long_term_memory(self, user_id: Union[int, str], text: str, max_entries: int = 100):
        """
        Ajoute une information à la mémoire à long terme d'un utilisateur.
        Dédoublonne et tronque la mémoire.
        """
        user_dir = get_user_dir(user_id)
        long_memory_path = user_dir / USER_LONG_MEMORY_FILE

        long_mem = self.long_term_memory.get(user_id, await load_json(long_memory_path, []))
        if not isinstance(long_mem, list):
            long_mem = []

        long_mem.append(text.strip())
        long_mem = unique_preserve_order(long_mem)[-max_entries:]
        self.long_term_memory[user_id] = long_mem
        asyncio.create_task(save_json(long_memory_path, long_mem))
        log_message(f"Information ajoutée à la mémoire à long terme de {user_id}.")

    async def get_long_term_memory(self, user_id: Union[int, str], limit: int = 20) -> str:
        """
        Récupère les N dernières entrées de la mémoire à long terme d'un utilisateur.
        Retourne une chaîne formatée.
        """
        user_dir = get_user_dir(user_id)
        long_memory_path = user_dir / USER_LONG_MEMORY_FILE
        if user_id not in self.long_term_memory:
            self.long_term_memory[user_id] = await load_json(long_memory_path, [])

        if not isinstance(self.long_term_memory[user_id], list):
            self.long_term_memory[user_id] = []

        return "\n".join(self.long_term_memory[user_id][-limit:])

    async def check_for_similar_prompt(self, user_id: Union[int, str], prompt: str) -> Optional[str]:
        """
        Vérifie si un prompt similaire a déjà été posé récemment et retourne la réponse si trouvée.
        Utilise `MAX_CACHE_SIZE` pour la fenêtre de recherche.
        """
        recent_chat_history = await self.get_chat_history(user_id, limit=MAX_CACHE_SIZE)
        for i in range(len(recent_chat_history) - 1, 0, -1): # Iterate backwards from second to last
            entry = recent_chat_history[i]
            if entry.get("role") == "user" and "content" in entry:
                if similar(prompt, entry["content"]) > 0.92:
                    # Check the next entry for a bot response
                    if i + 1 < len(recent_chat_history) and recent_chat_history[i+1].get("role") == "bot":
                        log_message(f"Prompt similaire détecté pour {user_id}. Réponse en cache utilisée.")
                        return recent_chat_history[i+1]["content"]
        return None

    def update_ia_status(self, ia_name: str, success: bool, error_message: Optional[str] = None):
        """
        Met à jour le statut et le score d'une IA après une utilisation.
        Ajuste le score de performance et le score de diversification.
        """
        status = self.ia_status.get(ia_name)
        if not status:
            log_message(f"Tentative de mise à jour d'un statut d'IA inconnu: {ia_name}", level="warning")
            return

        now = get_current_time()
        status["last_used"] = format_datetime(now)

        if success:
            status["success_count"] += 1
            status["error_count"] = 0
            status["cooldown_until"] = None
            status["last_error"] = None
            status["current_score"] = min(1.0, status["current_score"] + 0.1)
            status["diversification_score"] = max(0.1, status["diversification_score"] - 0.1)
            log_message(f"IA {ia_name} : Succès enregistré. Nouveau score: {status['current_score']:.2f}, Diversification: {status['diversification_score']:.2f}")
        else:
            status["error_count"] += 1
            status["last_error"] = error_message
            if status["error_count"] >= 3:
                status["cooldown_until"] = format_datetime(now + timedelta(seconds=API_COOLDOWN_DURATION_SECONDS))
                status["current_score"] = max(0.1, status["current_score"] - 0.2)
                log_message(f"IA {ia_name} : Trop d'erreurs ({status['error_count']}). Cooldown jusqu'à {status['cooldown_until']}. Nouveau score: {status['current_score']:.2f}", level="warning")
            else:
                 status["current_score"] = max(0.1, status["current_score"] - 0.05)
                 log_message(f"IA {ia_name} : Erreur enregistrée. Nouveau score: {status['current_score']:.2f}", level="warning")

        asyncio.create_task(save_json(IA_STATUS_FILE, self.ia_status))

    def recover_diversification_scores(self):
        """
        Augmente le score de diversification pour les IA qui n'ont pas été utilisées récemment.
        Ceci encourage la rotation des APIs.
        """
        now = get_current_time()
        updated = False
        for ia_name, status in self.ia_status.items():
            last_used_str = status.get("last_used")
            if last_used_str:
                try:
                    last_used_dt = datetime.strptime(last_used_str, "%Y-%m-%d %H:%M:%S UTC")
                    if (now - last_used_dt).total_seconds() > API_ROTATION_INTERVAL_MINUTES * 60 * 2:
                        if status["diversification_score"] < 1.0:
                            status["diversification_score"] = min(1.0, status["diversification_score"] + 0.05)
                            updated = True
                            log_message(f"IA {ia_name}: Score de diversification récupéré à {status['diversification_score']:.2f}")
                except ValueError:
                    status["last_used"] = format_datetime(now)
                    status["diversification_score"] = 1.0
                    updated = True
                    log_message(f"last_used malformé pour {ia_name}, réinitialisation du score de diversification.", level="warning")
            else:
                if status["diversification_score"] < 1.0:
                    status["diversification_score"] = 1.0
                    updated = True
        if updated:
            asyncio.create_task(save_json(IA_STATUS_FILE, self.ia_status))

    def get_ia_status(self, ia_name: str) -> Optional[Dict]:
        """Récupère le statut d'une IA spécifique."""
        return self.ia_status.get(ia_name)

    def get_available_ias(self) -> List[str]:
        """
        Retourne les noms des IA actuellement non en cooldown.
        """
        available = []
        now = get_current_time()
        for name, status in self.ia_status.items():
            cooldown_until_str = status.get("cooldown_until")
            if cooldown_until_str:
                try:
                    cooldown_until = datetime.strptime(cooldown_until_str, "%Y-%m-%d %H:%M:%S UTC")
                    if now < cooldown_until:
                        continue
                except ValueError:
                    log_message(f"cooldown_until malformé pour {name}, considéré comme non en cooldown.", level="warning")
            available.append(name)
        return available

class QuotaManager:
    """
    Gère les quotas d'utilisation pour toutes les APIs.
    Suit l'utilisation mensuelle, journalière et horaire, et peut alerter en cas de dépassement.
    Prend également en charge le mode "brûlage" de quota.
    C'est un singleton.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Implémente le patron de conception Singleton."""
        if cls._instance is None:
            cls._instance = super(cls, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """Initialise les structures de données pour les quotas."""
        if self._initialized:
            return
        self.quotas = {}
        # `_initialized` est géré par `init_manager` pour les opérations asynchrones
        self.bot_instance = None # Sera injecté par main.py pour l'envoi d'alertes

    async def init_manager(self):
        """
        Initialise le gestionnaire de quotas de manière asynchrone.
        Charge les données de quotas persistantes et s'assure qu'elles sont à jour.
        """
        if not self._initialized:
            self.quotas = await load_json(QUOTAS_FILE, {})
            self._initialize_quotas()
            self._initialized = True
            log_message("Gestionnaire de quotas initialisé.")

    def set_bot_instance(self, bot_instance: Any):
        """
        Permet d'injecter l'instance du bot pour envoyer des alertes de quota au groupe privé.
        """
        self.bot_instance = bot_instance

    def _initialize_quotas(self):
        """
        Initialise les quotas pour toutes les APIs basées sur `config.API_QUOTAS`.
        Nettoie et met à jour les entrées existantes si nécessaire.
        """
        updated = False
        now = get_current_time()

        default_quota_structure = {
            "monthly_usage": 0, "daily_usage": 0, "hourly_usage": 0,
            "hourly_timestamps": [], "last_reset_month": now.month,
            "last_reset_day": now.day, "last_usage": None,
            "total_calls": 0, "last_hourly_reset": format_datetime(now)
        }

        for api_name, quota_info in API_QUOTAS.items():
            if api_name not in self.quotas:
                self.quotas[api_name] = default_quota_structure.copy()
                updated = True
            else:
                for key, default_value in default_quota_structure.items():
                    if key not in self.quotas[api_name]:
                        self.quotas[api_name][key] = default_value
                        updated = True

                if not isinstance(self.quotas[api_name].get("hourly_timestamps"), list):
                    self.quotas[api_name]["hourly_timestamps"] = []

                one_hour_ago = now - timedelta(hours=1)
                self.quotas[api_name]["hourly_timestamps"] = [
                    ts for ts in self.quotas[api_name]["hourly_timestamps"]
                    if datetime.strptime(ts, "%Y-%m-%d %H:%M:%S UTC").replace(tzinfo=timezone.utc) > one_hour_ago
                ]
                self.quotas[api_name]["hourly_usage"] = len(self.quotas[api_name]["hourly_timestamps"])
                self.quotas[api_name]["last_hourly_reset"] = format_datetime(now)
                updated = True

        api_names_to_remove = [name for name in self.quotas if name not in API_QUOTAS]
        for name in api_names_to_remove:
            del self.quotas[name]
            updated = True
            log_message(f"API '{name}' trouvée dans quotas.json mais non définie dans API_QUOTAS. Supprimée.", level="warning")

        if updated:
            asyncio.create_task(save_json(QUOTAS_FILE, self.quotas))
            log_message("Quotas API initialisés/mis à jour.")

    def _reset_quotas_if_needed(self):
        """
        Réinitialise les quotas journaliers, mensuels et horaires si nécessaire.
        Cette méthode est appelée avant chaque vérification de quota pour s'assurer de l'actualité.
        """
        now = get_current_time()
        for api_name, data in self.quotas.items():
            if now.month != data["last_reset_month"]:
                data["monthly_usage"] = 0
                data["last_reset_month"] = now.month
                log_message(f"Quota mensuel pour {api_name} réinitialisé.")
            if now.day != data["last_reset_day"]:
                data["daily_usage"] = 0
                data["last_reset_day"] = now.day
                log_message(f"Quota journalier pour {api_name} réinitialisé.")

            one_hour_ago = now - timedelta(hours=1)
            if not isinstance(data.get("hourly_timestamps"), list):
                data["hourly_timestamps"] = []

            data["hourly_timestamps"] = [
                ts for ts in data["hourly_timestamps"]
                if datetime.strptime(ts, "%Y-%m-%d %H:%M:%S UTC").replace(tzinfo=timezone.utc) > one_hour_ago
            ]
            data["hourly_usage"] = len(data["hourly_timestamps"])
            data["last_hourly_reset"] = format_datetime(now)

        asyncio.create_task(save_json(QUOTAS_FILE, self.quotas))

    async def check_and_update_quota(self, api_name: str, cost: int = 1) -> bool:
        """
        Vérifie si une API a du quota disponible et le décrémente si oui.
        Retourne `True` si l'opération est autorisée (quota disponible), `False` sinon.
        """
        self._reset_quotas_if_needed()

        if api_name not in API_QUOTAS:
            log_message(f"Tentative de vérification de quota pour une API non définie: {api_name}. Autorisation refusée.", level="error")
            return False

        if api_name not in self.quotas:
            log_message(f"API {api_name} non trouvée dans les quotas gérés. Re-initialisation non bloquante.", level="warning")
            self._initialize_quotas()
            if api_name not in self.quotas:
                return False

        quota_data = self.quotas[api_name]
        api_limits = API_QUOTAS.get(api_name, {})
        now = get_current_time()

        limits = {
            "monthly": api_limits.get("monthly"),
            "daily": api_limits.get("daily"),
            "hourly": api_limits.get("hourly")
        }
        usages = {
            "monthly": quota_data["monthly_usage"],
            "daily": quota_data["daily_usage"],
            "hourly": quota_data["hourly_usage"]
        }

        for limit_type, limit_value in limits.items():
            if limit_value is not None and (usages[limit_type] + cost) > limit_value:
                log_message(f"Quota {limit_type} dépassé pour {api_name}", level="warning")
                await self._alert_quota_if_needed(api_name, limit_type)
                return False

        rate_limit_per_sec = api_limits.get("rate_limit_per_sec")
        if rate_limit_per_sec:
            last_usage_str = quota_data.get("last_usage")
            if last_usage_str:
                try:
                    last_usage = datetime.strptime(last_usage_str, "%Y-%m-%d %H:%M:%S UTC").replace(tzinfo=timezone.utc)
                    time_since_last_call = (now - last_usage).total_seconds()
                    if time_since_last_call < (1 / rate_limit_per_sec):
                        log_message(f"Taux de requêtes dépassé pour {api_name}. Attendre {1/rate_limit_per_sec - time_since_last_call:.2f}s", level="warning")
                        return False
                except ValueError:
                    log_message(f"last_usage malformé pour {api_name}, considéré comme sans utilisation récente pour la limite de taux.", level="warning")

        if cost > 0:
            quota_data["monthly_usage"] += cost
            quota_data["daily_usage"] += cost
            quota_data["hourly_usage"] += cost
            quota_data["hourly_timestamps"].append(format_datetime(now))
            quota_data["total_calls"] += cost
            quota_data["last_usage"] = format_datetime(now)
            asyncio.create_task(save_json(QUOTAS_FILE, self.quotas))
            log_message(f"Quota pour {api_name} mis à jour. Usage mensuel: {quota_data['monthly_usage']}/{limits['monthly'] if limits['monthly'] else 'Illimité'}, Journalier: {quota_data['daily_usage']}/{limits['daily'] if limits['daily'] else 'Illimité'}, Horaire: {quota_data['hourly_usage']}/{limits['hourly'] if limits['hourly'] else 'Illimité'}")
        else:
            log_message(f"Quota pour {api_name} vérifié (coût 0). Usage mensuel: {quota_data['monthly_usage']}/{limits['monthly'] if limits['monthly'] else 'Illimité'}, Journalier: {quota_data['daily_usage']}/{limits['daily'] if limits['daily'] else 'Illimité'}, Horaire: {quota_data['hourly_usage']}/{limits['hourly'] if limits['hourly'] else 'Illimité'}")

        return True

    async def _alert_quota_if_needed(self, api_name: str, limit_type: str):
        """
        Envoie une alerte au groupe privé si un quota est atteint.
        Utilise la mémoire à long terme pour éviter de spammer les alertes.
        """
        if self.bot_instance and PRIVATE_GROUP_ID:
            message = f"🚨 Quota {limit_type} pour l'API '{api_name}' atteint !"
            log_message(message, level="warning")
            try:
                alert_key = f"quota_alert_{api_name}_{limit_type}_{get_current_time().strftime('%Y-%m-%d')}"
                bot_global_memory_id = "bot_global_alerts"
                
                global_alerts = await memory_manager.get_long_term_memory(bot_global_memory_id, limit=1000)
                if alert_key not in global_alerts:
                    await self.bot_instance.send_message(chat_id=PRIVATE_GROUP_ID, text=message)
                    await memory_manager.add_to_long_term_memory(bot_global_memory_id, alert_key)
            except Exception as e:
                log_message(f"Erreur lors de l'envoi de l'alerte de quota: {e}", level="error")

    def get_api_usage(self, api_name: str) -> Optional[Dict]:
        """Retourne les informations d'utilisation d'une API spécifique."""
        return self.quotas.get(api_name)

    def get_all_quotas_status(self) -> Dict:
        """
        Retourne le statut de tous les quotas API.
        S'assure que les quotas sont à jour avant de les retourner.
        """
        self._reset_quotas_if_needed()
        status = {}
        for api_name, quota_data in self.quotas.items():
            api_limits = API_QUOTAS.get(api_name, {})
            monthly_limit = api_limits.get("monthly", "Illimité")
            daily_limit = api_limits.get("daily", "Illimité")
            hourly_limit = api_limits.get("hourly", "Illimité")
            status[api_name] = {
                "monthly_usage": quota_data["monthly_usage"],
                "monthly_limit": monthly_limit,
                "daily_usage": quota_data["daily_usage"],
                "daily_limit": daily_limit,
                "hourly_usage": quota_data["hourly_usage"],
                "hourly_limit": hourly_limit,
                "total_calls": quota_data["total_calls"],
                "last_usage": quota_data["last_usage"],
                "last_reset_month": quota_data["last_reset_month"],
                "last_reset_day": quota_data["last_reset_day"],
                "last_hourly_reset": quota_data["last_hourly_reset"]
            }
        return status

    def get_burn_window_apis(self) -> List[str]:
        """
        Identifie les APIs dont les quotas sont sur le point d'être réinitialisés
        et où il est opportun de "brûler" le quota restant.
        """
        burn_apis = []
        now = get_current_time()

        for api_name, data in self.quotas.items():
            api_limits = API_QUOTAS.get(api_name, {})

            for limit_type in ["monthly", "daily"]:
                limit = api_limits.get(limit_type)
                if limit is not None and limit > 0:
                    current_usage = data[f"{limit_type}_usage"]
                    
                    if limit_type == "monthly":
                        next_month = now.month + 1
                        year_for_next_month = now.year
                        if next_month > 12:
                            next_month = 1
                            year_for_next_month += 1
                        reset_time = datetime(year_for_next_month, next_month, 1, 0, 0, 0, 0, tzinfo=timezone.utc)
                    else: # daily
                        reset_time = datetime(now.year, now.month, now.day, 0, 0, 0, 0, tzinfo=timezone.utc) + timedelta(days=1)
                    
                    if now < reset_time:
                        time_until_reset = reset_time - now
                        if time_until_reset <= timedelta(hours=BURN_QUOTA_BEFORE_RESET_HOURS):
                            if current_usage < limit:
                                remaining_quota_ratio = (limit - current_usage) / limit
                                if remaining_quota_ratio > 0 and remaining_quota_ratio <= BURN_QUOTA_THRESHOLD_RATIO:
                                    burn_apis.append(f"{api_name} ({limit_type}: {limit - current_usage} restants)")
        return burn_apis


    def should_burn_quota(self, api_name: str) -> bool:
        """
        Détermine si le quota pour une API donnée devrait être 'brûlé' avant réinitialisation.
        Cette méthode est appelée par `_auto_burn_quota_task` pour décider si une API doit être utilisée.
        """
        self._reset_quotas_if_needed()

        quota_data = self.quotas.get(api_name)
        api_limits = API_QUOTAS.get(api_name)

        if not quota_data or not api_limits:
            return False

        now = get_current_time()

        for limit_type in ["monthly", "daily"]:
            limit = api_limits.get(limit_type)
            if limit is not None and limit > 0:
                current_usage = quota_data[f"{limit_type}_usage"]

                if limit_type == "monthly":
                    next_month = now.month + 1
                    year_for_next_month = now.year
                    if next_month > 12:
                        next_month = 1
                        year_for_next_month += 1
                    reset_time = datetime(year_for_next_month, next_month, 1, 0, 0, 0, 0, tzinfo=timezone.utc)
                else: # daily
                    reset_time = datetime(now.year, now.month, now.day, 0, 0, 0, 0, tzinfo=timezone.utc) + timedelta(days=1)

                if now < reset_time:
                    time_until_reset = reset_time - now
                    if time_until_reset <= timedelta(hours=BURN_QUOTA_BEFORE_RESET_HOURS):
                        remaining_quota_ratio = (limit - current_usage) / limit
                        if remaining_quota_ratio > 0 and remaining_quota_ratio <= BURN_QUOTA_THRESHOLD_RATIO:
                            log_message(f"Mode Burn: {api_name} ({limit_type}) - usage {current_usage}/{limit}, ratio {remaining_quota_ratio:.2f}, reset dans {time_until_reset}", level="debug")
                            return True
        return False

# Instanciation des gestionnaires de mémoire et de quotas (seront initialisés dans main.py)
memory_manager = MemoryManager()
quota_manager = QuotaManager()

# filters_and_tools.py
import json
import asyncio
import random
import ast
import subprocess
import base64
import httpx
import io
import contextlib
import traceback
import hashlib
import difflib
import re
import logging
from datetime import datetime, timedelta, timezone
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, Any, Optional, List, Union, Tuple

# Imports des constantes depuis config.py
from config import FORBIDDEN_WORDS, ARCHIVES_DIR, MAX_FILE_SIZE

# Imports depuis utils.py
from utils import (
    log_message, neutralize_urls, get_user_dir
)

# Imports spécifiques pour les outils de développement (assurez-vous que ces bibliothèques sont installées)
try:
    from pygments import highlight
    from pygments.lexers import PythonLexer
    from pygments.formatters import TerminalFormatter
except ImportError:
    highlight = None
    PythonLexer = None
    TerminalFormatter = None
    log_message("Pygments non trouvé. La surbrillance syntaxique ne sera pas disponible.", level="warning")

try:
    from pyflakes.api import check
    from pyflakes.reporter import Reporter
except ImportError:
    check = None
    Reporter = None
    log_message("Pyflakes non trouvé. La vérification de code ne sera pas disponible.", level="warning")

try:
    import black
except ImportError:
    black = None
    log_message("Black non trouvé. Le formatage de code ne sera pas disponible.", level="warning")

# Instancier le client OCR API pour une utilisation directe dans perform_ocr_api
# Note: OCRApiClient est défini dans api_clients.py, donc il doit être importé.
# Pour éviter une dépendance circulaire si ce fichier est importé avant api_clients,
# l'instanciation est faite ici, mais la classe OCRApiClient doit être disponible.
# Dans le script final, toutes les classes sont dans le même fichier ou l'ordre d'importation est géré.
# On va passer l'instance du client OCR via une fonction setter ou directement lors de l'initialisation du bot.
# Pour l'instant, on déclare une variable globale qui sera assignée plus tard.
ocr_api_client_instance = None # Sera assigné par main.py

def set_ocr_api_client_instance(client_instance):
    global ocr_api_client_instance
    ocr_api_client_instance = client_instance

# Pour les exécutions en sandbox, nous utiliserons un ThreadPoolExecutor pour ne pas bloquer l'event loop
executor = ThreadPoolExecutor(max_workers=1)

def filter_bad_code(code: str) -> bool:
    """
    Filtre basique pour les commandes de code potentiellement dangereuses.
    Empêche l'exécution de certaines opérations système ou de réseau.
    """
    forbidden_patterns = [
        r'os\.system', r'subprocess\.run', r'shutil\.rmtree', r'requests\.post',
        r'open\([^,\'"]*\s*,\s*[\'"]w', r'import socket', r'import http',
        r'sys\.exit', r'while True:', r'import threading', r'import multiprocessing'
    ]
    for pattern in forbidden_patterns:
        if re.search(pattern, code):
            log_message(f"Code bloqué: motif dangereux détecté: {pattern}", level="warning")
            return True
    return False

def detect_and_correct_toxicity(text: str) -> str:
    """
    Remplace les propos toxiques (mots définis dans FORBIDDEN_WORDS)
    par des faits scientifiques ou des encouragements.
    """
    if any(word in text.lower() for word in FORBIDDEN_WORDS):
        facts = [
            "Savais-tu que 73% des conflits viennent de malentendus ?",
            "Le cerveau humain est câblé pour la coopération, pas le conflit.",
            "En 2025, l'IA émotionnelle sera la norme. Soyons précurseurs !",
            "Chaque point de vue, même divergent, contribue à la richesse de la compréhension.",
            "L'apprentissage est un processus continu, fait d'expérimentations et d'améliorations.",
            "La collaboration est la clé de l'innovation."
        ]
        log_message(f"Toxicité détectée et corrigée dans le message: '{text}'", level="info")
        return random.choice(facts) + " Continuons à construire ensemble !"
    return text

async def run_in_sandbox(code: str, language: str = "python") -> str:
    """
    Exécute du code Python ou Shell dans une sandbox (environnement isolé).
    Utilise un ThreadPoolExecutor pour exécuter des opérations bloquantes de manière asynchrone,
    évitant ainsi de bloquer l'event loop principal.
    """
    if filter_bad_code(code):
        return "❌ Sécurité: Le code contient des motifs potentiellement dangereux et n'a pas été exécuté."

    loop = asyncio.get_running_loop()
    if language == "python":
        return await loop.run_in_executor(executor, _run_python_sync, code)
    elif language == "shell":
        return await loop.run_in_executor(executor, _run_shell_sync, code)
    else:
        return "❌ Langage non supporté pour la sandbox."

def _run_python_sync(code: str) -> str:
    """
    Exécute du code Python de manière synchrone et capture la sortie standard et d'erreur.
    Utilise un dictionnaire `__builtins__` vide pour isoler l'exécution.
    """
    old_stdout = io.StringIO()
    old_stderr = io.StringIO()
    with contextlib.redirect_stdout(old_stdout), contextlib.redirect_stderr(old_stderr):
        try:
            exec(code, {'__builtins__': {}})
            output = old_stdout.getvalue()
            error = old_stderr.getvalue()
            if error:
                log_message(f"Erreur d'exécution Python en sandbox:\n{error}", level="warning")
                return f"🐍 Erreur Python:\n{error}\nSortie:\n{output}"
            return f"✅ Sortie Python:\n{output}"
        except Exception as e:
            log_message(f"Erreur d'exécution Python inattendue en sandbox: {e}\n{traceback.format_exc()}", level="error")
            return f"❌ Erreur d'exécution Python: {e}\nSortie standard:\n{old_stdout.getvalue()}\nErreur standard:\n{old_stderr.getvalue()}"

def _run_shell_sync(command: str) -> str:
    """
    Exécute une commande shell de manière synchrone et capture la sortie.
    Utilise `subprocess.run` avec un timeout pour éviter les blocages.
    """
    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            check=True,
            timeout=10
        )
        output = result.stdout
        error = result.stderr
        if error:
            log_message(f"Erreur d'exécution Shell en sandbox:\n{error}", level="warning")
            return f"🐚 Erreur Shell:\n{error}\nSortie:\n{output}"
        return f"✅ Sortie Shell:\n{output}"
    except subprocess.CalledProcessError as e:
        log_message(f"Erreur d'exécution Shell (Code: {e.returncode}):\n{e.stderr}\nSortie:\n{e.stdout}", level="error")
        return f"❌ Erreur d'exécution Shell (Code: {e.returncode}):\n{e.stderr}\nSortie:\n{e.stdout}"
    except subprocess.TimeoutExpired:
        log_message("Erreur Shell: La commande a dépassé le temps d'exécution imparti.", level="warning")
        return "❌ Erreur Shell: La commande a dépassé le temps d'exécution imparti."
    except Exception as e:
        log_message(f"Erreur inattendue lors de l'exécution Shell: {e}\n{traceback.format_exc()}", level="error")
        return f"❌ Erreur inattendue lors de l'exécution Shell: {e}"

def syntax_highlight(code: str) -> str:
    """
    Met en surbrillance la syntaxe du code Python en utilisant Pygments.
    Retourne le code formaté pour l'affichage en console ou dans un bloc `<pre>`.
    """
    if highlight and PythonLexer and TerminalFormatter:
        try:
            return highlight(code, PythonLexer(), TerminalFormatter())
        except Exception as e:
            log_message(f"Erreur de surbrillance syntaxique: {e}", level="error")
            return code
    else:
        return "❌ Outil de surbrillance syntaxique (Pygments) non disponible."

def check_code(code: str) -> str:
    """
    Vérifie le code Python avec Pyflakes pour détecter les erreurs de syntaxe et les problèmes de style.
    """
    if check and Reporter:
        out = io.StringIO()
        reporter = Reporter(out, out)
        check(code, filename="<string>", reporter=reporter)
        result = out.getvalue()
        return result if result else "✅ Pyflakes: Aucun problème détecté."
    else:
        return "❌ Outil de vérification de code (Pyflakes) non disponible."

def format_code(code: str) -> str:
    """
    Formate le code Python avec Black pour assurer une cohérence stylistique.
    """
    if black:
        try:
            mode = black.Mode()
            return black.format_str(code, mode=mode)
        except black.InvalidInput as e:
            log_message(f"Erreur de formatage (Black): Code Python invalide. {e}", level="error")
            return f"❌ Erreur de formatage (Black): Code Python invalide. {e}"
        except Exception as e:
            log_message(f"Erreur de formatage (Black): {e}", level="error")
            return f"❌ Erreur de formatage (Black): {e}"
    else:
        return "❌ Outil de formatage de code (Black) non disponible."

def extract_functions(code: str) -> Union[List[str], str]:
    """
    Extrait les noms des fonctions définies dans un code Python en utilisant l'AST.
    """
    try:
        tree = ast.parse(code)
        functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        return functions if functions else "Aucune fonction détectée."
    except SyntaxError as e:
        log_message(f"Erreur de syntaxe Python lors de l'extraction des fonctions: {e}", level="error")
        return f"❌ Erreur de syntaxe Python: {e}"
    except Exception as e:
        log_message(f"Erreur lors de l'extraction des fonctions: {e}", level="error")
        return f"❌ Erreur lors de l'extraction des fonctions: {e}"

def analyze_code_structure(code: str) -> str:
    """
    Analyse la structure AST (Arbre Syntaxique Abstrait) d'un code Python.
    Utile pour comprendre la composition du code.
    """
    try:
        tree = ast.parse(code)
        return ast.dump(tree, indent=2)
    except SyntaxError as e:
        log_message(f"Erreur de syntaxe Python lors de l'analyse AST: {e}", level="error")
        return f"❌ Erreur de syntaxe Python: {e}"
    except Exception as e:
        log_message(f"Erreur lors de l'analyse de la structure AST: {e}", level="error")
        return f"❌ Erreur lors de l'analyse de la structure AST: {e}"

async def perform_ocr_api(image_url: str) -> str:
    """
    Effectue l'OCR sur une URL d'image en utilisant l'API OCR.space via `OCRApiClient`.
    Télécharge l'image, l'encode en base64, puis envoie à l'API.
    """
    if ocr_api_client_instance is None:
        return "❌ L'instance du client OCR n'est pas initialisée."

    try:
        async with httpx.AsyncClient(timeout=10) as client:
            img_response = await client.get(image_url)
            img_response.raise_for_status()

        base64_image_data = base64.b64encode(img_response.content).decode('utf-8')
        content_type = img_response.headers.get("Content-Type", "image/jpeg")
        image_base64_with_prefix = f"data:{content_type};base64,{base64_image_data}"

        result = await ocr_api_client_instance.query(image_base64_with_prefix)
        return result

    except httpx.HTTPStatusError as e:
        log_message(f"Erreur HTTP/réseau lors de l'OCR: {e.response.status_code} - {e.response.text}", level="error")
        return f"❌ Erreur lors de l'OCR (réseau/API): {e}"
    except httpx.RequestError as e:
        log_message(f"Erreur de requête lors de l'OCR: {e}", level="error")
        return f"❌ Erreur lors de l'OCR (requête): {e}"
    except Exception as e:
        log_message(f"Erreur inattendue lors de l'OCR: {e}\n{traceback.format_exc()}", level="error")
        return f"❌ Erreur inattendue lors de l'OCR: {e}"

async def fetch_and_archive_pages(links: List[str], user_id: Union[int, str], bot_instance: Any):
    """
    Télécharge toutes les pages des liens fournis, les archive localement,
    puis les envoie au groupe privé Telegram.
    """
    user_archive_dir = get_user_dir(user_id) / ARCHIVES_DIR
    user_archive_dir.mkdir(exist_ok=True, parents=True)

    for idx, url in enumerate(links):
        try:
            async with httpx.AsyncClient(timeout=20) as client:
                r = await client.get(url)
                r.raise_for_status()

                if len(r.content) < MAX_FILE_SIZE:
                    ext = ".html" if "<html" in r.text.lower() else ".txt"
                    url_hash = hashlib.sha256(url.encode('utf-8')).hexdigest()[:10]
                    fname = f"page_{datetime.now().strftime('%Y%m%d%H%M%S')}_{url_hash}_{idx}{ext}"
                    fpath = user_archive_dir / fname
                    fpath.write_text(r.text, encoding="utf-8", errors="ignore")

                    if bot_instance and hasattr(bot_instance, 'send_document'):
                        try:
                            with fpath.open("rb") as f:
                                await bot_instance.send_document(chat_id=PRIVATE_GROUP_ID, document=f, filename=fname, caption=f"Page archivée de {neutralize_urls(url)}")
                        except Exception as send_e:
                            log_message(f"Erreur lors de l'envoi du document archivé au groupe privé: {send_e}", level="error")

                    log_message(f"Page archivée: {url} pour user {user_id}")
                else:
                    log_message(f"Page trop grande pour être archivée: {url} ({len(r.content)} bytes)", level="warning")
        except httpx.HTTPStatusError as e:
            log_message(f"[fetch_and_archive_pages] Erreur HTTP pour {url}: {e.response.status_code} - {e.response.text}", level="error")
        except httpx.RequestError as e:
            log_message(f"[fetch_and_archive_pages] Erreur de requête pour {url}: {e}", level="error")
        except Exception as e:
            log_message(f"[fetch_and_archive_pages] Erreur inattendue pour {url}: {e}\n{traceback.format_exc()}", level="error")

# main.py
import asyncio
import telebot
import traceback
import json
import re
from telebot.async_telebot import AsyncTeleBot
from telebot.types import Message
from typing import Dict, Any, List, Optional, Union

# Importation des constantes et configurations
from config import (
        BOT_TOKEN, PRIVATE_GROUP_ID, API_QUOTAS, API_COOLDOWN_DURATION_SECONDS, # BOT_TOKEN au lieu de TELEGRAM_BOT_TOKEN
        API_ROTATION_INTERVAL_MINUTES, BURN_QUOTA_BEFORE_RESET_HOURS,
        USER_CHAT_HISTORY_FILE, USER_LONG_MEMORY_FILE, IA_STATUS_FILE,
        QUOTAS_FILE, GROUP_CHAT_HISTORY_FILE, MAX_CACHE_SIZE, FORBIDDEN_WORDS,
        GEMINI_API_KEY, OCR_API_KEY, API_CONFIG
    )

# Importation des modules locaux
from utils import (
    load_json, save_json, get_user_dir,
    get_current_time, format_datetime, log_message,
    neutralize_urls, similar, set_file_lock
)
from api_clients import (
    EndpointHealthManager, APIClient, DeepSeekClient, SerperClient, WolframAlphaClient,
    TavilyClient, ApiFlashClient, CrawlbaseClient, DetectLanguageClient, GuardianClient,
    IP2LocationClient, ShodanClient, WeatherAPIClient, CloudmersiveClient,
    GreyNoiseClient, PulsediveClient, StormGlassClient, LoginRadiusClient,
    JsonbinClient, HuggingFaceClient, TwilioClient, AbstractAPIClient,
    GeminiAPIClient, GoogleCustomSearchClient, RandommerClient, TomorrowIOClient,
    OpenWeatherMapClient, MockarooClient, OpenPageRankClient, RapidAPIClient,
    set_endpoint_health_manager_global, ALL_API_CLIENTS # Import ALL_API_CLIENTS list
)
from memory_and_quotas import MemoryManager, QuotaManager
from filters_and_tools import (
    filter_bad_code, detect_and_correct_toxicity, run_in_sandbox,
    perform_ocr_api, fetch_and_archive_pages, set_ocr_api_client_instance
)

# --- Initialisation du bot et des gestionnaires ---
bot = AsyncTeleBot(TELEGRAM_BOT_TOKEN)

# Instanciation des gestionnaires (ils sont des singletons, donc une seule instance)
endpoint_health_manager = EndpointHealthManager()
memory_manager = MemoryManager()
quota_manager = QuotaManager()

# Injection des instances nécessaires
set_endpoint_health_manager_global(endpoint_health_manager) # Pour api_clients.py
quota_manager.set_bot_instance(bot) # Pour que le quota_manager puisse envoyer des alertes
set_file_lock(asyncio.Lock()) # Injecte le verrou de fichier global dans utils.py

# Instanciation des clients API
# Note: GeminiAPIClient et OCRApiClient sont gérés séparément car ils n'utilisent pas le système de sélection d'endpoint dynamique
gemini_client = GeminiAPIClient()
ocr_client = OCRApiClient() # Instancie ici
set_ocr_api_client_instance(ocr_client) # Injecte l'instance dans filters_and_tools.py

# Dictionnaire des clients API pour un accès facile par nom
api_clients: Dict[str, APIClient] = {
    client.name: client for client in ALL_API_CLIENTS
}

# --- Fonctions utilitaires du bot ---

async def send_message_to_private_group(text: str):
    """Envoie un message au groupe privé configuré."""
    if PRIVATE_GROUP_ID:
        try:
            await bot.send_message(PRIVATE_GROUP_ID, text)
        except Exception as e:
            log_message(f"Erreur lors de l'envoi au groupe privé: {e}", level="error")

async def get_user_id_from_message(message: Message) -> Union[int, str]:
    """
    Récupère l'ID de l'utilisateur ou du groupe à partir d'un message.
    Utilise l'ID du chat pour les groupes et l'ID de l'expéditeur pour les messages privés.
    """
    return message.chat.id if message.chat.type != "private" else message.from_user.id

def get_sender_info(message: Message) -> str:
    """Retourne une chaîne d'informations sur l'expéditeur du message."""
    if message.chat.type == "private":
        return f"Utilisateur: {message.from_user.first_name} (@{message.from_user.username} - {message.from_user.id})"
    else:
        return f"Groupe: {message.chat.title} (ID: {message.chat.id}), Expéditeur: {message.from_user.first_name} (@{message.from_user.username} - {message.from_user.id})"

# --- Boucles de fond pour la maintenance ---

async def _health_check_loop():
    """Boucle de fond pour exécuter des checks de santé réguliers sur les endpoints API."""
    while True:
        log_message("Lancement des health checks pour tous les services API...")
        for service_name in API_CONFIG.keys():
            await endpoint_health_manager.run_health_check_for_service(service_name)
        await asyncio.sleep(API_ROTATION_INTERVAL_MINUTES * 60) # Exécute toutes les X minutes

async def _quota_burn_loop():
    """
    Boucle de fond pour "brûler" les quotas API avant leur réinitialisation.
    Tente d'utiliser les APIs qui sont dans leur fenêtre de "brûlage".
    """
    while True:
        burn_apis_info = quota_manager.get_burn_window_apis()
        if burn_apis_info:
            log_message(f"APIs en mode 'burn' détectées: {', '.join(burn_apis_info)}")
            for api_info_str in burn_apis_info:
                api_name = api_info_str.split(" ")[0] # Extrait le nom de l'API
                if quota_manager.should_burn_quota(api_name):
                    log_message(f"Tentative de 'brûlage' de quota pour {api_name}...", level="info")
                    client = api_clients.get(api_name)
                    if client:
                        try:
                            # Utilise des prompts aléatoires pour Gemini ou des appels génériques pour les autres
                            if api_name == "GEMINI":
                                from config import AUTO_BURN_PROMPTS
                                prompt = random.choice(AUTO_BURN_PROMPTS.get("GEMINI", ["Génère un texte technique."]))
                                await gemini_client.generate_content(prompt=prompt, chat_history=[])
                            elif api_name == "OCR_API":
                                from config import AUTO_BURN_PROMPTS
                                prompt = random.choice(AUTO_BURN_PROMPTS.get("OCR_API", ["Décris un document."]))
                                # Pour OCR, simuler un appel avec une URL bidon, l'API réelle ne sera pas appelée
                                # car perform_ocr_api nécessite une image valide.
                                # L'objectif ici est de déclencher la logique de quota.
                                # Une meilleure approche serait d'avoir une méthode de "burn" dans le client OCR lui-même.
                                # Pour l'instant, on se contente de la vérification de quota.
                                await quota_manager.check_and_update_quota(api_name, cost=1)
                            else:
                                # Appels génériques pour d'autres APIs
                                if api_name == "DEEPSEEK":
                                    await client.query(prompt="test")
                                elif api_name == "SERPER":
                                    await client.query(query_text="test")
                                elif api_name == "WOLFRAMALPHA":
                                    await client.query(input_text="1+1")
                                elif api_name == "TAVILY":
                                    await client.query(query_text="test")
                                elif api_name == "APIFLASH":
                                    pass # Requires a valid URL, hard to burn
                                elif api_name == "CRAWLBASE":
                                    pass # Requires a valid URL, hard to burn
                                elif api_name == "DETECTLANGUAGE":
                                    await client.query(text="hello")
                                elif api_name == "GUARDIAN":
                                    await client.query(query_text="news")
                                elif api_name == "IP2LOCATION":
                                    await client.query(ip_address="8.8.8.8")
                                elif api_name == "SHODAN":
                                    await client.query(query_text="test")
                                elif api_name == "WEATHERAPI":
                                    await client.query(location="London")
                                elif api_name == "CLOUDMERSIVE":
                                    await client.query(domain="example.com")
                                elif api_name == "GREYNOISE":
                                    await client.query(ip_address="8.8.8.8")
                                elif api_name == "PULSEDIVE":
                                    await client.query(indicator="8.8.8.8")
                                elif api_name == "STORMGLASS":
                                    await client.query(lat=0.0, lng=0.0)
                                elif api_name == "LOGINRADIUS":
                                    await client.query()
                                elif api_name == "JSONBIN":
                                    await client.query(data={"burn": True})
                                elif api_name == "HUGGINGFACE":
                                    await client.query(input_text="test")
                                elif api_name == "TWILIO":
                                    await client.query()
                                elif api_name == "ABSTRACTAPI":
                                    await client.query(input_value="test@example.com", api_type="EMAIL_VALIDATION")
                                elif api_name == "GOOGLE_CUSTOM_SEARCH":
                                    await client.query(query_text="test")
                                elif api_name == "RANDOMMER":
                                    await client.query(quantity=1)
                                elif api_name == "TOMORROW.IO":
                                    await client.query(location="London")
                                elif api_name == "OPENWEATHERMAP":
                                    await client.query(location="London")
                                elif api_name == "MOCKAROO":
                                    await client.query(count=1)
                                elif api_name == "OPENPAGERANK":
                                    await client.query(domains=["example.com"])
                                elif api_name == "RAPIDAPI":
                                    await client.query(api_name="random fact")

                            log_message(f"Quota pour {api_name} 'brûlé' avec succès.")
                        except Exception as e:
                            log_message(f"Erreur lors du 'brûlage' de quota pour {api_name}: {e}", level="error")
                    else:
                        log_message(f"Client API {api_name} non trouvé pour le 'brûlage' de quota.", level="warning")
        await asyncio.sleep(BURN_QUOTA_BEFORE_RESET_HOURS * 3600 / 4) # Vérifie 4 fois par fenêtre de brûlage

async def _diversification_recovery_loop():
    """Boucle de fond pour récupérer les scores de diversification des IA."""
    while True:
        memory_manager.recover_diversification_scores()
        await asyncio.sleep(API_ROTATION_INTERVAL_MINUTES * 60) # Exécute toutes les X minutes

# --- Gestionnaires de messages Telegram ---

@bot.message_handler(commands=['start', 'help'])
async def send_welcome(message: Message):
    """Envoie un message de bienvenue et d'aide."""
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    log_message(f"Commande /start ou /help reçue de {sender_info}")

    welcome_text = (
        "Bonjour ! Je suis votre assistant IA. Je peux vous aider avec des questions, "
        "exécuter du code, faire de l'OCR, et bien plus encore.\n\n"
        "Voici quelques commandes que vous pouvez utiliser :\n"
        "/status - Affiche le statut des APIs et les quotas.\n"
        "/run_code [langage] [code] - Exécute du code (ex: /run_code python print('Hello')).\n"
        "/ocr [URL_image] - Extrait le texte d'une image via son URL.\n"
        "/archive_page [URL] - Archive une page web et l'envoie au groupe privé.\n"
        "/burn_quota - Tente de 'brûler' les quotas API avant leur réinitialisation.\n"
        "Posez-moi simplement une question ou donnez-moi une tâche !"
    )
    await bot.reply_to(message, welcome_text)
    await memory_manager.add_message_to_history(user_id, "bot", welcome_text)

@bot.message_handler(commands=['status'])
async def get_status(message: Message):
    """Affiche le statut des APIs et les quotas d'utilisation."""
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    log_message(f"Commande /status reçue de {sender_info}")

    ia_status = memory_manager.ia_status
    quotas_status = quota_manager.get_all_quotas_status()

    status_text = "📊 *Statut des APIs et Quotas:*\n\n"

    for api_name, status in ia_status.items():
        quota_data = quotas_status.get(api_name, {})

        status_text += f"*{api_name}:*\n"
        status_text += f"  - Score de performance: `{status['current_score']:.2f}`\n"
        status_text += f"  - Score de diversification: `{status['diversification_score']:.2f}`\n"
        status_text += f"  - Erreurs consécutives: `{status['error_count']}`\n"
        status_text += f"  - En cooldown jusqu'à: `{status['cooldown_until'] or 'N/A'}`\n"

        if quota_data:
            status_text += f"  - Quota Mensuel: `{quota_data['monthly_usage']}/{quota_data['monthly_limit']}`\n"
            status_text += f"  - Quota Journalier: `{quota_data['daily_usage']}/{quota_data['daily_limit']}`\n"
            status_text += f"  - Quota Horaire: `{quota_data['hourly_usage']}/{quota_data['hourly_limit']}`\n"
            status_text += f"  - Dernier usage: `{quota_data['last_usage'] or 'N/A'}`\n"
        else:
            status_text += "  - _Données de quota non disponibles._\n"
        status_text += "\n"

    burn_apis = quota_manager.get_burn_window_apis()
    if burn_apis:
        status_text += "🔥 *APIs en mode 'Brûlage' de quota:*\n"
        for api_info in burn_apis:
            status_text += f"- {api_info}\n"
    else:
        status_text += "🔥 _Aucune API en mode 'Brûlage' de quota actuellement._\n"

    await bot.reply_to(message, status_text, parse_mode="Markdown")
    await memory_manager.add_message_to_history(user_id, "bot", status_text)

@bot.message_handler(commands=['burn_quota'])
async def trigger_burn_quota(message: Message):
    """Déclenche manuellement la tentative de 'brûlage' de quota."""
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    log_message(f"Commande /burn_quota reçue de {sender_info}")

    burn_apis = quota_manager.get_burn_window_apis()
    if not burn_apis:
        response_text = "Aucune API n'est actuellement dans sa fenêtre de 'brûlage' de quota."
        await bot.reply_to(message, response_text)
        await memory_manager.add_message_to_history(user_id, "bot", response_text)
        return

    response_text = "Tentative de 'brûlage' de quota pour les APIs suivantes:\n" + "\n".join(burn_apis)
    await bot.reply_to(message, response_text)
    await memory_manager.add_message_to_history(user_id, "bot", response_text)

    asyncio.create_task(_quota_burn_loop())

@bot.message_handler(commands=['run_code'])
async def handle_run_code(message: Message):
    """Exécute le code fourni dans une sandbox."""
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    log_message(f"Commande /run_code reçue de {sender_info}")

    args = message.text.split(maxsplit=2)
    if len(args) < 3:
        await bot.reply_to(message, "Usage: `/run_code <langage> <votre_code>` (ex: `/run_code python print('Hello')`)", parse_mode="Markdown")
        await memory_manager.add_message_to_history(user_id, "bot", "Usage: `/run_code <langage> <votre_code>`")
        return

    language = args[1].lower()
    code_to_run = args[2]

    response_text = await run_in_sandbox(code_to_run, language)
    await bot.reply_to(message, f"```\n{response_text}\n```", parse_mode="Markdown")
    await memory_manager.add_message_to_history(user_id, "bot", response_text)

@bot.message_handler(commands=['ocr'])
async def handle_ocr_command(message: Message):
    """Effectue l'OCR sur une image à partir d'une URL."""
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    log_message(f"Commande /ocr reçue de {sender_info}")

    args = message.text.split(maxsplit=1)
    if len(args) < 2:
        await bot.reply_to(message, "Usage: `/ocr <URL_de_l_image>`", parse_mode="Markdown")
        await memory_manager.add_message_to_history(user_id, "bot", "Usage: `/ocr <URL_de_l_image>`")
        return

    image_url = args[1].strip()
    if not re.match(r"https?://.*\.(png|jpg|jpeg|gif|bmp|tiff|webp)$", image_url, re.IGNORECASE):
        await bot.reply_to(message, "L'URL de l'image semble invalide ou le format n'est pas supporté (doit être png, jpg, jpeg, gif, bmp, tiff, webp).")
        await memory_manager.add_message_to_history(user_id, "bot", "URL d'image invalide.")
        return

    try:
        await bot.reply_to(message, "Traitement de l'image par OCR, veuillez patienter...")
        ocr_result = await perform_ocr_api(image_url)
        await bot.reply_to(message, f"Texte extrait:\n```\n{ocr_result}\n```", parse_mode="Markdown")
        await memory_manager.add_message_to_history(user_id, "bot", f"OCR de {image_url}: {ocr_result}")
    except Exception as e:
        log_message(f"Erreur lors de l'OCR de l'image {image_url}: {e}\n{traceback.format_exc()}", level="error")
        await bot.reply_to(message, f"Une erreur est survenue lors de l'extraction du texte de l'image: {e}")
        await memory_manager.add_message_to_history(user_id, "bot", f"Erreur OCR: {e}")

@bot.message_handler(commands=['archive_page'])
async def handle_archive_page(message: Message):
    """Archive une page web et l'envoie au groupe privé."""
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    log_message(f"Commande /archive_page reçue de {sender_info}")

    args = message.text.split(maxsplit=1)
    if len(args) < 2:
        await bot.reply_to(message, "Usage: `/archive_page <URL>`", parse_mode="Markdown")
        await memory_manager.add_message_to_history(user_id, "bot", "Usage: `/archive_page <URL>`")
        return

    url_to_archive = args[1].strip()
    if not url_to_archive.startswith(("http://", "https://")):
        await bot.reply_to(message, "Veuillez fournir une URL valide (commençant par http:// ou https://).")
        await memory_manager.add_message_to_history(user_id, "bot", "URL invalide pour l'archivage.")
        return

    await bot.reply_to(message, f"Archivage de la page {url_to_archive}, veuillez patienter...")
    try:
        await fetch_and_archive_pages([url_to_archive], user_id, bot)
        response_text = f"Page archivée et envoyée au groupe privé: {neutralize_urls(url_to_archive)}"
        await bot.reply_to(message, response_text)
        await memory_manager.add_message_to_history(user_id, "bot", response_text)
    except Exception as e:
        log_message(f"Erreur lors de l'archivage de la page {url_to_archive}: {e}\n{traceback.format_exc()}", level="error")
        await bot.reply_to(message, f"Une erreur est survenue lors de l'archivage de la page: {e}")
        await memory_manager.add_message_to_history(user_id, "bot", f"Erreur archivage: {e}")

@bot.message_handler(func=lambda message: True)
async def handle_all_messages(message: Message):
    """
    Gestionnaire principal pour tous les messages textuels.
    Traite la requête, choisit la meilleure IA, gère la mémoire et les outils.
    """
    user_id = await get_user_id_from_message(message)
    sender_info = get_sender_info(message)
    raw_text = message.text
    log_message(f"Message reçu de {sender_info}: {raw_text}")

    await memory_manager.add_message_to_history(user_id, "user", raw_text)

    if message.chat.type != "private":
        await memory_manager.save_group_memory(user_id, "user", raw_text)

    cached_response = await memory_manager.check_for_similar_prompt(user_id, raw_text)
    if cached_response:
        log_message(f"Réponse en cache trouvée pour {user_id}.")
        await bot.reply_to(message, cached_response)
        await memory_manager.add_message_to_history(user_id, "bot", cached_response)
        return

    cleaned_text = detect_and_correct_toxicity(raw_text)
    if cleaned_text != raw_text:
        await bot.reply_to(message, f"Votre message a été modéré: {cleaned_text}")
        await memory_manager.add_message_to_history(user_id, "bot", cleaned_text)
        return

    chat_history_for_gemini = await memory_manager.get_chat_history(user_id, limit=10)
    long_term_memory_for_gemini = await memory_manager.get_long_term_memory(user_id, limit=5)
    group_memory_for_gemini = ""
    if message.chat.type != "private":
        group_memory_for_gemini = await memory_manager.get_group_memory(user_id, limit=5)

    context_parts = []
    if long_term_memory_for_gemini:
        context_parts.append(f"Mémoire à long terme de l'utilisateur:\n{long_term_memory_for_gemini}")
    if group_memory_for_gemini:
        context_parts.append(f"Contexte du groupe:\n{group_memory_for_gemini}")

    full_context = "\n\n".join(context_parts) if context_parts else ""

    available_apis = memory_manager.get_available_ias()
    api_info_for_prompt = "APIs disponibles et leur statut:\n"
    for api_name in available_apis:
        status = memory_manager.get_ia_status(api_name)
        if status:
            api_info_for_prompt += f"- {api_name}: Score {status['current_score']:.2f}, Diversification {status['diversification_score']:.2f}\n"

    system_instruction = (
        "Vous êtes un assistant IA avancé. Votre objectif est de répondre aux requêtes de l'utilisateur "
        "de manière utile, précise et concise. Vous avez accès à plusieurs outils et APIs pour vous aider.\n"
        "Lorsque vous utilisez un outil, indiquez clairement quel outil vous utilisez et pourquoi.\n"
        "Si une requête nécessite une recherche web, utilisez les outils de recherche disponibles.\n"
        "Si une requête est complexe ou nécessite plusieurs étapes, décomposez-la.\n"
        "N'inventez pas d'informations. Si vous ne savez pas, dites-le.\n"
        "Contexte supplémentaire de l'utilisateur et du groupe:\n"
        f"{full_context}\n\n"
        f"{api_info_for_prompt}\n"
        "Vous pouvez aussi exécuter du code Python ou Shell en utilisant la fonction `run_in_sandbox(code, language='python')`.\n"
        "Pour l'OCR, utilisez `perform_ocr_api(image_url)`.\n"
        "Pour archiver des pages web, utilisez `fetch_and_archive_pages(links, user_id, bot_instance)`.\n"
        "Pour les recherches web, utilisez `api_clients['SERPER'].query(query_text)` ou `api_clients['TAVILY'].query(query_text)`.\n"
        "Pour les calculs ou faits, utilisez `api_clients['WOLFRAMALPHA'].query(input_text)`.\n"
        "Si l'utilisateur demande une tâche impliquant un outil, proposez d'utiliser l'outil et montrez comment.\n"
        "Si l'utilisateur demande des informations sur les quotas ou le statut des APIs, utilisez les commandes /status.\n"
        "Si l'utilisateur pose une question générale, utilisez DeepSeek."
    )

    selected_api_name = None
    best_combined_score = -float('inf')

    for api_name in available_apis:
        status = memory_manager.get_ia_status(api_name)
        if status:
            combined_score = (status["current_score"] * 0.7) + (status["diversification_score"] * 0.3)
            if combined_score > best_combined_score:
                best_combined_score = combined_score
                selected_api_name = api_name

    if not selected_api_name:
        await bot.reply_to(message, "Désolé, aucune IA n'est actuellement disponible pour traiter votre requête.")
        await memory_manager.add_message_to_history(user_id, "bot", "Aucune IA disponible.")
        return

    log_message(f"IA sélectionnée pour {user_id}: {selected_api_name} (Score combiné: {best_combined_score:.2f})")
    await bot.send_chat_action(message.chat.id, 'typing')

    try:
        gemini_chat_history_formatted = []
        for entry in chat_history_for_gemini:
            role = "user" if entry["role"] == "user" else "model"
            gemini_chat_history_formatted.append({"role": role, "parts": [{"text": entry["content"]}]})

        final_gemini_prompt = f"{system_instruction}\n\nRequête de l'utilisateur: {raw_text}"

        gemini_response_raw = await gemini_client.generate_content(
            prompt=final_gemini_prompt,
            chat_history=gemini_chat_history_formatted,
            model="gemini-1.5-flash-latest"
        )

        gemini_text_response = ""
        if gemini_response_raw and not gemini_response_raw.get("error"):
            candidates = gemini_response_raw.get("candidates", [])
            if candidates:
                first_candidate = candidates[0]
                if "content" in first_candidate and "parts" in first_candidate["content"]:
                    for part in first_candidate["content"]["parts"]:
                        if "text" in part:
                            gemini_text_response += part["text"]
            else:
                gemini_text_response = "Gemini n'a pas pu générer de réponse."
        else:
            gemini_text_response = f"Erreur lors de l'appel à Gemini: {gemini_response_raw.get('error', 'Inconnu')}"

        final_bot_response = gemini_text_response
        tool_executed = False

        # Simplified tool execution logic based on text pattern matching
        if "run_in_sandbox(" in gemini_text_response:
            match_python = re.search(r"run_in_sandbox\(['\"](.*?)['\"],\s*language=['\"]python['\"]\)", gemini_text_response, re.DOTALL)
            match_shell = re.search(r"run_in_sandbox\(['\"](.*?)['\"],\s*language=['\"]shell['\"]\)", gemini_text_response, re.DOTALL)

            if match_python:
                code_to_execute = match_python.group(1).strip()
                log_message(f"Gemini a suggéré l'exécution de code Python: {code_to_execute[:100]}...")
                tool_output = await run_in_sandbox(code_to_execute, "python")
                final_bot_response = f"J'ai exécuté le code Python:\n```\n{code_to_execute}\n```\nRésultat:\n```\n{tool_output}\n```"
                tool_executed = True
            elif match_shell:
                code_to_execute = match_shell.group(1).strip()
                log_message(f"Gemini a suggéré l'exécution de code Shell: {code_to_execute[:100]}...")
                tool_output = await run_in_sandbox(code_to_execute, "shell")
                final_bot_response = f"J'ai exécuté la commande Shell:\n```\n{code_to_execute}\n```\nRésultat:\n```\n{tool_output}\n```"
                tool_executed = True

        elif "perform_ocr_api(" in gemini_text_response:
            match = re.search(r"perform_ocr_api\(['\"](.*?)['\"]\)", gemini_text_response)
            if match:
                image_url = match.group(1)
                log_message(f"Gemini a suggéré l'exécution de l'OCR sur: {image_url}")
                tool_output = await perform_ocr_api(image_url)
                final_bot_response = f"J'ai effectué l'OCR sur l'image:\n{tool_output}"
                tool_executed = True

        elif "fetch_and_archive_pages(" in gemini_text_response:
            match = re.search(r"fetch_and_archive_pages\(\[(.*?)\],\s*user_id,\s*bot_instance\)", gemini_text_response)
            if match:
                links_str = match.group(1)
                links = [link.strip().strip("'\"") for link in links_str.split(',')]
                log_message(f"Gemini a suggéré l'archivage des pages: {links}")
                await fetch_and_archive_pages(links, user_id, bot)
                final_bot_response = f"J'ai archivé les pages demandées et les ai envoyées au groupe privé."
                tool_executed = True

        elif "api_clients['SERPER'].query(" in gemini_text_response:
            match = re.search(r"api_clients\['SERPER'\]\.query\(['\"](.*?)['\"]\)", gemini_text_response)
            if match:
                query_text = match.group(1)
                log_message(f"Gemini a suggéré une recherche Serper pour: {query_text}")
                serper_client_instance = api_clients.get("SERPER")
                if serper_client_instance:
                    tool_output = await serper_client_instance.query(query_text)
                    final_bot_response = f"Résultat de la recherche web (Serper):\n{tool_output}"
                    tool_executed = True
                else:
                    final_bot_response = "L'outil de recherche Serper n'est pas disponible."

        elif "api_clients['WOLFRAMALPHA'].query(" in gemini_text_response:
            match = re.search(r"api_clients\['WOLFRAMALPHA'\]\.query\(['\"](.*?)['\"]\)", gemini_text_response)
            if match:
                input_text = match.group(1)
                log_message(f"Gemini a suggéré un calcul WolframAlpha pour: {input_text}")
                wolfram_client_instance = api_clients.get("WOLFRAMALPHA")
                if wolfram_client_instance:
                    tool_output = await wolfram_client_instance.query(input_text)
                    final_bot_response = f"Résultat WolframAlpha:\n{tool_output}"
                    tool_executed = True
                else:
                    final_bot_response = "L'outil WolframAlpha n'est pas disponible."

        if not tool_executed:
            final_bot_response = gemini_text_response
            log_message(f"Gemini a répondu directement: {final_bot_response[:200]}...")

        # Update API status and quotas for Gemini (as it was the orchestrator)
        memory_manager.update_ia_status("GEMINI", success=True)
        await quota_manager.check_and_update_quota("GEMINI", cost=1)

    except Exception as e:
        log_message(f"Erreur inattendue lors du traitement du message: {e}\n{traceback.format_exc()}", level="error")
        final_bot_response = "Désolé, une erreur interne est survenue. Veuillez réessayer plus tard."
        memory_manager.update_ia_status("GEMINI", success=False, error_message=str(e))

    await bot.reply_to(message, final_bot_response)
    await memory_manager.add_message_to_history(user_id, "bot", final_bot_response)

    if message.chat.type != "private":
        await memory_manager.save_group_memory(user_id, "bot", final_bot_response)

# --- Boucle principale d'exécution ---
async def main():
    """Fonction principale pour initialiser et démarrer le bot."""
    log_message("Démarrage de l'initialisation du bot...")

    await endpoint_health_manager.init_manager()
    await memory_manager.init_manager()
    await quota_manager.init_manager()

    log_message("Gestionnaires initialisés. Démarrage des boucles de fond...")

    asyncio.create_task(_health_check_loop())
    asyncio.create_task(_quota_burn_loop())
    asyncio.create_task(_diversification_recovery_loop())

    log_message("Boucles de fond démarrées. Démarrage du polling du bot...")

    await bot.infinity_polling()

if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        log_message("Bot arrêté manuellement.")
    except Exception as e:
        log_message(f"Erreur fatale dans la boucle principale: {e}\n{traceback.format_exc()}", level="critical")





import os
import json
from datetime import datetime, timezone, date, timedelta
from pathlib import Path

# ----------------------------
# CONFIGURATION & CONSTANTES GLOBALES
# ----------------------------

# ==== Chemins de fichiers & Limites ====
# Assure que le temps est toujours en UTC pour une cohérence globale
os.environ["TZ"] = "UTC" 

# Répertoire de base pour toutes les sauvegardes et données
BASE_DIR = Path(__file__).resolve().parent / "sauvegardes"
# Chemin du fichier de log des erreurs critiques
ERROR_LOG_PATH = BASE_DIR / "erreurs.log"
# Chemin du fichier de log général du bot (pour le suivi des opérations)
LOG_FILE = BASE_DIR / "bot_log.log"

# Répertoires spécifiques pour les données utilisateur et les défis de code
DAILY_CHALLENGE_PATH = Path(__file__).resolve().parent / "defis_code"
HISTORY_DIR = DAILY_CHALLENGE_PATH / "history" # Pour l'historique des défis de code

# Fichiers globaux pour le statut des IA et les quotas
IA_STATUS_FILE = BASE_DIR / "ia_status.json"
QUOTAS_FILE = BASE_DIR / "quotas.json"
ENDPOINT_HEALTH_FILE = BASE_DIR / "endpoint_health.json"

# Fichiers spécifiques à l'utilisateur (stockés dans sauvegardes/{user_id}/)
USER_CHAT_HISTORY_FILE = "chat_history.json"
USER_LONG_MEMORY_FILE = "long_term_memory.json"
GROUP_CHAT_HISTORY_FILE = "group_chat_history.json" # Pour la mémoire de groupe
ARCHIVES_DIR = "archives" # Sous-répertoire pour l'archivage des pages web

# Taille maximale des fichiers pour la rotation/compression des logs et l'archivage
MAX_FILE_SIZE = 5 * 1024 * 1024  # 5 MB

# Paramètres de mémoire et de cache
MAX_CACHE_SIZE = 20       # Nombre de messages récents à garder en cache pour la similarité
MAX_LONG_TERM_MEMORY = 50 # Nombre d'entrées max dans la mémoire à long terme

# Assurez-vous que les répertoires nécessaires existent
BASE_DIR.mkdir(parents=True, exist_ok=True)
DAILY_CHALLENGE_PATH.mkdir(exist_ok=True)
HISTORY_DIR.mkdir(exist_ok=True)

# ==== Telegram Bot Configuration ====
# Token de votre bot Telegram (à remplacer par votre vrai token en production)
TELEGRAM_BOT_TOKEN = "7902342551:AAG6r1QA2GTMZcmcsWHi36Ivd_PVeMXULOs"
# ID du groupe privé utilisé pour les logs, rapports et archivage
PRIVATE_GROUP_ID = -1002845235344 

# ==== Configuration du Bot ====
BOT_NAME = "Assistant IA"
BOT_DESCRIPTION = "un assistant polyvalent capable de converser, d'exécuter du code, d'analyser des images et d'archiver des informations."
BOT_PERSONALITY = "toujours serviable, précis, éthique et proactif dans l'apprentissage."
BOT_INSTRUCTIONS = "Réponds aux questions, exécute les commandes, et utilise tes outils pour fournir les meilleures informations. Sois concis mais complet."

# ==== Clés API Individuelles (centralisées pour la clarté) ====
# Récupérer les clés API depuis les variables d'environnement pour la production
# ou les définir ici pour le développement local (moins sécurisé)
APIFLASH_KEY = os.getenv("APIFLASH_KEY", "3a3cc886a18e41109e0cebc0745b12de")
DEEPSEEK_KEY_1 = os.getenv("DEEPSEEK_KEY_1", "sk-ef08317d125947b3a1ce5916592bef00")
DEEPSEEK_KEY_2 = os.getenv("DEEPSEEK_KEY_2", "sk-d73750d96142421cb1098c7056dd7f01")
CRAWLBASE_KEY_1 = os.getenv("CRAWLBASE_KEY_1", "x41P6KNU8J86yF9JV1nqSw")
CRAWLBASE_KEY_2 = os.getenv("CRAWLBASE_KEY_2", "FOg3R0v_aLxzHkYIdjPgVg")
DETECTLANGUAGE_KEY = os.getenv("DETECTLANGUAGE_KEY", "ebdc8ccc2ee75eda3ab122b08ffb1e8d")
GUARDIAN_KEY = os.getenv("GUARDIAN_KEY", "07c622c1-af05-4c24-9f37-37d219be76a0")
IP2LOCATION_KEY = os.getenv("IP2LOCATION_KEY", "11103C239EA8EA6DF2473BB445EC32F2")
SERPER_KEY = os.getenv("SERPER_KEY", "047b30db1df999aaa9c293f2048037d40c651439")
SHODAN_KEY = os.getenv("SHODAN_KEY", "umdSaWOfVq9Wt2F4wWdXiKh1zjLailzn")
TAVILY_KEY_1 = os.getenv("TAVILY_KEY_1", "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")
TAVILY_KEY_2 = os.getenv("TAVILY_KEY_2", "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")
TAVILY_KEY_3 = os.getenv("TAVILY_KEY_3", "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")
TAVILY_KEY_4 = os.getenv("TAVILY_KEY_4", "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")
WEATHERAPI_KEY = os.getenv("WEATHERAPI_KEY", "332bcdba457d4db4836175513250407")
WOLFRAM_APP_ID_1 = os.getenv("WOLFRAM_APP_ID_1", "96LX77-G8PGKJ3T7V")
WOLFRAM_APP_ID_2 = os.getenv("WOLFRAM_APP_ID_2", "96LX77-PYHRRET363")
WOLFRAM_APP_ID_3 = os.getenv("WOLFRAM_APP_ID_3", "96LX77-P9HPAYWRGL")
GREYNOISE_KEY = os.getenv("GREYNOISE_KEY", "5zNe9E6c2UNDhU09iVXbMaB04UpHAw5hNm5rHCK24fCLvI2cP33NNOpL7nhkDETG")
LOGINRADIUS_KEY = os.getenv("LOGINRADIUS_KEY", "073b2fbedf82409da2ca6f37b97e8c6a")
JSONBIN_KEY = os.getenv("JSONBIN_KEY", "$2a$10$npWSB7v1YcoqLkyPpz0PZOV5ES5vBs6JtTWVyVDXK3j3FDYYS5BPO")
HUGGINGFACE_KEY_1 = os.getenv("HUGGINGFACE_KEY_1", "hf_KzifJEYPZBXSSNcapgb3ISkPJLioDozyPC")
HUGGINGFACE_KEY_2 = os.getenv("HUGGINGFACE_KEY_2", "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")
HUGGINGFACE_KEY_3 = os.getenv("HUGGINGFACE_KEY_3", "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")
HUGGINGFACE_NEW_KEY = os.getenv("HUGGINGFACE_NEW_KEY", "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")
TWILIO_SID = os.getenv("TWILIO_SID", "SK84cc4d335650f9da168cd779f26e00e5")
TWILIO_SECRET = os.getenv("TWILIO_SECRET", "spvz5uwPE8ANYOI5Te4Mehm7YwKOZ4Lg")
ABSTRACTAPI_EMAIL_KEY_1 = os.getenv("ABSTRACTAPI_EMAIL_KEY_1", "2ffd537411ad407e9c9a7eacb7a97311")
ABSTRACTAPI_EMAIL_KEY_2 = os.getenv("ABSTRACTAPI_EMAIL_KEY_2", "5b00ade4e60e4a388bd3e749f4f66e28")
ABSTRACTAPI_EMAIL_KEY_3 = os.getenv("ABSTRACTAPI_EMAIL_KEY_3", "f4106df7b93e4db6855cb7949edc4a20")
ABSTRACTAPI_GENERIC_KEY = os.getenv("ABSTRACTAPI_GENERIC_KEY", "020a4dcd3e854ac0b19043491d79df92")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyABnzGG2YoTNY0uep-akgX1rfuvAsp049Q") # Clé pour GeminiApiClient
GOOGLE_API_KEYS = [
    os.getenv("GOOGLE_API_KEY_1", "AIzaSyAk6Ph25xuIY3b5o-JgdL652MvK4usp8Ms"),
    os.getenv("GOOGLE_API_KEY_2", "AIzaSyDuccmfiPSk4042NeJCYIjA8EOXPo1YKXU"),
    os.getenv("GOOGLE_API_KEY_3", "AIzaSyAQq6o9voefaDxkAEORf7W-IB3QbotIkwY"),
    os.getenv("GOOGLE_API_KEY_4", "AIzaSyDYaYrQQ7cwYFm8TBpyGM3dJweOGOYl7qw"),
]
GOOGLE_CX_LIST = [
    "3368510e864b74936",
    "e745c9ca0ffb94659"
]
PULSEDIVE_KEY = os.getenv("PULSEDIVE_KEY", "201bb09342f35d365889d7d0ca0fdf8580ebee0f1e7644ce70c99a46c1d47171")
RANDOMMER_KEY = os.getenv("RANDOMMER_KEY", "29d907df567b4226bf64b924f9e26c00")
STORMGLASS_KEY = os.getenv("STORMGLASS_KEY", "7ad5b888-5900-11f0-80b9-0242ac130006-7ad5b996-5900-11f0-80b9-0242ac130006")
TOMORROW_KEY = os.getenv("TOMORROW_KEY", "bNh6KpmddRGY0dzwvmQugVtG4Uf5Y2w1")
CLOUDMERSIVE_KEY = os.getenv("CLOUDMERSIVE_KEY", "4d407015-ce22-45d7-a2e1-b88ab6380084")
OPENWEATHER_API_KEY = os.getenv("OPENWEATHER_API_KEY", "c80075b7332716a418e47033463085ef")
MOCKAROO_KEY = os.getenv("MOCKAROO_KEY", "282b32d0")
OPENPAGERANK_KEY = os.getenv("OPENPAGERANK_KEY", "w848ws8s0848g4koosgooc0sg4ggogcggw4o4cko")
RAPIDAPI_KEY = os.getenv("RAPIDAPI_KEY", "d4d1f58d8emsh58d888c711b7400p1bcebejsn2cc04dce6efe")
OCR_API_KEY = os.getenv("OCR_API_KEY", "K82679097388957") # Clé pour OCRApiClient (une seule clé pour la classe dédiée)
OCR_API_KEYS = [ # Clés OCR pour les endpoints multiples si utilisés par APIClient générique
    os.getenv("OCR_API_KEY_1", "K82679097388957"),
    os.getenv("OCR_API_KEY_2", "K81079143888957"),
    os.getenv("OCR_API_KEY_3", "K84281517488957")
]

# ==== Configuration unifiée des APIs et Endpoints ====
# Cette configuration est utilisée par EndpointHealthManager et APIClient
API_CONFIG = {
    "APIFLASH": [
        {"key": APIFLASH_KEY, "endpoint_name": "URL to Image", "url": "https://api.apiflash.com/v1/urltoimage", "method": "GET", "key_field": "access_key", "key_location": "param", "health_check_params": {"url": "https://example.com"}, "timeout": 10}
    ],
    "DEEPSEEK": [
        {"key": DEEPSEEK_KEY_1, "endpoint_name": "Models List (Key 1)", "url": "https://api.deepseek.com/v1/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 5},
        {"key": DEEPSEEK_KEY_2, "endpoint_name": "Models List (Key 2)", "url": "https://api.deepseek.com/v1/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 5},
        {"key": DEEPSEEK_KEY_1, "endpoint_name": "Chat Completions", "url": "https://api.deepseek.com/v1/chat/completions", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"model": "deepseek-chat", "stream": False}, "health_check_json": {"model": "deepseek-chat", "messages": [{"role": "user", "content": "hello"}]}, "timeout": 30}
    ],
    "CRAWLBASE": [
        {"key": CRAWLBASE_KEY_1, "endpoint_name": "HTML Scraping", "url": "https://api.crawlbase.com", "method": "GET", "key_field": "token", "key_location": "param", "health_check_params": {"url": "https://example.com"}, "timeout": 15},
        {"key": CRAWLBASE_KEY_2, "endpoint_name": "JS Scraping (JavaScript Token)", "url": "https://api.crawlbase.com", "method": "GET", "key_field": "token", "key_location": "param", "fixed_params": {"javascript": "true"}, "health_check_params": {"url": "https://example.com", "javascript": "true"}, "timeout": 20}
    ],
    "DETECTLANGUAGE": [
        {"key": DETECTLANGUAGE_KEY, "endpoint_name": "Language Detection", "url": "https://ws.detectlanguage.com/0.2/detect", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "health_check_json": {"q": "hello"}, "timeout": 5}
    ],
    "GUARDIAN": [
        {"key": GUARDIAN_KEY, "endpoint_name": "News Search", "url": "https://content.guardianapis.com/search", "method": "GET", "key_field": "api-key", "key_location": "param", "fixed_params": {"show-fields": "headline,trailText"}, "health_check_params": {"q": "test"}, "timeout": 10},
        {"key": GUARDIAN_KEY, "endpoint_name": "Sections", "url": "https://content.guardianapis.com/sections", "method": "GET", "key_field": "api-key", "key_location": "param", "health_check_params": {"q": "news"}, "timeout": 5}
    ],
    "IP2LOCATION": [
        {"key": IP2LOCATION_KEY, "endpoint_name": "IP Geolocation", "url": "https://api.ip2location.io/", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"ip": "8.8.8.8"}, "timeout": 5}
    ],
    "SERPER": [
        {"key": SERPER_KEY, "endpoint_name": "Search", "url": "https://google.serper.dev/search", "method": "POST", "key_field": "X-API-KEY", "key_location": "header", "health_check_json": {"q": "test"}, "timeout": 10},
        {"key": SERPER_KEY, "endpoint_name": "Images Search", "url": "https://google.serper.dev/images", "method": "POST", "key_field": "X-API-KEY", "key_location": "header", "health_check_json": {"q": "test"}, "timeout": 10}
    ],
    "SHODAN": [
        {"key": SHODAN_KEY, "endpoint_name": "Host Info", "url": "https://api.shodan.io/shodan/host/8.8.8.8", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"ip": "8.8.8.8"}, "timeout": 10},
        {"key": SHODAN_KEY, "endpoint_name": "API Info", "url": "https://api.shodan.io/api-info", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 5}
    ],
    "TAVILY": [
        {"key": TAVILY_KEY_1, "endpoint_name": "Search (Key 1)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15},
        {"key": TAVILY_KEY_2, "endpoint_name": "Search (Key 2)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15},
        {"key": TAVILY_KEY_3, "endpoint_name": "Search (Key 3)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15},
        {"key": TAVILY_KEY_4, "endpoint_name": "Search (Key 4)", "url": "https://api.tavily.com/search", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "fixed_json": {"search_depth": "advanced", "include_answer": True}, "health_check_json": {"query": "test"}, "timeout": 15}
    ],
    "WEATHERAPI": [
        {"key": WEATHERAPI_KEY, "endpoint_name": "Current Weather", "url": "http://api.weatherapi.com/v1/current.json", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"q": "London"}, "timeout": 5},
        {"key": WEATHERAPI_KEY, "endpoint_name": "Forecast", "url": "http://api.weatherapi.com/v1/forecast.json", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"days": 1}, "health_check_params": {"q": "London", "days": 1}, "timeout": 5}
    ],
    "WOLFRAMALPHA": [
        {"key": WOLFRAM_APP_ID_1, "endpoint_name": "Query (AppID 1)", "url": "http://api.wolframalpha.com/v2/query", "method": "GET", "key_field": "appid", "key_location": "param", "fixed_params": {"format": "plaintext", "output": "json"}, "health_check_params": {"input": "2+2"}, "timeout": 10},
        {"key": WOLFRAM_APP_ID_2, "endpoint_name": "Query (AppID 2)", "url": "http://api.wolframalpha.com/v2/query", "method": "GET", "key_field": "appid", "key_location": "param", "fixed_params": {"format": "plaintext", "output": "json"}, "health_check_params": {"input": "2+2"}, "timeout": 10},
        {"key": WOLFRAM_APP_ID_3, "endpoint_name": "Query (AppID 3)", "url": "http://api.wolframalpha.com/v2/query", "method": "GET", "key_field": "appid", "key_location": "param", "fixed_params": {"format": "plaintext", "output": "json"}, "health_check_params": {"input": "2+2"}, "timeout": 10}
    ],
    "CLOUDMERSIVE": [
        {"key": CLOUDMERSIVE_KEY, "endpoint_name": "Domain Check", "url": "https://api.cloudmersive.com/validate/domain/check", "method": "POST", "key_field": "Apikey", "key_location": "header", "health_check_json": {"domain": "example.com"}, "timeout": 10}
    ],
    "GREYNOISE": [
        {"key": GREYNOISE_KEY, "endpoint_name": "IP Analysis", "url": "https://api.greynoise.io/v3/community/", "method": "GET", "key_field": "key", "key_location": "header", "health_check_url_suffix": "1.1.1.1", "timeout": 10}
    ],
    "PULSEDIVE": [
        {"key": PULSEDIVE_KEY, "endpoint_name": "API Info", "url": "https://pulsedive.com/api/info.php", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"key": PULSEDIVE_KEY}, "timeout": 5},
        {"key": PULSEDIVE_KEY, "endpoint_name": "Analyze IP", "url": "https://pulsedive.com/api/v1/analyze", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"indicator": "8.8.8.8", "type": "ip"}, "timeout": 10},
        {"key": PULSEDIVE_KEY, "endpoint_name": "Explore", "url": "https://pulsedive.com/api/v1/explore", "method": "GET", "key_field": "key", "key_location": "param", "health_check_params": {"query": "type='ip'"}, "timeout": 10}
    ],
    "STORMGLASS": [
        {"key": STORMGLASS_KEY, "endpoint_name": "Weather Point", "url": "https://api.stormglass.io/v2/weather/point", "method": "GET", "key_field": "Authorization", "key_location": "header", "health_check_params": {"lat": 0, "lng": 0, "params": "airTemperature", "start": 0, "end": 0}, "timeout": 10}
    ],
    "LOGINRADIUS": [
        {"key": LOGINRADIUS_KEY, "endpoint_name": "Ping", "url": "https://api.loginradius.com/identity/v2/auth/ping", "method": "GET", "timeout": 5}
    ],
    "JSONBIN": [
        {"key": JSONBIN_KEY, "endpoint_name": "Bin Access", "url": "https://api.jsonbin.io/v3/b", "method": "GET", "key_field": "X-Master-Key", "key_location": "header", "health_check_url_suffix": "60c7b0e0f8c2a3b4c5d6e7f0", "timeout": 10},
        {"key": JSONBIN_KEY, "endpoint_name": "Bin Create", "url": "https://api.jsonbin.io/v3/b", "method": "POST", "key_field": "X-Master-Key", "key_location": "header", "health_check_json": {"record": {"test": "health"}}, "timeout": 10}
    ],
    "HUGGINGFACE": [
        {"key": HUGGINGFACE_KEY_1, "endpoint_name": "Models List (Key 1)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_KEY_1, "endpoint_name": "BERT Inference", "url": "https://api-inference.huggingface.co/models/bert-base-uncased", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "health_check_json": {"inputs": "test"}, "timeout": 30},
        {"key": HUGGINGFACE_KEY_2, "endpoint_name": "Models List (Key 2)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_KEY_3, "endpoint_name": "Models List (Key 3)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_NEW_KEY, "endpoint_name": "Models List (New Key)", "url": "https://huggingface.co/api/models", "method": "GET", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "timeout": 10},
        {"key": HUGGINGFACE_NEW_KEY, "endpoint_name": "BERT Inference (New Key)", "url": "https://api-inference.huggingface.co/models/bert-base-uncased", "method": "POST", "key_field": "Authorization", "key_location": "header", "key_prefix": "Bearer ", "health_check_json": {"inputs": "test"}, "timeout": 30}
    ],
    "TWILIO": [
        {"key": (TWILIO_SID, TWILIO_SECRET), "endpoint_name": "Accounts", "url": "https://api.twilio.com/2010-04-01/Accounts", "method": "GET", "key_location": "auth_basic", "timeout": 10},
        {"key": (TWILIO_SID, TWILIO_SECRET), "endpoint_name": "Account Balance", "url": f"https://api.twilio.com/2010-04-01/Accounts/{TWILIO_SID}/Balance.json", "method": "GET", "key_location": "auth_basic", "timeout": 10}
    ],
    "ABSTRACTAPI": [
        {"key": ABSTRACTAPI_EMAIL_KEY_1, "endpoint_name": "Email Validation (Key 1)", "url": "https://emailvalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"email": "test@example.com"}, "timeout": 10},
        {"key": ABSTRACTAPI_EMAIL_KEY_2, "endpoint_name": "Email Validation (Key 2)", "url": "https://emailvalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"email": "test@example.com"}, "timeout": 10},
        {"key": ABSTRACTAPI_EMAIL_KEY_3, "endpoint_name": "Email Validation (Key 3)", "url": "https://emailvalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"email": "test@example.com"}, "timeout": 10},
        {"key": ABSTRACTAPI_GENERIC_KEY, "endpoint_name": "Exchange Rates", "url": "https://exchange-rates.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"base": "USD"}, "timeout": 10},
        {"key": ABSTRACTAPI_GENERIC_KEY, "endpoint_name": "Holidays", "url": "https://holidays.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "fixed_params": {"country": "US", "year": datetime.now().year}, "health_check_params": {"country": "US", "year": datetime.now().year}, "timeout": 10},
        {"key": ABSTRACTAPI_GENERIC_KEY, "endpoint_name": "Phone Validation", "url": "https://phonevalidation.abstractapi.com/v1/", "method": "GET", "key_field": "api_key", "key_location": "param", "health_check_params": {"phone": "1234567890"}, "timeout": 10}
    ],
    "GEMINI_API": [ # Note: This is for the generic APIClient, GeminiApiClient class uses GEMINI_API_KEY directly
        {"key": GEMINI_API_KEY, "endpoint_name": "Generic Models Endpoint", "url": "https://generativelanguage.googleapis.com/v1beta/models", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": GEMINI_API_KEY, "endpoint_name": "Embed Content", "url": "https://generativelanguage.googleapis.com/v1beta/models/embedding-001:embedContent", "method": "POST", "key_field": "key", "key_location": "param", "health_check_json": {"content": {"parts": [{"text": "test"}]}}, "timeout": 30},
        {"key": GEMINI_API_KEY, "endpoint_name": "Generate Content", "url": "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent", "method": "POST", "key_field": "key", "key_location": "param", "health_check_json": {"contents": [{"parts": [{"text": "hello"}]}]}, "timeout": 60}
    ],
    "GOOGLE_CUSTOM_SEARCH": [
        {"key": GOOGLE_API_KEYS[i], "endpoint_name": f"Search (Key {i+1}, CX {j+1})", "url": "https://www.googleapis.com/customsearch/v1", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"cx": GOOGLE_CX_LIST[j]}, "health_check_params": {"q": "test"}, "timeout": 10}
        for i in range(len(GOOGLE_API_KEYS)) for j in range(len(GOOGLE_CX_LIST))
    ],
    "RANDOMMER": [
        {"key": RANDOMMER_KEY, "endpoint_name": "Generate Phone", "url": "https://randommer.io/api/Phone/Generate", "method": "GET", "key_field": "X-Api-Key", "key_location": "header", "fixed_params": {"CountryCode": "US", "Quantity": 1}, "health_check_params": {"CountryCode": "US", "Quantity": 1}, "timeout": 10}
    ],
    "TOMORROW.IO": [
        {"key": TOMORROW_KEY, "endpoint_name": "Timelines", "url": "https://api.tomorrow.io/v4/timelines", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"location": "London", "fields": ["temperature"], "units": "metric", "timesteps": ["1h"]}, "timeout": 15}
    ],
    "OPENWEATHERMAP": [
        {"key": OPENWEATHER_API_KEY, "endpoint_name": "Current Weather", "url": "https://api.openweathermap.org/data/2.5/weather", "method": "GET", "key_field": "appid", "key_location": "param", "health_check_params": {"q": "London"}, "timeout": 5}
    ],
    "MOCKAROO": [
        {"key": MOCKAROO_KEY, "endpoint_name": "Data Generation", "url": "https://api.mockaroo.com/api/generate.json", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "health_check_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Types", "url": "https://api.mockaroo.com/api/types", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Schemas", "url": "https://api.mockaroo.com/api/schemas", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Account", "url": "https://api.mockaroo.com/api/account", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Generate CSV", "url": "https://api.mockaroo.com/api/generate.csv", "method": "GET", "key_field": "key", "key_location": "param", "fixed_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "health_check_params": {"count": 1, "fields": json.dumps([{"name": "id", "type": "Row Number"}])}, "timeout": 10},
        {"key": MOCKAROO_KEY, "endpoint_name": "Status", "url": "https://api.mockaroo.com/api/status", "method": "GET", "key_field": "key", "key_location": "param", "timeout": 10}
    ],
    "OPENPAGERANK": [
        {"key": OPENPAGERANK_KEY, "endpoint_name": "Domain Rank", "url": "https://openpagerank.com/api/v1.0/getPageRank", "method": "GET", "key_field": "API-OPR", "key_location": "header", "fixed_params": {"domains[]": "google.com"}, "timeout": 10}
    ],
    "RAPIDAPI": [
        {"key": RAPIDAPI_KEY, "endpoint_name": "Programming Joke", "url": "https://jokeapi-v2.p.rapidapi.com/joke/Programming", "method": "GET", "key_field": "X-RapidAPI-Key", "key_location": "header", "fixed_headers": {"X-RapidAPI-Host": "jokeapi-v2.p.rapidapi.com"}, "timeout": 10},
        {"key": RAPIDAPI_KEY, "endpoint_name": "Currency List Quotes", "url": "https://currency-exchange.p.rapidapi.com/listquotes", "method": "GET", "key_field": "X-RapidAPI-Key", "key_location": "header", "fixed_headers": {"X-RapidAPI-Host": "currency-exchange.p.rapidapi.com"}, "timeout": 10},
        {"key": RAPIDAPI_KEY, "endpoint_name": "Random Fact", "url": "https://random-facts2.p.rapidapi.com/getfact", "method": "GET", "key_field": "X-RapidAPI-Key", "key_location": "header", "fixed_headers": {"X-RapidAPI-Host": "random-facts2.p.rapidapi.com"}, "timeout": 10}
    ],
    "OCR_API": [ # Note: This is for the generic APIClient, OCRApiClient class uses OCR_API_KEY directly
        {"key": OCR_API_KEYS[0], "endpoint_name": "OCR Space (Key 1)", "url": "https://api.ocr.space/parse/image", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"base64Image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="}, "timeout": 30},
        {"key": OCR_API_KEYS[1], "endpoint_name": "OCR Space (Key 2)", "url": "https://api.ocr.space/parse/image", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"base64Image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="}, "timeout": 30},
        {"key": OCR_API_KEYS[2], "endpoint_name": "OCR Space (Key 3)", "url": "https://api.ocr.space/parse/image", "method": "POST", "key_field": "apikey", "key_location": "header", "health_check_json": {"base64Image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="}, "timeout": 30},
    ]
}

# ==== Quotas API (Définitions des limites pour le QuotaManager) ====
# Ces valeurs sont utilisées pour le suivi et la gestion des quotas d'utilisation.
# Mettre None pour indiquer une limite illimitée.
API_QUOTAS = {
    "gemini": {
        "monthly": 1000000, # Exemple: 1 million de tokens par mois
        "daily": 50000,    # Exemple: 50 000 tokens par jour
        "hourly": 5000,    # Exemple: 5 000 tokens par heure
        "rate_limit_per_sec": 5 # Exemple: 5 requêtes par seconde
    },
    "ocr_space": { # Nom interne utilisé par OCRApiClient
        "monthly": 25000,  # Exemple: 25 000 requêtes par mois (free tier)
        "daily": None,
        "hourly": None,
        "rate_limit_per_sec": 1 # Exemple: 1 requête par seconde
    },
    # Ajouter les quotas pour toutes les APIs listées dans API_CONFIG si elles ont des limites
    # Utiliser les valeurs du premier snippet si non spécifiées ici
    "APIFLASH": {"monthly": 100, "daily": 3, "hourly": 3},
    "DEEPSEEK": {"monthly": None, "hourly": 50},
    "CRAWLBASE": {"monthly": 1000, "daily": 33, "hourly": 1},
    "DETECTLANGUAGE": {"daily": 1000, "hourly": 41},
    "GUARDIAN": {"daily": 5000, "rate_limit_per_sec": 12},
    "IP2LOCATION": {"monthly": 50, "daily": 2, "hourly": 2},
    "SERPER": {"monthly": 2500, "daily": 83, "hourly": 3},
    "SHODAN": {"monthly": 100, "daily": 3, "hourly": 3},
    "TAVILY": {"monthly": 1000, "daily": 33, "hourly": 1},
    "WEATHERAPI": {"monthly": None},
    "WOLFRAMALPHA": {"monthly": None, "hourly": 67},
    "CLOUDMERSIVE": {"monthly": 25, "daily": 1, "hourly": 1},
    "GREYNOISE": {"monthly": 100, "daily": 3, "hourly": 3},
    "PULSEDIVE": {"monthly": 50, "daily": 2, "hourly": 2},
    "STORMGLASS": {"monthly": None},
    "LOGINRADIUS": {"monthly": 25000, "daily": 833, "hourly": 34},
    "JSONBIN": {"monthly": 10000, "daily": 333, "hourly": 13},
    "HUGGINGFACE": {"hourly": 100},
    "TWILIO": {"monthly": 15},
    "ABSTRACTAPI": {"monthly": 250, "rate_limit_per_sec": 1, "daily": 8, "hourly": 1},
    "MOCKAROO": {"monthly": 200, "daily": 7, "hourly": 1},
    "OPENPAGERANK": {"monthly": 1000, "daily": 33, "hourly": 1},
    "RAPIDAPI": {"monthly": None, "hourly": 30},
    "GOOGLE_CUSTOM_SEARCH": {"daily": 100, "hourly": 4},
    "RANDOMMER": {"monthly": 1000, "daily": 100, "hourly": 4},
    "TOMORROW.IO": {"monthly": None},
    "OPENWEATHERMAP": {"monthly": 1000000, "daily": 100, "hourly": 4},
    # "OCR_API" est déjà géré par "ocr_space" pour la classe dédiée.
    # Si d'autres clients OCR sont ajoutés via APIClient, ils devraient être listés ici.
}


# Modèle Gemini et ses paramètres
GEMINI_MODEL_NAME = "gemini-1.5-flash-latest" # Ou "gemini-pro"
GEMINI_TEMPERATURE = 0.7
GEMINI_TOP_P = 0.95
GEMINI_TOP_K = 40
GEMINI_MAX_OUTPUT_TOKENS = 2048
GEMINI_SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]


# ==== Bot Behavior Configuration ====
API_COOLDOWN_DURATION_SECONDS = 300 # Durée du cooldown en secondes (5 minutes)
API_ROTATION_INTERVAL_MINUTES = 10 # Intervalle de rotation des APIs en minutes

# Quota Burning Configuration
# Ratio de quota restant en dessous duquel le mode "brûlage" s'active (ex: 0.2 signifie 20% ou moins restant)
BURN_QUOTA_THRESHOLD_RATIO = 0.2
# Fenêtre de temps avant la réinitialisation du quota où le "brûlage" peut s'activer (en heures)
BURN_QUOTA_BEFORE_RESET_HOURS = 6

# Prompts pour l'auto-génération de contenu technique afin de "brûler" le quota.
# Ces prompts sont choisis aléatoirement pour les APIs en mode "burn".
AUTO_BURN_PROMPTS = {
    "gemini": [
        "Génère un script Python qui utilise une API REST pour récupérer des données et les stocker dans une base de données NoSQL.",
        "Explique en détail les principes de l'architecture microservices et comment ils se comparent aux architectures monolithiques.",
        "Décris les étapes pour déployer une application web Flask sur un serveur cloud (AWS EC2 ou Google Cloud Run).",
        "Écris un tutoriel sur l'utilisation de Docker Compose pour orchestrer plusieurs conteneurs (par exemple, une application web et une base de données).",
        "Analyse les avantages et les inconvénients des bases de données relationnelles vs non-relationnelles pour un projet de grande envergure.",
        "Propose un plan de test complet pour une application web critique, incluant tests unitaires, d'intégration, de bout en bout et de performance.",
        "Génère un exemple de code JavaScript pour une application React qui gère l'état avec Redux ou Context API.",
        "Explique le concept de CI/CD (intégration et livraison continues) et son importance dans le développement logiciel moderne.",
        "Décris les meilleures pratiques de sécurité pour une API RESTful, incluant l'authentification, l'autorisation et la protection contre les attaques courantes.",
        "Écris un algorithme de tri efficace (par exemple, Quicksort ou Mergesort) et explique sa complexité temporelle et spatiale."
    ],
    "ocr_space": [
        "Décris les défis techniques de l'OCR sur des documents manuscrits et les approches modernes pour les surmonter.",
        "Explique comment l'OCR peut être utilisée dans le domaine de la gestion documentaire ou de l'automatisation des processus métier.",
        "Quelles sont les métriques d'évaluation courantes pour les performances d'un système OCR ?",
        "Comment la pré-traitement d'image (bruit, binarisation, redressement) affecte-t-il la précision de l'OCR ?",
        "Compare les différentes technologies OCR disponibles sur le marché (cloud vs on-premise, open-source vs propriétaires)."
    ]
}

# ==== Paramètres de Sécurité et Filtrage ====
FORBIDDEN_WORDS = ["fuck", "shit", "bitch", "asshole", "pute", "enculé", "haine", "stupide", "détruire", "conflit", "malveillance", "idiot", "nul", "débile"]

# ==== IA PROMPTS (Exemples, à affiner selon tes besoins spécifiques pour chaque IA) ====
GENERAL_IA_PROMPT = """
Tu es une IA de l'année 2025, experte en information, programmation et résolution de problèmes.
Ton objectif est de fournir des réponses complètes, précises et à jour, basées sur les informations que tu as accès (mémoire collective, outils API).
Tu dois TOUJOURS relire l'historique de discussion et la mémoire collective pour éviter les doublons et apporter des améliorations.
Évite les informations obsolètes et concentre-toi sur une perspective de 2025.
Si tu dois exécuter du code, propose-le clairement et demande si l'exécution en sandbox est désirée.
N'hésite pas à croiser les informations de plusieurs sources.
"""

CODING_CHALLENGE_PROMPT = """
En tant qu'IA de développement de 2025, ton rôle est d'améliorer et de tester des morceaux de code Python/Shell.
Tu as accès à une sandbox sécurisée pour exécuter le code.
Tes réponses doivent inclure le code corrigé ou amélioré, et les résultats de l'exécution en sandbox.
Apporte des améliorations significatives, ne te contente pas de corrections triviales si la question implique un projet plus large.
Pense à l'efficacité du code et à l'optimisation des ressources.
Chaque version doit être une amélioration nette de la précédente, inédite.
Commence par un commentaire indiquant ce qui a été amélioré.
Le code doit être direct, lisible, et prêt à être utilisé.
"""

# ==== Tool Reformulation Configuration ====
TOOL_RETRY_MAX_ATTEMPTS = 3

import os
import json
import gzip
import shutil
import hashlib
import difflib
import re
import logging
import io
import contextlib
import fcntl
import traceback
import asyncio
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Union, List, Dict

# Import des constantes depuis config.py
from config import BASE_DIR, MAX_FILE_SIZE, ERROR_LOG_PATH, MAX_CACHE_SIZE, FORBIDDEN_WORDS

# Configure logging pour tout le script
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Global lock for file operations to ensure atomic writes and prevent race conditions
_file_lock = None 

def set_file_lock(lock_instance: asyncio.Lock):
    """
    Permet d'injecter l'instance d'asyncio.Lock après l'initialisation de l'event loop.
    Ceci est crucial pour la gestion des accès concurrents aux fichiers.
    """
    global _file_lock
    _file_lock = lock_instance

def _acquire_file_lock_sync(f):
    """
    Acquires an exclusive lock on a file using fcntl (Unix-like systems).
    This prevents other processes from writing to the file simultaneously.
    """
    try:
        if os.name == 'posix' and fcntl:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
    except Exception as e:
        logging.warning(f"Could not acquire file lock: {e}")

def _release_file_lock_sync(f):
    """
    Releases an exclusive lock on a file using fcntl (Unix-like systems).
    """
    try:
        if os.name == 'posix' and fcntl:
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
    except Exception as e:
        logging.warning(f"Could not release file lock: {e}")

def get_user_dir(uid: Union[int, str]) -> Path:
    """
    Retourne le répertoire de sauvegarde spécifique à un utilisateur, le créant si nécessaire.
    Chaque utilisateur (ou groupe privé) a son propre répertoire pour stocker les données.
    """
    p = BASE_DIR / str(uid)
    p.mkdir(parents=True, exist_ok=True)
    return p

def rotate_log_if_needed(path: Path):
    """
    Fait pivoter le fichier log (ou tout fichier de données) si sa taille dépasse MAX_FILE_SIZE.
    Un nouveau fichier est créé avec un horodatage pour l'archivage, et le fichier original est réinitialisé.
    """
    if path.exists() and path.stat().st_size > MAX_FILE_SIZE:
        timestamp = int(datetime.now().timestamp())
        # Renomme le fichier existant pour l'archiver
        new_path = path.with_suffix(f".old_{timestamp}{path.suffix}.gz")
        try:
            # Compresse et déplace l'ancien fichier
            with open(path, "rb") as f_in, gzip.open(new_path, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out)
            path.unlink() # Supprime l'original
            logging.info(f"Log rotated and compressed: {path} -> {new_path}")
        except Exception as e:
            log_message(f"[Rotation log] Erreur lors de la compression/rotation de {path}: {e}\n{traceback.format_exc()}", level="error")
        # Le fichier sera recréé vide lors de la prochaine sauvegarde

def compress_if_large(path: Path):
    """
    Compresse le fichier s'il dépasse 1MB après une écriture, puis le renomme pour garder le nom original.
    Ceci est une mesure de gestion de l'espace disque pour les fichiers de données.
    """
    try:
        # Vérifie si le fichier existe et si sa taille est supérieure à 1MB
        if path.exists() and path.stat().st_size > 1_000_000:
            gz_path = path.with_suffix(path.suffix + ".gz") # Chemin temporaire pour le fichier compressé
            with open(path, "rb") as f_in, gzip.open(gz_path, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out) # Copie et compresse
            path.unlink() # Supprime le fichier original non compressé
            # Renomme le fichier .gz pour qu'il ait le nom original, simulant une compression "in-place"
            gz_path.rename(path) 
            logging.info(f"File compressed: {path}")
    except Exception as e:
        log_message(f"[Compression auto] Erreur : {e}\n{traceback.format_exc()}", level="error")

async def load_json(filepath: Path, default_value: Union[Dict, List] = None) -> Union[Dict, List]:
    """
    Charge un fichier JSON de manière asynchrone.
    Retourne `default_value` si le fichier n'existe pas, est vide ou est corrompu.
    Utilise un verrou global pour la sécurité des accès concurrents.
    """
    if default_value is None:
        default_value = {} # Default to empty dict if not specified

    if _file_lock:
        async with _file_lock:
            return _load_json_sync(filepath, default_value)
    else:
        # Fallback for synchronous loading if lock is not set (e.g., during early initialization)
        return _load_json_sync(filepath, default_value)

def _load_json_sync(filepath: Path, default_value: Union[Dict, List]) -> Union[Dict, List]:
    """
    Charge un fichier JSON de manière synchrone avec verrouillage de fichier.
    """
    if not filepath.exists():
        logging.info(f"Fichier non trouvé: {filepath}. Création d'un fichier vide.")
        _save_json_sync(filepath, default_value) # Crée un fichier vide avec la valeur par défaut
        return default_value
    
    # Ouvre le fichier en mode lecture/écriture pour pouvoir le vider en cas de corruption
    with open(filepath, 'r+', encoding='utf-8') as f:
        _acquire_file_lock_sync(f) # Acquiert le verrou avant de lire
        try:
            f.seek(0) # Se positionne au début du fichier
            content = f.read()
            if not content:
                logging.warning(f"Fichier vide: {filepath}. Retourne la valeur par défaut.")
                return default_value
            return json.loads(content)
        except json.JSONDecodeError as e:
            logging.error(f"Erreur de décodage JSON dans {filepath}: {e}. Le fichier sera réinitialisé.")
            # Si le fichier est corrompu, le réinitialise avec la valeur par défaut
            f.seek(0)
            f.truncate()
            json.dump(default_value, f, indent=4, ensure_ascii=False)
            return default_value
        except Exception as e:
            logging.error(f"Erreur inattendue lors du chargement de {filepath}: {e}. Retourne la valeur par défaut.")
            return default_value
        finally:
            _release_file_lock_sync(f) # Relâche le verrou

async def save_json(filepath: Path, data: Union[Dict, List]):
    """
    Sauvegarde les données dans un fichier JSON de manière asynchrone et atomique,
    avec rotation et compression si nécessaire.
    Utilise un verrou global pour la sécurité des accès concurrents.
    """
    if _file_lock:
        async with _file_lock:
            _save_json_sync(filepath, data)
    else:
        # Fallback for synchronous saving if lock is not set
        _save_json_sync(filepath, data)

def _save_json_sync(filepath: Path, data: Union[Dict, List]):
    """
    Sauvegarde les données dans un fichier JSON de manière synchrone et atomique,
    avec verrouillage de fichier.
    """
    rotate_log_if_needed(filepath) # Effectue la rotation avant la sauvegarde
    temp_filepath = filepath.with_suffix(filepath.suffix + ".tmp") # Utilise un fichier temporaire pour l'atomicité
    try:
        with temp_filepath.open('w', encoding='utf-8') as f:
            _acquire_file_lock_sync(f) # Acquiert le verrou avant d'écrire
            try:
                json.dump(data, f, indent=4, ensure_ascii=False)
            finally:
                _release_file_lock_sync(f) # Relâche le verrou
        os.replace(temp_filepath, filepath) # Remplace l'ancien fichier par le nouveau de manière atomique
        compress_if_large(filepath) # Compresse le fichier après la sauvegarde si nécessaire
    except Exception as e:
        logging.error(f"Erreur lors de la sauvegarde atomique de {filepath}: {e}")
        if temp_filepath.exists():
            os.remove(temp_filepath) # Nettoie le fichier temporaire en cas d'erreur

def get_current_time():
    """
    Retourne l'heure UTC actuelle comme objet datetime.
    """
    return datetime.now(timezone.utc) # Utilise datetime.now(timezone.utc) pour être explicite sur UTC

def format_datetime(dt_obj: datetime) -> str:
    """
    Formate un objet datetime en chaîne de caractères lisible (UTC).
    """
    return dt_obj.strftime("%Y-%m-%d %H:%M:%S UTC")

def is_within_time_window(target_time: datetime, start_minutes_before: int, end_minutes_after: int) -> bool:
    """
    Vérifie si l'heure actuelle est dans une fenêtre de temps spécifiée autour d'une heure cible.
    """
    now = get_current_time()
    window_start = target_time - timedelta(minutes=start_minutes_before)
    window_end = target_time + timedelta(minutes=end_minutes_after)
    return window_start <= now <= window_end

def log_message(message: str, level: str = "info"):
    """
    Log un message avec un niveau spécifié.
    Les messages d'erreur critiques sont dirigés vers un fichier de log d'erreurs séparé.
    """
    if level == "error":
        error_logger = logging.getLogger("erreurs_api")
        if not error_logger.handlers: # Configurer le logger d'erreurs si pas déjà fait
            eh = logging.FileHandler(ERROR_LOG_PATH, encoding="utf-8")
            eh.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S"))
            error_logger.addHandler(eh)
            error_logger.setLevel(logging.ERROR)
        error_logger.error(message)
    else:
        # Utilise le logger par défaut pour les autres niveaux
        if level == "info":
            logging.info(message)
        elif level == "warning":
            logging.warning(message)
        elif level == "debug":
            logging.debug(message)
        else:
            logging.debug(message) # Fallback pour les niveaux non reconnus

def neutralize_urls(text: str) -> str:
    """
    Remplace les URLs dans le texte par un placeholder pour prévenir les problèmes de lien direct
    et la fuite d'informations sensibles dans les logs ou la mémoire.
    """
    # Remplace http(s):// par hxxp(s)://
    text = re.sub(r"https?://", lambda m: m.group(0).replace("t", "x", 1), text)
    # Remplace www. par wxx.
    text = re.sub(r"www\.", "wxx.", text)
    # Remplace .com, .net, .org par [.]com, [.]net, [.]org
    text = re.sub(r"\.com", "[.]com", text)
    text = re.sub(r"\.net", "[.]net", text)
    text = re.sub(r"\.org", "[.]org", text)
    return text

def clean_html_tags(text: str) -> str:
    """
    Supprime les balises HTML d'une chaîne de caractères en utilisant une expression régulière.
    """
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

def hash_text(t: str) -> str:
    """
    Calcule le hachage SHA256 d'une chaîne de caractères.
    """
    return hashlib.sha256(t.encode('utf-8')).hexdigest()

def extract_keywords(text: str) -> List[str]:
    """
    Extrait les mots-clés les plus fréquents d'un texte.
    Retourne une liste des 5 mots les plus fréquents (de 4 caractères ou plus).
    """
    # Trouve tous les mots de 4 caractères ou plus (incluant les accents)
    words = re.findall(r'\b[a-zA-Zéèêôàùçîïœ]{4,}\b', text.lower())
    freq = {}
    for w in words:
        freq[w] = freq.get(w, 0) + 1
    # Trie les mots par fréquence décroissante
    keywords = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [w for w,_ in keywords[:5]] # Retourne seulement les mots

def tag_conversation(text: str) -> str:
    """
    Génère un tag de conversation basé sur les mots-clés extraits du texte.
    """
    words = extract_keywords(text)
    return f"#tags : {', '.join(words)}"

def unique_preserve_order(seq: List[Any]) -> List[Any]:
    """
    Élimine les doublons d'une séquence tout en préservant l'ordre original des éléments.
    """
    seen = set()
    result = []
    for item in seq:
        if item not in seen:
            seen.add(item)
            result.append(item)
    return result

def similar(a: str, b: str) -> float:
    """
    Calcule la similarité entre deux chaînes de caractères en utilisant le ratio de SequenceMatcher.
    Retourne un ratio de 0 à 1, où 1 indique une identité parfaite.
    """
    return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio()

def is_code(text: str) -> bool:
    """
    Détecte si le texte ressemble à du code (Python ou autre) en cherchant des motifs courants.
    """
    return bool(re.search(r"^\s*(def |class |import |print\()", text, re.MULTILINE)) or text.strip().startswith("```")

def is_python_code_block(text: str) -> bool:
    """
    Détecte si le texte est un bloc de code Python formaté en Markdown.
    """
    return text.strip().startswith("```python") and text.strip().endswith("```")

import time
import httpx
import json
import base64
import asyncio
import re # Importation ajoutée pour la validation IP dans ShodanClient
from typing import Dict, Any, Optional, Union, List, Tuple

# Import des constantes et fonctions utilitaires depuis les modules locaux
from config import API_CONFIG, ENDPOINT_HEALTH_FILE, OCR_API_KEYS, GEMINI_API_KEY # Assurez-vous que GEMINI_API_KEY est importé
from utils import load_json, save_json, get_current_time, format_datetime, log_message, neutralize_urls

class EndpointHealthManager:
    """
    Gère la santé des endpoints API et sélectionne le meilleur endpoint disponible
    en fonction de critères comme la latence, le taux de succès et le nombre d'erreurs.
    C'est un singleton pour s'assurer qu'il n'y a qu'une seule instance de gestionnaire de santé.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Implémente le patron de conception Singleton."""
        if cls._instance is None:
            cls._instance = super(cls, cls).__new__(cls)
            cls._instance._initialized = False # Indicateur pour l'initialisation asynchrone
        return cls._instance

    def __init__(self):
        """Initialise le gestionnaire. L'initialisation réelle se fait via `init_manager`."""
        if self._initialized:
            return
        self.health_status = {}
        # `_initialized` est géré par `init_manager` pour les opérations asynchrones

    async def init_manager(self):
        """
        Initialise le gestionnaire de santé de manière asynchrone.
        Charge l'état de santé persistant et s'assure que tous les endpoints sont suivis.
        """
        if not self._initialized:
            self.health_status = await load_json(ENDPOINT_HEALTH_FILE, {})
            self._initialize_health_status()
            self._initialized = True
            log_message("Gestionnaire de santé des endpoints initialisé.")

    def _initialize_health_status(self):
        """
        Initialise ou met à jour le statut de santé pour tous les endpoints configurés dans `API_CONFIG`.
        Ajoute les nouveaux endpoints et s'assure que toutes les clés nécessaires sont présentes.
        """
        updated = False
        for service_name, endpoints_config in API_CONFIG.items():
            if service_name not in self.health_status:
                self.health_status[service_name] = {}
                updated = True
            for endpoint_config in endpoints_config:
                # Crée une clé unique pour chaque endpoint en combinant son nom et sa clé API
                endpoint_key = f"{endpoint_config['endpoint_name']}-{str(endpoint_config['key'])}"
                if endpoint_key not in self.health_status[service_name]:
                    self.health_status[service_name][endpoint_key] = {
                        "latency": 0.0,
                        "success_rate": 1.0, # Commence avec un taux de succès parfait (sain)
                        "last_checked": None,
                        "error_count": 0,
                        "total_checks": 0,
                        "is_healthy": True # Présumé sain au début
                    }
                    updated = True
        if updated:
            # Sauvegarde l'état mis à jour de manière asynchrone en tâche de fond
            asyncio.create_task(save_json(ENDPOINT_HEALTH_FILE, self.health_status))
            log_message("Statut de santé des endpoints initialisé/mis à jour.")

    async def run_health_check_for_service(self, service_name: str):
        """
        Exécute des checks de santé pour tous les endpoints d'un service donné.
        Tente d'appeler l'endpoint avec des paramètres de santé prédéfinis.
        """
        endpoints_config = API_CONFIG.get(service_name)
        if not endpoints_config:
            log_message(f"Aucune configuration d'endpoint trouvée pour le service: {service_name}", level="warning")
            return

        log_message(f"Lancement du health check pour le service: {service_name}")
        for endpoint_config in endpoints_config:
            endpoint_key = f"{endpoint_config['endpoint_name']}-{str(endpoint_config['key'])}"
            start_time = time.monotonic()
            success = False
            try:
                request_method = endpoint_config.get("method", "GET")
                url = endpoint_config["url"]
                
                # Prépare les paramètres/données pour le health check
                params = endpoint_config.get("health_check_params", endpoint_config.get("fixed_params", {})).copy()
                json_data = endpoint_config.get("health_check_json", endpoint_config.get("fixed_json", {})).copy()
                headers = endpoint_config.get("fixed_headers", {}).copy()
                auth = None # Pour l'authentification de base (Basic Auth)
                
                check_timeout = endpoint_config.get("timeout", 5) # Timeout spécifique pour le health check

                # Ajoute un suffixe à l'URL si spécifié (ex: pour les APIs basées sur des chemins d'accès)
                if "health_check_url_suffix" in endpoint_config:
                    url += endpoint_config["health_check_url_suffix"]

                # Gère l'insertion de la clé API selon sa localisation (paramètre, en-tête, Basic Auth)
                key_field = endpoint_config.get("key_field")
                key_location = endpoint_config.get("key_location")
                key_prefix = endpoint_config.get("key_prefix", "") # Préfixe pour les clés dans les en-têtes (ex: "Bearer ")
                api_key = endpoint_config["key"]

                if key_field and key_location:
                    if key_location == "param":
                        params[key_field] = api_key
                    elif key_location == "header":
                        headers[key_field] = f"{key_prefix}{api_key}"
                    elif key_location == "auth_basic":
                        if isinstance(api_key, tuple) and len(api_key) == 2:
                            auth = httpx.BasicAuth(api_key[0], api_key[1])
                        else:
                            log_message(f"Clé API pour auth_basic non valide pour {service_name}:{endpoint_key}", level="error")
                            success = False
                            continue # Passe à l'endpoint suivant

                async with httpx.AsyncClient(timeout=check_timeout) as client:
                    response = await client.request(request_method, url, params=params, headers=headers, json=json_data, auth=auth)
                    response.raise_for_status() # Lève une exception pour les codes d'état HTTP 4xx/5xx
                    success = True
            except httpx.HTTPStatusError as e:
                log_level = "warning"
                # Les codes 4xx (sauf 429 - Too Many Requests) indiquent souvent une erreur client
                # (clé invalide, paramètre manquant) qui ne se résoudra pas avec un réessai
                # et n'indique pas forcément un problème de "santé" du service lui-même.
                # Nous les loguons en debug pour ne pas surcharger les logs.
                if 400 <= e.response.status_code < 500 and e.response.status_code != 429:
                    log_level = "debug" 
                log_message(f"Health check pour {endpoint_key} ({service_name}) a échoué (HTTP {e.response.status_code}): {e.response.text}", level=log_level)
                success = False
            except httpx.RequestError as e:
                # Erreurs réseau (connexion, timeout, DNS, etc.)
                log_message(f"Health check pour {endpoint_key} ({service_name}) a échoué (Réseau): {e}", level="warning")
                success = False
            except Exception as e:
                # Autres erreurs inattendues
                log_message(f"Health check pour {endpoint_key} ({service_name}) a échoué (Inattendu): {e}", level="error")
                success = False
            finally:
                latency = time.monotonic() - start_time # Calcule la latence du check
                self.update_endpoint_health(service_name, endpoint_key, success, latency)
        log_message(f"Health check terminé pour le service: {service_name}")

    def update_endpoint_health(self, service_name: str, endpoint_key: str, success: bool, latency: float):
        """
        Met à jour le statut de santé d'un endpoint spécifique.
        Utilise une moyenne glissante pour le taux de succès et la latence.
        """
        # S'assure que la structure de données existe
        if service_name not in self.health_status:
            self.health_status[service_name] = {}
        if endpoint_key not in self.health_status[service_name]:
            self.health_status[service_name][endpoint_key] = {
                "latency": 0.0,
                "success_rate": 1.0,
                "last_checked": None,
                "error_count": 0,
                "total_checks": 0,
                "is_healthy": True
            }

        status = self.health_status[service_name][endpoint_key]
        status["total_checks"] += 1
        status["last_checked"] = format_datetime(get_current_time())

        alpha = 0.1 # Facteur de lissage pour les moyennes glissantes (0.1 signifie 10% de la nouvelle valeur, 90% de l'ancienne)
        if success:
            status["error_count"] = max(0, status["error_count"] - 1) # Diminue le compteur d'erreurs en cas de succès
            status["success_rate"] = status["success_rate"] * (1 - alpha) + 1.0 * alpha # Augmente le taux de succès
            status["latency"] = status["latency"] * (1 - alpha) + latency * alpha # Met à jour la latence
        else:
            status["error_count"] += 1 # Incrémente le compteur d'erreurs
            status["success_rate"] = status["success_rate"] * (1 - alpha) + 0.0 * alpha # Diminue le taux de succès
            # Si échec, pénalise la latence pour rendre l'endpoint moins attrayant (valeur arbitraire élevée)
            status["latency"] = status["latency"] * (1 - alpha) + 10.0 * alpha 

        # Détermine si l'endpoint est sain basé sur le nombre d'erreurs consécutives ou le taux de succès
        if status["error_count"] >= 3 or status["success_rate"] < 0.5:
            status["is_healthy"] = False
        else:
            status["is_healthy"] = True
        
        # Sauvegarde l'état mis à jour de manière asynchrone
        asyncio.create_task(save_json(ENDPOINT_HEALTH_FILE, self.health_status))
        log_message(f"Santé de {service_name}:{endpoint_key} mise à jour: Succès: {success}, Latence: {latency:.2f}s, Taux Succès: {status['success_rate']:.2f}, Sain: {status['is_healthy']}", level="debug" if not status["is_healthy"] else "info")

    def get_best_endpoint(self, service_name: str) -> Optional[Dict]:
        """
        Sélectionne le meilleur endpoint pour un service donné basé sur son statut de santé.
        Priorise les endpoints sains, puis les moins mauvais en cas d'absence d'endpoints sains.
        """
        service_health = self.health_status.get(service_name)
        if not service_health:
            log_message(f"Aucune donnée de santé pour le service {service_name}. Retourne None.", level="warning")
            return None

        best_endpoint_key = None
        best_score = -float('inf')

        # Filtre les endpoints actuellement considérés comme sains
        healthy_endpoints = [
            (key, status) for key, status in service_health.items() if status["is_healthy"]
        ]

        if not healthy_endpoints:
            log_message(f"Aucun endpoint sain pour le service {service_name}. Tentative de sélection d'un endpoint non sain.", level="warning")
            all_endpoints = service_health.items()
            if not all_endpoints: 
                return None # Aucun endpoint du tout
            
            # Si aucun endpoint sain, choisit le "moins mauvais" : moins d'erreurs, meilleure latence
            sorted_endpoints = sorted(all_endpoints, key=lambda item: (item[1]["error_count"], item[1]["latency"]))
            best_endpoint_key = sorted_endpoints[0][0]
            log_message(f"Fallback: Endpoint {best_endpoint_key} sélectionné pour {service_name} (non sain).", level="warning")
        else:
            # Calcule un score pour chaque endpoint sain pour choisir le meilleur
            for endpoint_key, status in healthy_endpoints:
                # Score = (Taux de succès * 100) - (Latence * 10) - (Compteur d'erreurs * 5)
                # Favorise le succès, pénalise la latence et les erreurs
                score = (status["success_rate"] * 100) - (status["latency"] * 10) - (status["error_count"] * 5)
                if score > best_score:
                    best_score = score
                    best_endpoint_key = endpoint_key
            log_message(f"Meilleur endpoint sélectionné pour {service_name}: {best_endpoint_key} (Score: {best_score:.2f})")

        if best_endpoint_key:
            # Une fois la clé du meilleur endpoint trouvée, on récupère sa configuration complète
            # depuis `API_CONFIG` pour l'utiliser dans la requête.
            for endpoint_config in API_CONFIG.get(service_name, []):
                current_endpoint_key = f"{endpoint_config['endpoint_name']}-{str(endpoint_config['key'])}"
                if current_endpoint_key == best_endpoint_key:
                    return endpoint_config
        return None # Aucun endpoint approprié trouvé

class APIClient:
    """
    Classe de base pour tous les clients API.
    Elle gère la sélection dynamique d'endpoints, les réessais en cas d'échec
    et l'intégration avec le gestionnaire de santé des endpoints.
    """
    def __init__(self, name: str, endpoint_health_manager: EndpointHealthManager):
        self.name = name
        self.endpoints_config = API_CONFIG.get(name, []) # Récupère la config des endpoints pour cette API
        self.endpoint_health_manager = endpoint_health_manager
        if not self.endpoints_config:
            log_message(f"Client API {self.name} initialisé sans configuration d'endpoint.", level="error")

    async def _make_request(self, params: Optional[Dict] = None, headers: Optional[Dict] = None, 
                            json_data: Optional[Dict] = None, timeout: Optional[int] = None, 
                            max_retries: int = 3, initial_delay: float = 1.0, 
                            url: Optional[str] = None, method: Optional[str] = None, 
                            key_field: Optional[str] = None, key_location: Optional[str] = None, 
                            api_key: Optional[Union[str, Tuple[str, str]]] = None, 
                            fixed_params: Optional[Dict] = None, fixed_headers: Optional[Dict] = None, 
                            fixed_json: Optional[Dict] = None) -> Optional[Union[Dict, str, bytes]]:
        """
        Méthode interne pour effectuer les requêtes HTTP en utilisant le meilleur endpoint avec réessais.
        
        Args:
            params (Dict, optional): Paramètres de requête à ajouter à l'URL.
            headers (Dict, optional): En-têtes HTTP supplémentaires.
            json_data (Dict, optional): Données JSON à envoyer dans le corps de la requête (pour POST/PUT).
            timeout (int, optional): Timeout pour la requête en secondes.
            max_retries (int): Nombre maximal de tentatives en cas d'échec.
            initial_delay (float): Délai initial entre les réessais (exponentiel).
            url (str, optional): URL spécifique à utiliser (si non basé sur un endpoint configuré).
            method (str, optional): Méthode HTTP (GET, POST, etc.).
            key_field (str, optional): Nom du champ pour la clé API.
            key_location (str, optional): Où placer la clé API ('param', 'header', 'auth_basic').
            api_key (Union[str, Tuple[str, str]], optional): Clé API à utiliser.
            fixed_params (Dict, optional): Paramètres fixes définis dans la configuration de l'endpoint.
            fixed_headers (Dict, optional): En-têtes fixes définis dans la configuration de l'endpoint.
            fixed_json (Dict, optional): Données JSON fixes définies dans la configuration de l'endpoint.

        Returns:
            Union[Dict, str, bytes, None]: La réponse de l'API (JSON décodé, texte brut, ou bytes pour les images),
                                          ou un dictionnaire d'erreur en cas d'échec.
        """
        
        selected_endpoint_config = None
        endpoint_key_for_health = "Dynamic" # Clé par défaut pour les requêtes dynamiques (non configurées)

        if url and method: # Si l'URL et la méthode sont fournies directement (requête dynamique)
            selected_endpoint_config = {
                "url": url,
                "method": method,
                "key_field": key_field,
                "key_location": key_location,
                "key": api_key,
                "fixed_params": fixed_params if fixed_params is not None else {},
                "fixed_headers": fixed_headers if fixed_headers is not None else {},
                "fixed_json": fixed_json if fixed_json is not None else {},
                "endpoint_name": "Dynamic", # Nom générique pour les endpoints dynamiques
                "timeout": timeout if timeout is not None else 30
            }
            if api_key:
                # Crée une clé de santé unique pour les requêtes dynamiques avec clé API
                endpoint_key_for_health = f"Dynamic-{str(api_key)}"
            log_message(f"Requête dynamique pour {self.name} vers {url}")
        else: # Sélectionne le meilleur endpoint configuré via le gestionnaire de santé
            selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
            if not selected_endpoint_config:
                log_message(f"Aucun endpoint sain ou disponible pour {self.name}.", level="error")
                return {"error": True, "message": f"Aucun endpoint sain ou disponible pour {self.name}."}
            # Construit la clé de santé pour l'endpoint sélectionné
            endpoint_key_for_health = f"{selected_endpoint_config['endpoint_name']}-{str(selected_endpoint_config['key'])}"
            log_message(f"Endpoint sélectionné pour {self.name}: {selected_endpoint_config['endpoint_name']}")
            # Utilise le timeout de la config de l'endpoint si non spécifié
            timeout = timeout if timeout is not None else selected_endpoint_config.get("timeout", 30)

        url_to_use = selected_endpoint_config["url"]
        method_to_use = selected_endpoint_config["method"]

        # Initialise les paramètres, en-têtes et données JSON avec les valeurs fixes de la config
        request_params = selected_endpoint_config.get("fixed_params", {}).copy()
        request_headers = selected_endpoint_config.get("fixed_headers", {}).copy()
        request_json_data = selected_endpoint_config.get("fixed_json", {}).copy()
        auth = None # Pour l'authentification de base

        # Fusionne les paramètres/en-têtes/données JSON fournis avec les valeurs fixes
        if params:
            request_params.update(params)
        if headers:
            request_headers.update(headers)
        if json_data:
            request_json_data.update(json_data)

        # Gère l'insertion de la clé API pour la requête
        key_field_to_use = selected_endpoint_config.get("key_field")
        key_location_to_use = selected_endpoint_config.get("key_location")
        key_prefix = selected_endpoint_config.get("key_prefix", "")
        api_key_to_use = selected_endpoint_config["key"]

        if key_field_to_use and key_location_to_use:
            if key_location_to_use == "param":
                request_params[key_field_to_use] = api_key_to_use
            elif key_location_to_use == "header":
                request_headers[key_field_to_use] = f"{key_prefix}{api_key_to_use}"
            elif key_location_to_use == "auth_basic":
                if isinstance(api_key_to_use, tuple) and len(api_key_to_use) == 2:
                    auth = httpx.BasicAuth(api_key_to_use[0], api_key_to_use[1])
                else:
                    log_message(f"Clé API pour auth_basic non valide pour {self.name}:{endpoint_key_for_health}", level="error")
                    # Marque l'endpoint comme non sain et retourne une erreur
                    self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, 0.0)
                    return {"error": True, "message": "Configuration d'authentification basique invalide."}

        current_delay = initial_delay # Délai initial pour les réessais
        for attempt in range(max_retries):
            start_time = time.monotonic()
            success = False
            try:
                async with httpx.AsyncClient(timeout=timeout) as client:
                    response = await client.request(method_to_use, url_to_use, params=request_params, headers=request_headers, json=request_json_data, auth=auth)
                    response.raise_for_status() # Lève une exception pour les codes d'état HTTP 4xx/5xx
                    success = True
                    
                    content_type = response.headers.get("Content-Type", "").lower()
                    if "application/json" in content_type:
                        try:
                            return response.json() # Tente de décoder la réponse JSON
                        except json.JSONDecodeError:
                            log_message(f"API {self.name} réponse non JSON valide (tentative {attempt+1}/{max_retries}): {response.text[:200]}...", level="warning")
                            if attempt < max_retries - 1: # Si ce n'est pas la dernière tentative, réessaie
                                await asyncio.sleep(current_delay)
                                current_delay *= 2 # Augmente le délai de manière exponentielle
                                continue
                            return {"error": True, "message": "Réponse API non JSON valide.", "raw_response": response.text}
                    else:
                        log_message(f"API {self.name} a renvoyé un Content-Type non JSON: {content_type}", level="info")
                        return response.content # Retourne le contenu brut (ex: pour les images)

            except httpx.HTTPStatusError as e:
                log_message(f"API {self.name} erreur HTTP (tentative {attempt+1}/{max_retries}): {e.response.status_code} - {e.response.text}", level="warning")
                # Ne pas réessayer pour les erreurs client (4xx) sauf 429 (Too Many Requests)
                if 400 <= e.response.status_code < 500 and e.response.status_code != 429:
                    log_message(f"API {self.name}: Erreur client {e.response.status_code}, pas de réessai.", level="error")
                    self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, e.response.elapsed.total_seconds())
                    return {"error": True, "status_code": e.response.status_code, "message": e.response.text}
                
                if attempt < max_retries - 1:
                    log_message(f"API {self.name}: Réessai dans {current_delay:.2f}s...", level="info")
                    await asyncio.sleep(current_delay)
                    current_delay *= 2
            except httpx.RequestError as e:
                log_message(f"API {self.name} erreur de requête (tentative {attempt+1}/{max_retries}): {e}", level="warning")
                if attempt < max_retries - 1:
                    log_message(f"API {self.name}: Réessai dans {current_delay:.2f}s...", level="info")
                    await asyncio.sleep(current_delay)
                    current_delay *= 2
            except Exception as e:
                log_message(f"API {self.name} erreur inattendue (tentative {attempt+1}/{max_retries}): {e}", level="error")
                self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, time.monotonic() - start_time)
                return {"error": True, "message": str(e)}
            finally:
                # Met à jour la santé de l'endpoint même en cas d'échec pour la sélection future
                if not success:
                    latency = time.monotonic() - start_time
                    self.endpoint_health_manager.update_endpoint_health(self.name, endpoint_key_for_health, False, latency)
        
        log_message(f"API {self.name}: Toutes les tentatives ont échoué après {max_retries} réessais.", level="error")
        return {"error": True, "message": f"Échec de la requête après {max_retries} tentatives."}

    async def query(self, *args, **kwargs) -> Any:
        """
        Méthode abstraite pour interroger l'API.
        Doit être implémentée par chaque sous-classe de client API.
        """
        raise NotImplementedError("La méthode query doit être implémentée par les sous-classes.")

# Instancier le gestionnaire de santé des endpoints (sera initialisé dans main.py)
endpoint_health_manager = EndpointHealthManager()

def set_endpoint_health_manager_global(manager: EndpointHealthManager):
    """
    Permet d'injecter l'instance du gestionnaire de santé des endpoints.
    Ceci est utilisé pour s'assurer que tous les clients API utilisent la même instance.
    """
    global endpoint_health_manager
    endpoint_health_manager = manager

# --- Clients API Spécifiques ---
# Chaque classe de client API hérite de `APIClient` et implémente la méthode `query`
# pour interagir avec une API spécifique.

class DeepSeekClient(APIClient):
    def __init__(self):
        super().__init__("DEEPSEEK", endpoint_health_manager)

    async def query(self, prompt: str, model: str = "deepseek-chat") -> str:
        """Interroge l'API DeepSeek pour des complétions de chat."""
        payload = {"model": model, "messages": [{"role": "user", "content": prompt}]}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            content = response.get("choices", [{}])[0].get("message", {}).get("content")
            if content:
                return content
            return "DeepSeek: Pas de contenu de réponse trouvé."
        return f"DeepSeek: Erreur: {response.get('message', 'Inconnu')}" if response else "DeepSeek: Réponse vide ou erreur interne."

class SerperClient(APIClient):
    def __init__(self):
        super().__init__("SERPER", endpoint_health_manager)

    async def query(self, query_text: str) -> str:
        """Effectue une recherche web via l'API Serper."""
        payload = {"q": query_text}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            organic_results = response.get("organic", [])
            if organic_results:
                snippet = organic_results[0].get("snippet", "Pas de snippet.")
                link = organic_results[0].get("link", "")
                return f"Serper (recherche web):\n{snippet} {neutralize_urls(link)}"
            return "Serper: Aucune information trouvée."
        return f"Serper: Erreur: {response.get('message', 'Inconnu')}" if response else "Serper: Réponse vide ou erreur interne."

class WolframAlphaClient(APIClient):
    def __init__(self):
        super().__init__("WOLFRAMALPHA", endpoint_health_manager)

    async def query(self, input_text: str) -> str:
        """Interroge WolframAlpha pour des calculs ou des faits."""
        params = {"input": input_text}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            pods = response.get("queryresult", {}).get("pods", [])
            if pods:
                for pod in pods:
                    # Tente de trouver les pods les plus pertinents pour un résultat direct
                    if pod.get("title") in ["Result", "Input interpretation", "Decimal approximation"]:
                        subpods = pod.get("subpods", [])
                        if subpods and subpods[0].get("plaintext"):
                            return f"WolframAlpha:\n{subpods[0]['plaintext']}"
                # Fallback: prend le premier pod si aucun pod pertinent n'est trouvé
                if pods and pods[0].get("subpods") and pods[0]["subpods"][0].get("plaintext"):
                    return f"WolframAlpha:\n{pods[0]['subpods'][0]['plaintext']}"
            return "WolframAlpha: Pas de résultat clair."
        return f"WolframAlpha: Erreur: {response.get('message', 'Inconnu')}" if response else "WolframAlpha: Réponse vide ou erreur interne."

class TavilyClient(APIClient):
    def __init__(self):
        super().__init__("TAVILY", endpoint_health_manager)

    async def query(self, query_text: str, max_results: int = 3) -> str:
        """Effectue une recherche web avancée via l'API Tavily."""
        payload = {"query": query_text, "max_results": max_results}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            results = response.get("results", [])
            answer = response.get("answer", "Aucune réponse directe trouvée.")

            output = f"Tavily (recherche web):\nRéponse directe: {answer}\n"
            if results:
                output += "Extraits pertinents:\n"
                for i, res in enumerate(results[:max_results]):
                    output += f"- {res.get('title', 'N/A')}: {res.get('content', 'N/A')} {neutralize_urls(res.get('url', ''))}\n"
            return output
        return f"Tavily: Erreur: {response.get('message', 'Inconnu')}" if response else "Tavily: Réponse vide ou erreur interne."

class ApiFlashClient(APIClient):
    def __init__(self):
        super().__init__("APIFLASH", endpoint_health_manager)

    async def query(self, url: str) -> str:
        """Capture une capture d'écran d'une URL via ApiFlash."""
        params = {"url": url, "format": "jpeg", "full_page": "true"}
        response_content = await self._make_request(params=params)

        if isinstance(response_content, bytes):
            selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
            if selected_endpoint_config:
                # ApiFlash retourne l'image directement. On peut construire une URL de prévisualisation
                # si l'API le permet, ou indiquer que la capture a été faite.
                # Ici, nous construisons une URL potentielle de l'image capturée.
                capture_url = f"{selected_endpoint_config['url']}?access_key={selected_endpoint_config['key']}&url={url}&format=jpeg&full_page=true"
                return f"ApiFlash (capture d'écran): {neutralize_urls(capture_url)} (Vérifiez le lien pour l'image)"
            return "ApiFlash: Impossible de générer l'URL de capture."
        elif isinstance(response_content, dict) and response_content.get("error"):
            return f"ApiFlash: Erreur: {response_content.get('message', 'Inconnu')}"
        else:
            log_message(f"ApiFlash a renvoyé un type de réponse inattendu: {type(response_content)}", level="warning")
            return f"ApiFlash: Réponse inattendue de l'API. {response_content}"

class CrawlbaseClient(APIClient):
    def __init__(self):
        super().__init__("CRAWLBASE", endpoint_health_manager)

    async def query(self, url: str, use_js: bool = False) -> str:
        """Scrape le contenu HTML ou JavaScript d'une URL via Crawlbase."""
        params = {"url": url, "format": "json"}
        
        selected_endpoint_config = None
        if use_js:
            # Tente de trouver un endpoint spécifiquement configuré pour le scraping JS
            for config in API_CONFIG.get(self.name, []):
                if "JS Scraping" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break
        
        # Fallback si pas de config JS spécifique ou si use_js est False
        if not selected_endpoint_config: 
            selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)

        if not selected_endpoint_config:
            return f"Crawlbase: Aucun endpoint sain ou disponible pour {self.name}."

        response = await self._make_request(
            params=params,
            url=selected_endpoint_config["url"],
            method=selected_endpoint_config["method"],
            key_field=selected_endpoint_config["key_field"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            fixed_params=selected_endpoint_config.get("fixed_params", {}),
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            body = response.get("body")
            if body:
                try:
                    # Le corps est encodé en base64, il faut le décoder
                    decoded_body = base64.b64decode(body).decode('utf-8', errors='ignore')
                    return f"Crawlbase (contenu web):\n{decoded_body[:1000]}..." # Retourne les 1000 premiers caractères
                except Exception:
                    return f"Crawlbase (contenu web - brut):\n{body[:1000]}..." # En cas d'échec du décodage
            return "Crawlbase: Contenu non trouvé."
        return f"Crawlbase: Erreur: {response.get('message', 'Inconnu')}" if response else "Crawlbase: Réponse vide ou erreur interne."

class DetectLanguageClient(APIClient):
    def __init__(self):
        super().__init__("DETECTLANGUAGE", endpoint_health_manager)

    async def query(self, text: str) -> str:
        """Détecte la langue d'un texte via DetectLanguage API."""
        payload = {"q": text}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            detections = response.get("data", {}).get("detections", [])
            if detections:
                first_detection = detections[0]
                lang = first_detection.get("language")
                confidence = first_detection.get("confidence")
                return f"Langue détectée: {lang} (confiance: {confidence})"
            return "DetectLanguage: Aucune langue détectée."
        return f"DetectLanguage: Erreur: {response.get('message', 'Inconnu')}" if response else "DetectLanguage: Réponse vide ou erreur interne."

class GuardianClient(APIClient):
    def __init__(self):
        super().__init__("GUARDIAN", endpoint_health_manager)

    async def query(self, query_text: str) -> str:
        """Recherche des articles de presse via l'API The Guardian."""
        params = {"q": query_text}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            results = response.get("response", {}).get("results", [])
            if results:
                output = "Articles The Guardian:\n"
                for res in results[:3]: # Limite à 3 articles pour la concision
                    output += f"- {res.get('webTitle', 'N/A')}: {res.get('fields', {}).get('trailText', 'N/A')} {neutralize_urls(res.get('webUrl', ''))}\n"
                return output
            return "Guardian: Aucun article trouvé."
        return f"Guardian: Erreur: {response.get('message', 'Inconnu')}" if response else "Guardian: Réponse vide ou erreur interne."

class IP2LocationClient(APIClient):
    def __init__(self):
        super().__init__("IP2LOCATION", endpoint_health_manager)

    async def query(self, ip_address: str) -> str:
        """Géolocalise une adresse IP via IP2Location API."""
        params = {"ip": ip_address}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            if "country_name" in response:
                return f"IP2Location (Géolocalisation IP {ip_address}): Pays: {response['country_name']}, Ville: {response.get('city_name', 'N/A')}"
            return "IP2Location: Informations non trouvées."
        return f"IP2Location: Erreur: {response.get('message', 'Inconnu')}" if response else "IP2Location: Réponse vide ou erreur interne."

class ShodanClient(APIClient):
    def __init__(self):
        super().__init__("SHODAN", endpoint_health_manager)

    async def query(self, query_text: str = "") -> str:
        """
        Interroge Shodan pour des informations sur un hôte IP ou des informations sur la clé API.
        Si `query_text` est une IP, tente de récupérer les infos de l'hôte.
        Sinon, ou en cas d'échec, retourne les infos de la clé API.
        """
        if re.match(r"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$", query_text): # Vérifie si c'est une adresse IP
            selected_endpoint_config = None
            for config in API_CONFIG.get(self.name, []):
                if "Host Info" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break
            if selected_endpoint_config:
                # Construire l'URL pour la recherche d'hôte
                url = f"{selected_endpoint_config['url'].rstrip('/')}/{query_text}"
                response = await self._make_request(
                    params={"key": selected_endpoint_config["key"]},
                    url=url,
                    method="GET",
                    key_field=selected_endpoint_config["key_field"],
                    key_location=selected_endpoint_config["key_location"],
                    api_key=selected_endpoint_config["key"],
                    timeout=selected_endpoint_config.get("timeout")
                )
                if response and not response.get("error"):
                    return f"Shodan (info hôte {query_text}): Pays: {response.get('country_name', 'N/A')}, Ports: {response.get('ports', 'N/A')}, Vulnérabilités: {response.get('vulns', 'Aucune')}"
                return f"Shodan (info hôte): Erreur: {response.get('message', 'Inconnu')}" if response else "Shodan: Réponse vide ou erreur interne."
            else:
                return "Shodan: Endpoint 'Host Info' non configuré."
        else:
            # Si pas d'IP ou si la recherche d'hôte n'est pas applicable, retourne les infos de la clé API
            response = await self._make_request() # Utilise le premier endpoint disponible (API Info)
            if response and not response.get("error"):
                return f"Shodan (info clé): Requêtes restantes: {response.get('usage_limits', {}).get('query_credits', 'N/A')}, Scan crédits: {response.get('usage_limits', {}).get('scan_credits', 'N/A')}"
            return f"Shodan: Erreur: {response.get('message', 'Inconnu')}" if response else "Shodan: Réponse vide ou erreur interne."

class WeatherAPIClient(APIClient):
    def __init__(self):
        super().__init__("WEATHERAPI", endpoint_health_manager)

    async def query(self, location: str) -> str:
        """Récupère les conditions météorologiques actuelles pour une localisation via WeatherAPI."""
        params = {"q": location}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            current = response.get("current", {})
            location_info = response.get("location", {})
            if current and location_info:
                return (
                    f"Météo à {location_info.get('name', 'N/A')}, {location_info.get('country', 'N/A')}:\n"
                    f"Température: {current.get('temp_c', 'N/A')}°C, "
                    f"Conditions: {current.get('condition', {}).get('text', 'N/A')}, "
                    f"Vent: {current.get('wind_kph', 'N/A')} km/h"
                )
            return "WeatherAPI: Données météo non trouvées."
        return f"WeatherAPI: Erreur: {response.get('message', 'Inconnu')}" if response else "WeatherAPI: Réponse vide ou erreur interne."

class CloudmersiveClient(APIClient):
    def __init__(self):
        super().__init__("CLOUDMERSIVE", endpoint_health_manager)

    async def query(self, domain: str) -> str:
        """Vérifie la validité et le type d'un domaine via Cloudmersive API."""
        payload = {"domain": domain}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            return f"Cloudmersive (vérification de domaine {domain}): Valide: {response.get('ValidDomain', 'N/A')}, Type: {response.get('DomainType', 'N/A')}"
        return f"Cloudmersive: Erreur: {response.get('message', 'Inconnu')}" if response else "Cloudmersive: Réponse vide ou erreur interne."

class GreyNoiseClient(APIClient):
    def __init__(self):
        super().__init__("GREYNOISE", endpoint_health_manager)

    async def query(self, ip_address: str) -> str:
        """Analyse une adresse IP pour détecter des activités 'bruit' (malveillantes) via GreyNoise."""
        selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
        if not selected_endpoint_config:
            return f"GreyNoise: Aucun endpoint sain ou disponible pour {self.name}."

        # Construit l'URL avec l'adresse IP à la fin
        url = f"{selected_endpoint_config['url'].rstrip('/')}/{ip_address}"
        method = selected_endpoint_config["method"]
        headers = {selected_endpoint_config["key_field"]: selected_endpoint_config["key"]}

        response = await self._make_request(
            headers=headers,
            url=url,
            method=method,
            key_field=selected_endpoint_config["key_field"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if response.get("noise"):
                return f"GreyNoise (IP {ip_address}): C'est une IP 'bruit' (malveillante). Classification: {response.get('classification', 'N/A')}, Nom d'acteur: {response.get('actor', 'N/A')}"
            return f"GreyNoise (IP {ip_address}): Pas de 'bruit' détecté. Statut: {response.get('status', 'N/A')}"
        return f"GreyNoise: Erreur: {response.get('message', 'Inconnu')}" if response else "GreyNoise: Réponse vide ou erreur interne."

class PulsediveClient(APIClient):
    def __init__(self):
        super().__init__("PULSEDIVE", endpoint_health_manager)

    async def query(self, indicator: str, type: str = "auto") -> str:
        """Analyse un indicateur de menace (IP, domaine, URL) via Pulsedive."""
        params = {"indicator": indicator, "type": type}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            if response.get("results"):
                result = response["results"][0] # Prend le premier résultat
                return (
                    f"Pulsedive (Analyse {indicator}): Type: {result.get('type', 'N/A')}, "
                    f"Risk: {result.get('risk', 'N/A')}, "
                    f"Description: {result.get('description', 'N/A')[:200]}..." # Tronque la description
                )
            return "Pulsedive: Aucun résultat d'analyse trouvé."
        return f"Pulsedive: Erreur: {response.get('message', 'Inconnu')}" if response else "Pulsedive: Réponse vide ou erreur interne."

class StormGlassClient(APIClient):
    def __init__(self):
        super().__init__("STORMGLASS", endpoint_health_manager)

    async def query(self, lat: float, lng: float, params: str = "airTemperature,waveHeight") -> str:
        """Récupère les données météorologiques maritimes pour une coordonnée via StormGlass."""
        now = int(time.time())
        request_params = {
            "lat": lat,
            "lng": lng,
            "params": params,
            "start": now,
            "end": now + 3600 # Prévisions pour la prochaine heure (3600 secondes)
        }
        response = await self._make_request(params=request_params)
        if response and not response.get("error"):
            data = response.get("hours", [])
            if data:
                first_hour = data[0]
                # Accède aux valeurs spécifiques des paramètres demandés
                temp = first_hour.get('airTemperature', [{}])[0].get('value', 'N/A')
                wave_height = first_hour.get('waveHeight', [{}])[0].get('value', 'N/A')
                return f"StormGlass (Météo maritime à {lat},{lng}): Température air: {temp}°C, Hauteur vagues: {wave_height}m"
            return "StormGlass: Données non trouvées."
        return f"StormGlass: Erreur: {response.get('message', 'Inconnu')}" if response else "StormGlass: Réponse vide ou erreur interne."

class LoginRadiusClient(APIClient):
    def __init__(self):
        super().__init__("LOGINRADIUS", endpoint_health_manager)

    async def query(self) -> str:
        """Effectue un simple ping à l'API LoginRadius pour vérifier sa disponibilité."""
        response = await self._make_request()
        if response and not response.get("error"):
            return f"LoginRadius (Ping API): Statut: {response.get('Status', 'N/A')}, Message: {response.get('Message', 'N/A')}"
        return f"LoginRadius: Erreur: {response.get('message', 'Inconnu')}" if response else "LoginRadius: Réponse vide ou erreur interne."

class JsonbinClient(APIClient):
    def __init__(self):
        super().__init__("JSONBIN", endpoint_health_manager)

    async def query(self, data: Optional[Dict[str, Any]] = None, private: bool = True, bin_id: Optional[str] = None) -> str:
        """
        Crée un nouveau 'bin' JSON ou accède à un bin existant via Jsonbin.io.
        `data` est pour la création, `bin_id` pour l'accès.
        """
        if bin_id: # Accès à un bin existant
            selected_endpoint_config = None
            for config in API_CONFIG.get(self.name, []):
                if "Bin Access" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break
            if not selected_endpoint_config:
                return f"Jsonbin: Aucun endpoint d'accès de bin sain ou disponible pour {self.name}."

            url = f"{selected_endpoint_config['url'].rstrip('/')}/{bin_id}"
            method = "GET"
            headers = {selected_endpoint_config["key_field"]: selected_endpoint_config["key"]}
            
            response = await self._make_request(
                headers=headers,
                url=url,
                method=method,
                timeout=selected_endpoint_config.get("timeout")
            )
            if response and not response.get("error"):
                return f"Jsonbin (Accès bin {bin_id}):\n{json.dumps(response, indent=2)}"
            return f"Jsonbin (Accès bin): Erreur: {response.get('message', 'Inconnu')}" if response else "Jsonbin: Réponse vide ou erreur interne."
        
        else: # Création d'un nouveau bin
            selected_endpoint_config = None
            for config in API_CONFIG.get(self.name, []):
                if "Bin Create" in config.get("endpoint_name", ""):
                    selected_endpoint_config = config
                    break
            
            if not selected_endpoint_config:
                return f"Jsonbin: Aucun endpoint de création de bin sain ou disponible pour {self.name}."

            url = selected_endpoint_config["url"]
            method = "POST"
            headers = {selected_endpoint_config["key_field"]: selected_endpoint_config["key"], "Content-Type": "application/json"}
            payload = {"record": data if data is not None else {}, "private": private}

            response = await self._make_request(
                json_data=payload,
                headers=headers,
                url=url,
                method=method,
                timeout=selected_endpoint_config.get("timeout")
            )

            if response and not response.get("error"):
                return f"Jsonbin (Création de bin): ID: {response.get('metadata', {}).get('id', 'N/A')}, URL: {neutralize_urls(response.get('metadata', {}).get('url', 'N/A'))}"
            return f"Jsonbin (Création de bin): Erreur: {response.get('message', 'Inconnu')}" if response else "Jsonbin: Réponse vide ou erreur interne."

class HuggingFaceClient(APIClient):
    def __init__(self):
        super().__init__("HUGGINGFACE", endpoint_health_manager)

    async def query(self, model_name: str = "distilbert-base-uncased-finetuned-sst-2-english", input_text: str = "Hello world") -> str:
        """Effectue une inférence sur un modèle HuggingFace (ex: classification de texte, génération)."""
        selected_endpoint_config = self.endpoint_health_manager.get_best_endpoint(self.name)
        if not selected_endpoint_config:
            return f"HuggingFace: Aucun endpoint sain ou disponible pour {self.name}."

        # Construit l'URL d'inférence pour le modèle spécifié
        inference_url = f"https://api-inference.huggingface.co/models/{model_name}"
        
        headers = {
            selected_endpoint_config["key_field"]: f"{selected_endpoint_config['key_prefix']}{selected_endpoint_config['key']}",
            "Content-Type": "application/json"
        }
        payload = {"inputs": input_text}

        response = await self._make_request(
            json_data=payload,
            headers=headers,
            url=inference_url,
            method="POST",
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if isinstance(response, list) and response:
                first_result = response[0]
                if isinstance(first_result, list) and first_result: # Ex: pour les modèles de classification de texte
                    return f"HuggingFace ({model_name} - {first_result[0].get('label')}): Score {first_result[0].get('score', 'N/A'):.2f}"
                elif isinstance(first_result, dict) and "generated_text" in first_result: # Ex: pour les modèles de génération de texte
                    return f"HuggingFace ({model_name}): {first_result.get('generated_text')}"
            return f"HuggingFace ({model_name}): Réponse non parsée. {response}"
        return f"HuggingFace: Erreur: {response.get('message', 'Inconnu')}" if response else "HuggingFace: Réponse vide ou erreur interne."

class TwilioClient(APIClient):
    def __init__(self):
        super().__init__("TWILIO", endpoint_health_manager)

    async def query(self) -> str:
        """Récupère le solde du compte Twilio."""
        selected_endpoint_config = None
        for config in API_CONFIG.get(self.name, []):
            if "Account Balance" in config.get("endpoint_name", ""):
                selected_endpoint_config = config
                break
        if not selected_endpoint_config:
            # Fallback si l'endpoint spécifique n'est pas trouvé, prend le premier disponible
            if self.endpoints_config:
                selected_endpoint_config = self.endpoints_config[0]
            else:
                return f"Twilio: Aucune configuration d'endpoint disponible pour {self.name}."

        response = await self._make_request(
            url=selected_endpoint_config["url"],
            method=selected_endpoint_config["method"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            timeout=selected_endpoint_config.get("timeout")
        )
        if response and not response.get("error"):
            return f"Twilio (Balance): {response.get('balance', 'N/A')} {response.get('currency', 'N/A')}"
        return f"Twilio: Erreur: {response.get('message', 'Inconnu')}" if response else "Twilio: Réponse vide ou erreur interne."

class AbstractAPIClient(APIClient):
    def __init__(self):
        super().__init__("ABSTRACTAPI", endpoint_health_manager)

    async def query(self, input_value: str, api_type: str) -> str:
        """
        Interroge diverses APIs d'AbstractAPI (validation email/téléphone, taux de change, jours fériés).
        `input_value` dépend du `api_type`.
        """
        params = {}
        target_endpoint_name = ""

        if api_type == "PHONE_VALIDATION":
            params["phone"] = input_value
            target_endpoint_name = "Phone Validation"
        elif api_type == "EMAIL_VALIDATION":
            params["email"] = input_value
            target_endpoint_name = "Email Validation"
        elif api_type == "EXCHANGE_RATES":
            # Pour les taux de change, input_value peut être la devise de base (ex: "USD")
            params["base"] = input_value if input_value else "USD" 
            target_endpoint_name = "Exchange Rates"
        elif api_type == "HOLIDAYS":
            params["country"] = input_value if input_value else "US" # Pays par défaut si non spécifié
            params["year"] = datetime.now().year # Année actuelle
            target_endpoint_name = "Holidays"
        else:
            return f"AbstractAPI: Type d'API '{api_type}' non supporté pour la requête."

        selected_endpoint_config = None
        for config in API_CONFIG.get(self.name, []):
            if target_endpoint_name in config["endpoint_name"]:
                selected_endpoint_config = config
                break
        
        if not selected_endpoint_config:
            return f"AbstractAPI: Aucun endpoint sain ou disponible pour {self.name} pour le type {api_type}."

        response = await self._make_request(
            params=params,
            url=selected_endpoint_config["url"],
            method=selected_endpoint_config["method"],
            key_field=selected_endpoint_config["key_field"],
            key_location=selected_endpoint_config["key_location"],
            api_key=selected_endpoint_config["key"],
            fixed_params=selected_endpoint_config.get("fixed_params", {}),
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if api_type == "PHONE_VALIDATION":
                return (
                    f"AbstractAPI (Validation Tél): Numéro: {response.get('phone', 'N/A')}, "
                    f"Valide: {response.get('valid', 'N/A')}, "
                    f"Pays: {response.get('country', {}).get('name', 'N/A')}"
                )
            elif api_type == "EMAIL_VALIDATION":
                return (
                    f"AbstractAPI (Validation Email): Email: {response.get('email', 'N/A')}, "
                    f"Valide: {response.get('is_valid_format', 'N/A')}, "
                    f"Deliverable: {response.get('is_deliverable', 'N/A')}"
                )
            elif api_type == "EXCHANGE_RATES":
                # Affiche le taux de change pour USD par défaut, ou d'autres devises si présentes
                return f"AbstractAPI (Taux de change): Base: {response.get('base', 'N/A')}, Taux (USD): {response.get('exchange_rates', {}).get('USD', 'N/A')}"
            elif api_type == "HOLIDAYS":
                holidays = [h.get('name', 'N/A') for h in response if h.get('name')]
                return f"AbstractAPI (Jours fériés {params.get('country', 'US')} {params.get('year')}): {', '.join(holidays[:5])}..." if holidays else "Aucun jour férié trouvé."
            return f"AbstractAPI ({api_type}): Réponse brute: {response}"
        return f"AbstractAPI ({api_type}): Erreur: {response.get('message', 'Inconnu')}" if response else "AbstractAPI: Réponse vide ou erreur interne."

class GeminiAPIClient: # Cette classe est distincte de APIClient car elle gère l'API Gemini directement
    def __init__(self):
        self.api_key = GEMINI_API_KEY
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models/"
        # Le modèle est défini dans config.py et peut être passé à generate_content
        self.model_name = "gemini-1.5-flash-latest" # Valeur par défaut, peut être écrasée
        self.headers = {
            "Content-Type": "application/json",
        }
        # Les configurations de génération et de sécurité sont également dans config.py
        from config import GEMINI_TEMPERATURE, GEMINI_TOP_P, GEMINI_TOP_K, GEMINI_MAX_OUTPUT_TOKENS, GEMINI_SAFETY_SETTINGS
        self.generation_config = {
            "temperature": GEMINI_TEMPERATURE,
            "top_p": GEMINI_TOP_P,
            "top_k": GEMINI_TOP_K,
            "max_output_tokens": GEMINI_MAX_OUTPUT_TOKENS,
        }
        self.safety_settings = GEMINI_SAFETY_SETTINGS
        log_message(f"GeminiApiClient initialisé avec le modèle par défaut: {self.model_name}")

    async def generate_content(self, prompt: str, chat_history: List[Dict], image_data: Optional[str] = None, model: Optional[str] = None) -> Dict:
        """
        Génère du contenu textuel ou multimodal en utilisant l'API Gemini.
        `chat_history` est une liste de dictionnaires au format Gemini (role, parts).
        `image_data` est une chaîne base64 de l'image avec son préfixe mimeType (ex: "data:image/png;base64,...").
        `model` permet de spécifier un modèle différent si nécessaire.
        """
        model_to_use = model if model else self.model_name
        url = f"{self.base_url}{model_to_use}:generateContent?key={self.api_key}"

        # Construire les contenus de la requête. Le prompt utilisateur est toujours le dernier.
        contents = []
        for msg in chat_history:
            # L'API Gemini attend les rôles 'user' et 'model'
            role = "user" if msg["role"] == "user" else "model"
            contents.append({"role": role, "parts": [{"text": msg["content"]}]})
        
        user_parts = [{"text": prompt}]
        if image_data:
            # L'API Gemini attend un dictionnaire inlineData avec mimeType et data
            # Assurez-vous que image_data est au format "data:image/png;base64,..."
            # Extraire le mimeType et la base64 data
            if "," in image_data:
                mime_type_part, base64_data = image_data.split(",", 1)
                mime_type = mime_type_part.split(":", 1)[1].split(";", 1)[0]
            else:
                # Fallback si le préfixe est manquant, suppose un type d'image commun
                mime_type = "image/jpeg" 
                base64_data = image_data

            user_parts.append({
                "inlineData": {
                    "mimeType": mime_type,
                    "data": base64_data
                }
            })
            log_message(f"Image ajoutée au prompt Gemini (mimeType: {mime_type}).")

        contents.append({"role": "user", "parts": user_parts})

        payload = {
            "contents": contents,
            "generationConfig": self.generation_config,
            "safetySettings": self.safety_settings
        }

        log_message(f"Appel à Gemini API pour le modèle {model_to_use}...")
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(url, headers=self.headers, json=payload)
                response.raise_for_status() # Lève une exception pour les codes d'état HTTP 4xx/5xx
                result = response.json()
                log_message(f"Réponse Gemini reçue: {json.dumps(result, indent=2)}")
                return result
        except httpx.HTTPStatusError as e:
            log_message(f"Erreur HTTP Gemini API: {e.response.status_code} - {e.response.text}", level="error")
            return {"error": f"Erreur HTTP Gemini: {e.response.status_code} - {e.response.text}"}
        except httpx.RequestError as e:
            log_message(f"Erreur de requête Gemini API: {e}", level="error")
            return {"error": f"Erreur de requête Gemini: {e}"}
        except json.JSONDecodeError as e:
            log_message(f"Erreur de décodage JSON Gemini API: {e} - Réponse brute: {response.text}", level="error")
            return {"error": f"Erreur de décodage JSON Gemini: {e}"}
        except Exception as e:
            log_message(f"Erreur inattendue Gemini API: {e}\n{traceback.format_exc()}", level="error")
            return {"error": f"Erreur inattendue Gemini: {e}"}

class GoogleCustomSearchClient(APIClient):
    def __init__(self):
        super().__init__("GOOGLE_CUSTOM_SEARCH", endpoint_health_manager)

    async def query(self, query_text: str) -> str:
        """Effectue une recherche personnalisée Google via l'API Custom Search."""
        params = {"q": query_text}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            items = response.get("items", [])
            if items:
                output = "Google Custom Search:\n"
                for item in items[:3]: # Limite à 3 résultats
                    output += f"- {item.get('title', 'N/A')}: {item.get('snippet', 'N/A')} {neutralize_urls(item.get('link', ''))}\n"
                return output
            return "Google Custom Search: Aucun résultat trouvé."
        return f"Google Custom Search: Erreur: {response.get('message', 'Inconnu')}" if response else "Google Custom Search: Réponse vide ou erreur interne."

class RandommerClient(APIClient):
    def __init__(self):
        super().__init__("RANDOMMER", endpoint_health_manager)

    async def query(self, country_code: str = "US", quantity: int = 1) -> str:
        """Génère des numéros de téléphone aléatoires via Randommer.io."""
        params = {"CountryCode": country_code, "Quantity": quantity}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            if isinstance(response, list) and response:
                return f"Randommer (Numéros de téléphone): {', '.join(response)}"
            return f"Randommer: {response}" # Si la réponse n'est pas une liste (ex: erreur formatée)
        return f"Randommer: Erreur: {response.get('message', 'Inconnu')}" if response else "Randommer: Réponse vide ou erreur interne."

class TomorrowIOClient(APIClient):
    def __init__(self):
        super().__init__("TOMORROW.IO", endpoint_health_manager)

    async def query(self, location: str, fields: Optional[List[str]] = None) -> str:
        """Récupère les prévisions météorologiques via Tomorrow.io."""
        if fields is None:
            fields = ["temperature", "humidity", "windSpeed"] # Champs par défaut
        payload = {"location": location, "fields": fields, "units": "metric", "timesteps": ["1h"]}
        response = await self._make_request(json_data=payload)
        if response and not response.get("error"):
            data = response.get("data", {}).get("timelines", [{}])[0].get("intervals", [{}])[0].get("values", {})
            if data:
                output = f"Météo (Tomorrow.io) à {location}:\n"
                for field in fields:
                    output += f"- {field.capitalize()}: {data.get(field, 'N/A')}\n"
                return output
            return "Tomorrow.io: Données météo non trouvées."
        return f"Tomorrow.io: Erreur: {response.get('message', 'Inconnu')}" if response else "Tomorrow.io: Réponse vide ou erreur interne."

class OpenWeatherMapClient(APIClient):
    def __init__(self):
        super().__init__("OPENWEATHERMAP", endpoint_health_manager)

    async def query(self, location: str) -> str:
        """Récupère les conditions météorologiques actuelles via OpenWeatherMap."""
        params = {"q": location}
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            main_data = response.get("main", {})
            weather_desc = response.get("weather", [{}])[0].get("description", "N/A")
            if main_data:
                temp_kelvin = main_data.get('temp', 'N/A')
                feels_like_kelvin = main_data.get('feels_like', 'N/A')
                
                # Convertit les températures de Kelvin en Celsius
                temp_celsius = f"{temp_kelvin - 273.15:.2f}" if isinstance(temp_kelvin, (int, float)) else "N/A"
                feels_like_celsius = f"{feels_like_kelvin - 273.15:.2f}" if isinstance(feels_like_kelvin, (int, float)) else "N/A"

                return (
                    f"Météo (OpenWeatherMap) à {location}:\n"
                    f"Température: {temp_celsius}°C, "
                    f"Ressenti: {feels_like_celsius}°C, "
                    f"Humidité: {main_data.get('humidity', 'N/A')}%, "
                    f"Conditions: {weather_desc}"
                )
            return "OpenWeatherMap: Données météo non trouvées."
        return f"OpenWeatherMap: Erreur: {response.get('message', 'Inconnu')}" if response else "OpenWeatherMap: Réponse vide ou erreur interne."

class MockarooClient(APIClient):
    def __init__(self):
        super().__init__("MOCKAROO", endpoint_health_manager)

    async def query(self, count: int = 1, fields_json: Optional[str] = None) -> str:
        """Génère des données de test via Mockaroo."""
        params = {"count": count}
        if fields_json:
            params["fields"] = fields_json # `fields_json` doit être une chaîne JSON valide
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            return f"Mockaroo (Génération de données):\n{json.dumps(response, indent=2)}"
        return f"Mockaroo: Erreur: {response.get('message', 'Inconnu')}" if response else "Mockaroo: Réponse vide ou erreur interne."

class OpenPageRankClient(APIClient):
    def __init__(self):
        super().__init__("OPENPAGERANK", endpoint_health_manager)

    async def query(self, domains: List[str]) -> str:
        """Récupère le PageRank de domaines via OpenPageRank."""
        params = {"domains[]": domains} # L'API attend "domains[]" pour une liste
        response = await self._make_request(params=params)
        if response and not response.get("error"):
            results = response.get("response", [])
            if results:
                output = "OpenPageRank (Classement de domaine):\n"
                for res in results:
                    output += f"- Domaine: {res.get('domain', 'N/A')}, PageRank: {res.get('page_rank', 'N/A')}\n"
                return output
            return "OpenPageRank: Aucun résultat trouvé."
        return f"OpenPageRank: Erreur: {response.get('message', 'Inconnu')}" if response else "OpenPageRank: Réponse vide ou erreur interne."

class RapidAPIClient(APIClient):
    def __init__(self):
        super().__init__("RAPIDAPI", endpoint_health_manager)

    async def query(self, api_name: str, **kwargs) -> str:
        """
        Interroge diverses APIs disponibles via RapidAPI (blagues, taux de change, faits aléatoires).
        `api_name` spécifie l'API RapidAPI à utiliser.
        """
        selected_endpoint_config = None
        for config in API_CONFIG.get(self.name, []):
            if api_name.lower() in config["endpoint_name"].lower():
                selected_endpoint_config = config
                break
        
        if not selected_endpoint_config:
            return f"RapidAPI: Endpoint pour '{api_name}' non trouvé ou non configuré."

        url = selected_endpoint_config["url"]
        method = selected_endpoint_config["method"]
        
        request_params = selected_endpoint_config.get("fixed_params", {}).copy()
        request_headers = selected_endpoint_config.get("fixed_headers", {}).copy()
        request_json_data = selected_endpoint_config.get("fixed_json", {}).copy()

        # Ajoute les kwargs aux bons endroits selon la méthode HTTP
        if method == "GET":
            request_params.update(kwargs)
        elif method == "POST":
            request_json_data.update(kwargs)

        # Les en-têtes spécifiques à RapidAPI sont essentiels
        headers = {
            selected_endpoint_config["key_field"]: selected_endpoint_config["key"],
            "X-RapidAPI-Host": selected_endpoint_config["fixed_headers"].get("X-RapidAPI-Host")
        }
        
        response = await self._make_request(
            params=request_params,
            headers=headers,
            json_data=request_json_data,
            url=url,
            method=method,
            timeout=selected_endpoint_config.get("timeout")
        )

        if response and not response.get("error"):
            if api_name.lower() == "programming joke":
                return f"RapidAPI (Blague de Programmation): {response.get('setup', '')} - {response.get('punchline', '')}"
            elif api_name.lower() == "currency list quotes":
                return f"RapidAPI (Devises): {json.dumps(response, indent=2)}"
            elif api_name.lower() == "random fact":
                return f"RapidAPI (Fait Aléatoire): {response.get('text', 'N/A')}"
            return f"RapidAPI ({api_name}): {json.dumps(response, indent=2)}" # Retourne la réponse brute pour les autres
        return f"RapidAPI ({api_name}): Erreur: {response.get('message', 'Inconnu')}" if response else "RapidAPI: Réponse vide ou erreur interne."

class OCRApiClient: # Cette classe est distincte de APIClient car elle gère l'API OCR.space directement
    def __init__(self):
        from config import OCR_API_KEY # Importe la clé OCR spécifique pour cette classe
        self.api_key = OCR_API_KEY
        self.base_url = "https://api.ocr.space/parse/image"
        log_message("OCRApiClient initialisé.")

    async def query(self, image_base64: str) -> str:
        """
        Effectue une requête OCR à l'API OCR.space.
        `image_base64` doit être la chaîne base64 de l'image, incluant le préfixe mimeType
        (ex: "data:image/png;base64,...").
        """
        payload = {
            "base64Image": image_base64,
            "language": "fre", # Langue par défaut : Français
            "isOverlayRequired": False, # Ne pas inclure l'overlay des régions de texte
            "OCREngine": 2 # Utilise le moteur OCR 2 pour de meilleurs résultats
        }
        headers = {
            "apikey": self.api_key,
            "Content-Type": "application/json"
        }

        log_message("Appel à OCR.space API...")
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(self.base_url, headers=headers, json=payload)
                response.raise_for_status() # Lève une exception pour les codes d'état HTTP 4xx/5xx
                result = response.json()

                if result.get("IsErroredOnProcessing"):
                    error_message = result.get("ErrorMessage", ["Erreur inconnue lors du traitement OCR."])
                    log_message(f"Erreur OCR.space: {error_message}", level="error")
                    return f"❌ Erreur OCR: {', '.join(error_message)}"
                
                parsed_text = ""
                if "ParsedResults" in result and result["ParsedResults"]:
                    for parsed_result in result["ParsedResults"]:
                        parsed_text += parsed_result.get("ParsedText", "") + "\n"
                
                if parsed_text.strip():
                    log_message("OCR.space: Texte extrait avec succès.")
                    return parsed_text.strip()
                else:
                    log_message("OCR.space: Aucun texte extrait.", level="warning")
                    return "Aucun texte n'a pu être extrait de l'image."

        except httpx.HTTPStatusError as e:
            log_message(f"Erreur HTTP OCR.space API: {e.response.status_code} - {e.response.text}", level="error")
            return f"❌ Erreur HTTP OCR: {e.response.status_code} - {e.response.text}"
        except httpx.RequestError as e:
            log_message(f"Erreur de requête OCR.space API: {e}", level="error")
            return f"❌ Erreur de requête OCR: {e}"
        except json.JSONDecodeError as e:
            log_message(f"Erreur de décodage JSON OCR.space API: {e} - Réponse brute: {response.text}", level="error")
            return {"error": f"Erreur de décodage JSON OCR: {e}"}
        except Exception as e:
            log_message(f"Erreur inattendue OCR.space API: {e}\n{traceback.format_exc()}", level="error")
            return f"❌ Erreur inattendue OCR: {e}"


# Instancier tous les clients API en leur passant le gestionnaire de santé
# Note: GeminiApiClient et OCRApiClient sont instanciés séparément car ils gèrent leurs clés directement.
ALL_API_CLIENTS = [
    DeepSeekClient(), SerperClient(), WolframAlphaClient(), TavilyClient(),
    ApiFlashClient(), CrawlbaseClient(), DetectLanguageClient(), GuardianClient(),
    IP2LocationClient(), ShodanClient(), WeatherAPIClient(),
    CloudmersiveClient(), GreyNoiseClient(), PulsediveClient(), StormGlassClient(),
    LoginRadiusClient(), JsonbinClient(),
    HuggingFaceClient(), TwilioClient(), AbstractAPIClient(),
    GoogleCustomSearchClient(), RandommerClient(), TomorrowIOClient(),
    OpenWeatherMapClient(),
    MockarooClient(), OpenPageRankClient(), RapidAPIClient()
    # GeminiApiClient et OCRApiClient ne sont pas inclus ici car ils sont gérés par leurs propres classes
]

import json
import asyncio
import random
import ast
import subprocess
import base64
import httpx
import io
import contextlib
import traceback
import hashlib
import difflib
import re
import logging
from datetime import datetime, timedelta, timezone
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, Any, Optional, List, Union, Tuple

# Imports des constantes depuis config.py
from config import (
    API_QUOTAS, API_COOLDOWN_DURATION_SECONDS, API_ROTATION_INTERVAL_MINUTES,
    QUOTA_BURN_WINDOW_HOURS, USER_CHAT_HISTORY_FILE, USER_LONG_MEMORY_FILE,
    IA_STATUS_FILE, QUOTAS_FILE, GROUP_CHAT_HISTORY_FILE, PRIVATE_GROUP_ID,
    BURN_QUOTA_THRESHOLD_RATIO, BURN_QUOTA_BEFORE_RESET_HOURS,
    FORBIDDEN_WORDS, ARCHIVES_DIR, MAX_FILE_SIZE, GEMINI_API_KEY, OCR_API_KEY
)

# Imports depuis utils.py (en supposant qu'il est déjà défini ou sera dans le même fichier)
from utils import (
    load_json, save_json, get_current_time, format_datetime, log_message,
    neutralize_urls, extract_keywords, tag_conversation, unique_preserve_order,
    similar, get_user_dir
)

# --- Début du module memory_and_quotas.py ---

class MemoryManager:
    """
    Gère la mémoire à court et long terme du bot, ainsi que l'historique des conversations
    pour les utilisateurs individuels et les groupes.
    C'est un singleton pour s'assurer qu'il n'y a qu'une seule instance de gestionnaire de mémoire.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Implémente le patron de conception Singleton."""
        if cls._instance is None:
            cls._instance = super(cls, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """Initialise les structures de données pour la mémoire."""
        if self._initialized:
            return
        self.chat_history: Dict[Union[int, str], List[Dict]] = {} # {user_id: [messages]}
        self.long_term_memory: Dict[Union[int, str], List[str]] = {} # {user_id: [facts]}
        self.ia_status: Dict[str, Dict[str, Any]] = {} # Statut global des IA
        self.group_chat_history: Dict[int, List[Dict]] = {} # {group_id: [messages]}
        # `_initialized` est géré par `init_manager` pour les opérations asynchrones

    async def init_manager(self):
        """
        Initialise le gestionnaire de mémoire de manière asynchrone.
        Charge les statuts persistants et les historiques de groupe.
        """
        if not self._initialized:
            # Charge le statut global des IA
            self.ia_status = await load_json(IA_STATUS_FILE, {})
            self._initialize_ia_status()
            
            # Charge l'historique du groupe privé spécifique
            group_dir = get_user_dir(PRIVATE_GROUP_ID)
            self.group_chat_history[PRIVATE_GROUP_ID] = await load_json(group_dir / GROUP_CHAT_HISTORY_FILE, [])

            self._initialized = True
            log_message("Gestionnaire de mémoire initialisé.")

    def _initialize_ia_status(self):
        """
        Initialise ou met à jour le statut des IA si elles ne sont pas déjà présentes
        ou si leur statut est obsolète. S'assure que toutes les clés nécessaires sont là.
        """
        updated = False
        now = get_current_time()
        
        for client_name in API_QUOTAS.keys(): # Itère sur toutes les APIs définies dans les quotas
            if client_name not in self.ia_status:
                self.ia_status[client_name] = {
                    "last_used": None,
                    "last_error": None,
                    "error_count": 0,
                    "cooldown_until": None,
                    "success_count": 0,
                    "current_score": 1.0, # Score initial de 1.0 (parfait)
                    "last_rotation_check": format_datetime(now),
                    "diversification_score": 1.0 # Score de diversification initial de 1.0 (très diversifiable)
                }
                updated = True
            else:
                # S'assure que toutes les nouvelles clés sont présentes dans les statuts existants
                default_ia_status_keys = {
                    "last_used": None, "last_error": None, "error_count": 0,
                    "cooldown_until": None, "success_count": 0, "current_score": 1.0,
                    "last_rotation_check": format_datetime(now), "diversification_score": 1.0
                }
                for key, default_value in default_ia_status_keys.items():
                    if key not in self.ia_status[client_name]:
                        self.ia_status[client_name][key] = default_value
                        updated = True
                
                # Met à jour `last_rotation_check` si trop ancien pour permettre la récupération du score de diversification
                last_check_str = self.ia_status[client_name].get("last_rotation_check")
                if last_check_str:
                    try:
                        last_check_dt = datetime.strptime(last_check_str, "%Y-%m-%d %H:%M:%S UTC")
                        # Si le dernier check est plus ancien que deux fois l'intervalle de rotation, on le réinitialise
                        if (now - last_check_dt).total_seconds() > API_ROTATION_INTERVAL_MINUTES * 60 * 2:
                            self.ia_status[client_name]["last_rotation_check"] = format_datetime(now)
                            updated = True
                    except ValueError: # Gère les chaînes de date malformées
                        self.ia_status[client_name]["last_rotation_check"] = format_datetime(now)
                        updated = True
                        log_message(f"last_rotation_check malformé pour {client_name}, réinitialisation.", level="warning")

        # Supprime les noms d'IA qui ne sont plus définis dans `API_QUOTAS`
        current_api_names = set(API_QUOTAS.keys())
        ia_names_to_remove = [name for name in self.ia_status if name not in current_api_names]
        for name in ia_names_to_remove:
            del self.ia_status[name]
            updated = True
            log_message(f"IA '{name}' trouvée dans ia_status.json mais non définie dans API_QUOTAS. Supprimée.", level="warning")

        if updated:
            # Sauvegarde l'état mis à jour de manière asynchrone
            asyncio.create_task(save_json(IA_STATUS_FILE, self.ia_status))
            log_message("Statut des IA initialisé/mis à jour.")

    async def add_message_to_history(self, user_id: Union[int, str], role: str, content: str, max_log_entries: int = 100):
        """
        Ajoute un message à l'historique de la conversation d'un utilisateur.
        Gère également le log général de l'utilisateur et le taggage des messages.
        """
        user_dir = get_user_dir(user_id)
        chat_history_path = user_dir / USER_CHAT_HISTORY_FILE
        log_path = user_dir / "log.json" # Fichier de log général pour l'utilisateur

        # Charge les historiques existants pour l'utilisateur (ou utilise ceux en mémoire)
        user_chat_hist = self.chat_history.get(user_id, await load_json(chat_history_path, []))
        user_log = await load_json(log_path, [])

        # Neutralise les URLs pour le stockage (sécurité/confidentialité)
        neutralized_content = neutralize_urls(content)

        # Ajoute au chat history de l'utilisateur
        user_chat_hist.append({"role": role, "content": neutralized_content, "timestamp": format_datetime(get_current_time())})
        user_chat_hist = user_chat_hist[-max_log_entries:] # Tronque l'historique pour ne garder que les N derniers messages
        self.chat_history[user_id] = user_chat_hist
        asyncio.create_task(save_json(chat_history_path, user_chat_hist))
        
        # Ajoute au log général de l'utilisateur (contenu tronqué pour le log)
        log_entry_content = neutralized_content[:500] # Tronque pour le fichier de log
        log_entry = {"time": format_datetime(get_current_time()), "role": role, "text": log_entry_content}
        if role == "user": # Taggue les messages de l'utilisateur avec des mots-clés
            log_entry["tags"] = tag_conversation(content)
        user_log.append(log_entry)
        user_log = user_log[-max_log_entries:] # Tronque le log général
        asyncio.create_task(save_json(log_path, user_log))

        log_message(f"Message ajouté à l'historique de {user_id} par {role}.")

    async def get_chat_history(self, user_id: Union[int, str], limit: int = 10) -> List[Dict]:
        """
        Retourne les N derniers messages de l'historique de conversation d'un utilisateur.
        Charge l'historique depuis le fichier si non déjà en mémoire.
        """
        user_dir = get_user_dir(user_id)
        chat_history_path = user_dir / USER_CHAT_HISTORY_FILE
        if user_id not in self.chat_history:
            self.chat_history[user_id] = await load_json(chat_history_path, [])
        return self.chat_history[user_id][-limit:]

    async def save_group_memory(self, group_id: int, role: str, text: str, max_items: int = 1000):
        """
        Sauvegarde l'historique de chat pour un groupe spécifique.
        Utilise l'ID du groupe comme un ID utilisateur pour la structure de répertoire.
        """
        group_dir = get_user_dir(group_id)
        group_history_path = group_dir / GROUP_CHAT_HISTORY_FILE
        
        hist = self.group_chat_history.get(group_id, await load_json(group_history_path, []))
        hist.append({"time": format_datetime(get_current_time()), "role": role, "text": neutralize_urls(text)})
        hist = hist[-max_items:] # Tronque l'historique du groupe
        self.group_chat_history[group_id] = hist
        asyncio.create_task(save_json(group_history_path, hist))
        log_message(f"Message ajouté à la mémoire de groupe {group_id} par {role}.")

    async def get_group_memory(self, group_id: int, limit: int = 20) -> str:
        """
        Récupère les N derniers messages de la mémoire de groupe.
        Retourne une chaîne formatée pour être utilisée comme contexte.
        """
        group_dir = get_user_dir(group_id)
        group_history_path = group_dir / GROUP_CHAT_HISTORY_FILE
        if group_id not in self.group_chat_history:
            self.group_chat_history[group_id] = await load_json(group_history_path, [])
        
        if not isinstance(self.group_chat_history[group_id], list): # S'assure que c'est une liste
            self.group_chat_history[group_id] = []

        # Filtre les messages du bot si non nécessaires pour le contexte du prompt
        recent_messages = [f"{l['role']} : {l['text']}" for l in self.group_chat_history[group_id][-limit:] if l.get("role") != "bot"]
        return "\n".join(recent_messages)

    async def add_to_long_term_memory(self, user_id: Union[int, str], text: str, max_entries: int = 100):
        """
        Ajoute une information à la mémoire à long terme d'un utilisateur.
        Dédoublonne et tronque la mémoire.
        """
        user_dir = get_user_dir(user_id)
        long_memory_path = user_dir / USER_LONG_MEMORY_FILE
        
        long_mem = self.long_term_memory.get(user_id, await load_json(long_memory_path, []))
        if not isinstance(long_mem, list): # S'assure que c'est une liste
            long_mem = []

        long_mem.append(text.strip())
        long_mem = unique_preserve_order(long_mem)[-max_entries:] # Dédoublonne et tronque
        self.long_term_memory[user_id] = long_mem
        asyncio.create_task(save_json(long_memory_path, long_mem))
        log_message(f"Information ajoutée à la mémoire à long terme de {user_id}.")

    async def get_long_term_memory(self, user_id: Union[int, str], limit: int = 20) -> str:
        """
        Récupère les N dernières entrées de la mémoire à long terme d'un utilisateur.
        Retourne une chaîne formatée.
        """
        user_dir = get_user_dir(user_id)
        long_memory_path = user_dir / USER_LONG_MEMORY_FILE
        if user_id not in self.long_term_memory:
            self.long_term_memory[user_id] = await load_json(long_memory_path, [])
        
        if not isinstance(self.long_term_memory[user_id], list): # S'assure que c'est une liste
            self.long_term_memory[user_id] = []

        return "\n".join(self.long_term_memory[user_id][-limit:])

    async def check_for_similar_prompt(self, user_id: Union[int, str], prompt: str) -> Optional[str]:
        """
        Vérifie si un prompt similaire a déjà été posé récemment et retourne la réponse si trouvée.
        Utilise `MAX_CACHE_SIZE` pour la fenêtre de recherche.
        """
        recent_chat_history = await self.get_chat_history(user_id, limit=MAX_CACHE_SIZE)
        for entry in reversed(recent_chat_history): # Parcours l'historique du plus récent au plus ancien
            if entry.get("role") == "user" and "content" in entry:
                # Compare la similarité du prompt actuel avec les anciens prompts utilisateur
                if similar(prompt, entry["content"]) > 0.92: # Seuil de similarité (92%)
                    # Si un prompt similaire est trouvé, cherche la réponse du bot qui suit
                    for i in range(len(recent_chat_history) - 1, -1, -1):
                        if recent_chat_history[i] == entry and i + 1 < len(recent_chat_history):
                            if recent_chat_history[i+1].get("role") == "bot":
                                log_message(f"Prompt similaire détecté pour {user_id}. Réponse en cache utilisée.")
                                return recent_chat_history[i+1]["content"]
        return None

    def update_ia_status(self, ia_name: str, success: bool, error_message: Optional[str] = None):
        """
        Met à jour le statut et le score d'une IA après une utilisation.
        Ajuste le score de performance et le score de diversification.
        """
        status = self.ia_status.get(ia_name)
        if not status:
            log_message(f"Tentative de mise à jour d'un statut d'IA inconnu: {ia_name}", level="warning")
            return

        now = get_current_time()
        status["last_used"] = format_datetime(now)

        if success:
            status["success_count"] += 1
            status["error_count"] = 0 # Réinitialise le compteur d'erreurs consécutives
            status["cooldown_until"] = None # Annule le cooldown
            status["last_error"] = None
            status["current_score"] = min(1.0, status["current_score"] + 0.1) # Augmente le score de performance
            # Diminue le score de diversification car l'IA vient d'être utilisée
            status["diversification_score"] = max(0.1, status["diversification_score"] - 0.1) 
            log_message(f"IA {ia_name} : Succès enregistré. Nouveau score: {status['current_score']:.2f}, Diversification: {status['diversification_score']:.2f}")
        else:
            status["error_count"] += 1
            status["last_error"] = error_message
            if status["error_count"] >= 3: # Si 3 erreurs consécutives ou plus, met en cooldown
                status["cooldown_until"] = format_datetime(now + timedelta(seconds=API_COOLDOWN_DURATION_SECONDS))
                status["current_score"] = max(0.1, status["current_score"] - 0.2) # Diminue plus fortement le score
                log_message(f"IA {ia_name} : Trop d'erreurs ({status['error_count']}). Cooldown jusqu'à {status['cooldown_until']}. Nouveau score: {status['current_score']:.2f}", level="warning")
            else:
                 status["current_score"] = max(0.1, status["current_score"] - 0.05) # Diminue légèrement
                 log_message(f"IA {ia_name} : Erreur enregistrée. Nouveau score: {status['current_score']:.2f}", level="warning")

        # Sauvegarde l'état mis à jour de manière asynchrone
        asyncio.create_task(save_json(IA_STATUS_FILE, self.ia_status))

    def recover_diversification_scores(self):
        """
        Augmente le score de diversification pour les IA qui n'ont pas été utilisées récemment.
        Ceci encourage la rotation des APIs.
        """
        now = get_current_time()
        updated = False
        for ia_name, status in self.ia_status.items():
            last_used_str = status.get("last_used")
            if last_used_str:
                try:
                    last_used_dt = datetime.strptime(last_used_str, "%Y-%m-%d %H:%M:%S UTC")
                    # Si pas utilisée depuis 2x l'intervalle de rotation, augmente son score de diversification
                    if (now - last_used_dt).total_seconds() > API_ROTATION_INTERVAL_MINUTES * 60 * 2:
                        if status["diversification_score"] < 1.0:
                            status["diversification_score"] = min(1.0, status["diversification_score"] + 0.05)
                            updated = True
                            log_message(f"IA {ia_name}: Score de diversification récupéré à {status['diversification_score']:.2f}")
                except ValueError: # Gère les chaînes de date malformées
                    status["last_used"] = format_datetime(now) # Réinitialise `last_used`
                    status["diversification_score"] = 1.0 # Réinitialise le score de diversification
                    updated = True
                    log_message(f"last_used malformé pour {ia_name}, réinitialisation du score de diversification.", level="warning")
            else: # Si jamais utilisée, met à 1.0 (pleine diversification)
                if status["diversification_score"] < 1.0:
                    status["diversification_score"] = 1.0
                    updated = True
        if updated:
            asyncio.create_task(save_json(IA_STATUS_FILE, self.ia_status))

    def get_ia_status(self, ia_name: str) -> Optional[Dict]:
        """Récupère le statut d'une IA spécifique."""
        return self.ia_status.get(ia_name)

    def get_available_ias(self) -> List[str]:
        """
        Retourne les noms des IA actuellement non en cooldown.
        """
        available = []
        now = get_current_time()
        for name, status in self.ia_status.items():
            cooldown_until_str = status.get("cooldown_until")
            if cooldown_until_str:
                try:
                    cooldown_until = datetime.strptime(cooldown_until_str, "%Y-%m-%d %H:%M:%S UTC")
                    if now < cooldown_until: # Si toujours en cooldown, on la saute
                        continue
                except ValueError: # Date malformée, on la considère comme non en cooldown
                    log_message(f"cooldown_until malformé pour {name}, considéré comme non en cooldown.", level="warning")
            available.append(name)
        return available

class QuotaManager:
    """
    Gère les quotas d'utilisation pour toutes les APIs.
    Suit l'utilisation mensuelle, journalière et horaire, et peut alerter en cas de dépassement.
    Prend également en charge le mode "brûlage" de quota.
    C'est un singleton.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Implémente le patron de conception Singleton."""
        if cls._instance is None:
            cls._instance = super(cls, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """Initialise les structures de données pour les quotas."""
        if self._initialized:
            return
        self.quotas = {}
        # `_initialized` est géré par `init_manager` pour les opérations asynchrones
        self.bot_instance = None # Sera injecté par main.py pour l'envoi d'alertes

    async def init_manager(self):
        """
        Initialise le gestionnaire de quotas de manière asynchrone.
        Charge les données de quotas persistantes et s'assure qu'elles sont à jour.
        """
        if not self._initialized:
            self.quotas = await load_json(QUOTAS_FILE, {})
            self._initialize_quotas()
            self._initialized = True
            log_message("Gestionnaire de quotas initialisé.")

    def set_bot_instance(self, bot_instance: Any):
        """
        Permet d'injecter l'instance du bot pour envoyer des alertes de quota au groupe privé.
        """
        self.bot_instance = bot_instance

    def _initialize_quotas(self):
        """
        Initialise les quotas pour toutes les APIs basées sur `config.API_QUOTAS`.
        Nettoie et met à jour les entrées existantes si nécessaire.
        """
        updated = False
        now = get_current_time()

        for api_name, quota_info in API_QUOTAS.items():
            if api_name not in self.quotas:
                self.quotas[api_name] = {
                    "monthly_usage": 0,
                    "daily_usage": 0,
                    "hourly_usage": 0,
                    "hourly_timestamps": [], # Pour un suivi plus précis de l'heure
                    "last_reset_month": now.month,
                    "last_reset_day": now.day,
                    "last_usage": None,
                    "total_calls": 0,
                    "last_hourly_reset": format_datetime(now)
                }
                updated = True
            else:
                # S'assure que toutes les nouvelles clés sont présentes dans les données de quota existantes
                default_quota_structure = {
                    "monthly_usage": 0, "daily_usage": 0, "hourly_usage": 0,
                    "hourly_timestamps": [], "last_reset_month": now.month,
                    "last_reset_day": now.day, "last_usage": None,
                    "total_calls": 0, "last_hourly_reset": format_datetime(now)
                }
                for key, default_value in default_quota_structure.items():
                    if key not in self.quotas[api_name]:
                        self.quotas[api_name][key] = default_value
                        updated = True
                
                # Nettoie `hourly_timestamps` pour les entrées existantes (supprime les anciens horodatages)
                if not isinstance(self.quotas[api_name].get("hourly_timestamps"), list):
                    self.quotas[api_name]["hourly_timestamps"] = []
                
                one_hour_ago = now - timedelta(hours=1)
                self.quotas[api_name]["hourly_timestamps"] = [
                    ts for ts in self.quotas[api_name]["hourly_timestamps"]
                    if datetime.strptime(ts, "%Y-%m-%d %H:%M:%S UTC").replace(tzinfo=timezone.utc) > one_hour_ago # Ajoute tzinfo
                ]
                self.quotas[api_name]["hourly_usage"] = len(self.quotas[api_name]["hourly_timestamps"])
                self.quotas[api_name]["last_hourly_reset"] = format_datetime(now)
                updated = True # Marque comme mis à jour car les timestamps ont été nettoyés

        # Supprime les noms d'API qui ne sont plus définis dans `API_QUOTAS`
        api_names_to_remove = [name for name in self.quotas if name not in API_QUOTAS]
        for name in api_names_to_remove:
            del self.quotas[name]
            updated = True
            log_message(f"API '{name}' trouvée dans quotas.json mais non définie dans API_QUOTAS. Supprimée.", level="warning")

        if updated:
            asyncio.create_task(save_json(QUOTAS_FILE, self.quotas))
            log_message("Quotas API initialisés/mis à jour.")

    def _reset_quotas_if_needed(self):
        """
        Réinitialise les quotas journaliers, mensuels et horaires si nécessaire.
        Cette méthode est appelée avant chaque vérification de quota pour s'assurer de l'actualité.
        """
        now = get_current_time()
        for api_name, data in self.quotas.items():
            # Réinitialisation mensuelle
            if now.month != data["last_reset_month"]:
                data["monthly_usage"] = 0
                data["last_reset_month"] = now.month
                log_message(f"Quota mensuel pour {api_name} réinitialisé.")
            # Réinitialisation journalière
            if now.day != data["last_reset_day"]:
                data["daily_usage"] = 0
                data["last_reset_day"] = now.day
                log_message(f"Quota journalier pour {api_name} réinitialisé.")
            
            # Réinitialisation horaire (en nettoyant les anciens horodatages)
            one_hour_ago = now - timedelta(hours=1)
            # S'assure que hourly_timestamps est une liste
            if not isinstance(data.get("hourly_timestamps"), list):
                data["hourly_timestamps"] = []
            
            data["hourly_timestamps"] = [
                ts for ts in data["hourly_timestamps"]
                if datetime.strptime(ts, "%Y-%m-%d %H:%M:%S UTC").replace(tzinfo=timezone.utc) > one_hour_ago
            ]
            data["hourly_usage"] = len(data["hourly_timestamps"])
            data["last_hourly_reset"] = format_datetime(now) # Met à jour l'heure du dernier reset horaire

        asyncio.create_task(save_json(QUOTAS_FILE, self.quotas))

    async def check_and_update_quota(self, api_name: str, cost: int = 1) -> bool:
        """
        Vérifie si une API a du quota disponible et le décrémente si oui.
        Retourne `True` si l'opération est autorisée (quota disponible), `False` sinon.
        """
        self._reset_quotas_if_needed() # S'assure que les quotas sont à jour

        if api_name not in API_QUOTAS:
            log_message(f"Tentative de vérification de quota pour une API non définie: {api_name}. Autorisation refusée.", level="error")
            return False

        if api_name not in self.quotas:
            log_message(f"API {api_name} non trouvée dans les quotas gérés. Re-initialisation non bloquante.", level="warning")
            # Ce cas ne devrait pas arriver si _initialize_quotas est correctement appelé au démarrage
            self._initialize_quotas() # Ré-initialise tous les quotas pour s'assurer que cette API est ajoutée
            if api_name not in self.quotas: # Si toujours pas là, il y a un problème grave avec API_QUOTAS
                return False

        quota_data = self.quotas[api_name]
        api_limits = API_QUOTAS.get(api_name, {})
        now = get_current_time()

        # Vérifie la limite mensuelle
        monthly_limit = api_limits.get("monthly")
        if monthly_limit is not None and (quota_data["monthly_usage"] + cost) > monthly_limit:
            log_message(f"Quota mensuel dépassé pour {api_name}", level="warning")
            await self._alert_quota_if_needed(api_name, "mensuel")
            return False

        # Vérifie la limite journalière
        daily_limit = api_limits.get("daily")
        if daily_limit is not None and (quota_data["daily_usage"] + cost) > daily_limit:
            log_message(f"Quota journalier dépassé pour {api_name}", level="warning")
            await self._alert_quota_if_needed(api_name, "journalier")
            return False
        
        # Vérifie la limite horaire
        hourly_limit = api_limits.get("hourly")
        if hourly_limit is not None and (quota_data["hourly_usage"] + cost) > hourly_limit:
            log_message(f"Quota horaire dépassé pour {api_name}", level="warning")
            await self._alert_quota_if_needed(api_name, "horaire")
            return False

        # Vérifie le taux de requêtes par seconde
        rate_limit_per_sec = api_limits.get("rate_limit_per_sec")
        if rate_limit_per_sec:
            last_usage_str = quota_data.get("last_usage")
            if last_usage_str:
                try:
                    last_usage = datetime.strptime(last_usage_str, "%Y-%m-%d %H:%M:%S UTC").replace(tzinfo=timezone.utc)
                    time_since_last_call = (now - last_usage).total_seconds()
                    if time_since_last_call < (1 / rate_limit_per_sec):
                        log_message(f"Taux de requêtes dépassé pour {api_name}. Attendre {1/rate_limit_per_sec - time_since_last_call:.2f}s", level="warning")
                        return False
                except ValueError: # Date malformée, on la considère comme sans utilisation récente
                    log_message(f"last_usage malformé pour {api_name}, considéré comme sans utilisation récente pour la limite de taux.", level="warning")

        # Si toutes les vérifications passent, met à jour l'utilisation du quota
        if cost > 0:
            quota_data["monthly_usage"] += cost
            quota_data["daily_usage"] += cost
            quota_data["hourly_usage"] += cost
            quota_data["hourly_timestamps"].append(format_datetime(now)) # Ajoute l'horodatage pour le suivi horaire
            quota_data["total_calls"] += cost
            quota_data["last_usage"] = format_datetime(now)
            asyncio.create_task(save_json(QUOTAS_FILE, self.quotas)) # Sauvegarde de manière asynchrone
            log_message(f"Quota pour {api_name} mis à jour. Usage mensuel: {quota_data['monthly_usage']}/{monthly_limit if monthly_limit else 'Illimité'}, Journalier: {quota_data['daily_usage']}/{daily_limit if daily_limit else 'Illimité'}, Horaire: {quota_data['hourly_usage']}/{hourly_limit if hourly_limit else 'Illimité'}")
        else:
            log_message(f"Quota pour {api_name} vérifié (coût 0). Usage mensuel: {quota_data['monthly_usage']}/{monthly_limit if monthly_limit else 'Illimité'}, Journalier: {quota_data['daily_usage']}/{daily_limit if daily_limit else 'Illimité'}, Horaire: {quota_data['hourly_usage']}/{hourly_limit if hourly_limit else 'Illimité'}")

        return True
